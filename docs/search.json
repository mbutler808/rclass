[
  {
    "objectID": "posts/2023-01-31-what-is-the-question/index.html",
    "href": "posts/2023-01-31-what-is-the-question/index.html",
    "title": "What is the question?",
    "section": "",
    "text": "Take a look at the data in this recent paper: Winchell et. al, (2023) Genome-wide parallelism underlies contemporary adaptation in urban lizards. PNAS 120 (3) e2216789120 Check SLACK channel for pdf\n\n\n\n\n\nIn the age of Artificial Intelligence (AI), we have many data-intensive tools available. But can we just throw more data at a problem to get better outcomes? Please watch this thought provoking short talk by Sebastian Wernicke “How to use data to make a hit TV show”… What goes wrong when we look for decisions in the wrong places 🌴"
  },
  {
    "objectID": "posts/2023-03-02-multivariate/index.html",
    "href": "posts/2023-03-02-multivariate/index.html",
    "title": "A small tour of multivariate analysis",
    "section": "",
    "text": "Learning objectives\n\n\n\nAt the end of this lesson you will:\n\nBe able to perform basic univariate statistics\nBe able to perform basic multivariate statistics\nBe able to relate questions to graphical representations of data"
  },
  {
    "objectID": "posts/2023-03-02-multivariate/index.html#nature-of-the-relationship-among-variables",
    "href": "posts/2023-03-02-multivariate/index.html#nature-of-the-relationship-among-variables",
    "title": "A small tour of multivariate analysis",
    "section": "Nature of the relationship among variables",
    "text": "Nature of the relationship among variables\nYou may also expect your data to follow a power law, in which case a log-transformation will make the data linear. For example, things that scale with body size tend to have the form:\n\\[\nY = aMass^b  \n\\] \\[\nlog(Y) = log(a) + b\\times log(Mass)\n\\]"
  },
  {
    "objectID": "posts/2023-03-02-multivariate/index.html#fitting-assumptions-of-parametric-statistics",
    "href": "posts/2023-03-02-multivariate/index.html#fitting-assumptions-of-parametric-statistics",
    "title": "A small tour of multivariate analysis",
    "section": "Fitting assumptions of parametric statistics",
    "text": "Fitting assumptions of parametric statistics\nIf you plan to do parametric statistics, for example many forms of regression, ANOVA, etc. one of the major assumptions is that the errors are normally distributed.\nThat is, the relationship follows the form:\n\\[\nY \\sim X + e\n\\]\nWhich is read as Y is proportional to X plus random error. Where e ~ N or the errors or deviations from this relationship follow a normal distribution. Note that this assumes that X is known without error.\n\nChecking for normality\nA convenient tool for checking the normality of continuous data is qqnorm() which plots the QQ quantiles of the data. If it is normally distributed, the points should fall on a straight line:\n\nqqnorm(iris$Sepal.Length)\nqqline(iris$Sepal.Length)\n\n\n\n\nOr the ggplot2 version:\n\nrequire(ggplot2)\n\nLoading required package: ggplot2\n\nrequire(magrittr)\n\nLoading required package: magrittr\n\nggplot(iris, aes(sample=Sepal.Length)) + \n  stat_qq() +\n  stat_qq_line()\n\n\n\n\nThis data looks pretty good, except for some deviations along the edges. Most data will not ever be perfectly normal, you will get a sense of what is acceptable with more experience.\nHowever, we do know that this data contains three species – what happens if you were to look at the data by species?\n\niris %>% \n    ggplot( aes(sample=Sepal.Length)) + \n   stat_qq( aes(col=Species)) +\n   stat_qq_line( aes(col=Species) )\n\n\n\n\n\n\nSkew\nDeviations from normality are not the end of the world, and often a little is tolerated. What can be more problematic is strong skew. For that you will really want to transform the data:\n Source\nRight skewed data is data with a long tail to the right (the positive side). Left skeweed data has along tail to the left. Here are a few methods. There are more\n\n\n\nSkew\nTransform\nCode\n\n\n\n\nstrong right\ncube root\nz = x^(1/3)\n\n\n\nsquare root\nz = x6(1/2)\n\n\n\nlog\nz = log(x)\n\n\n\n\nz = log10(x)\n\n\n\n\nz = log2(x)\n\n\nstrong left\nsquare\nz = x^2"
  },
  {
    "objectID": "posts/2023-03-02-multivariate/index.html#separating-size-and-shape",
    "href": "posts/2023-03-02-multivariate/index.html#separating-size-and-shape",
    "title": "A small tour of multivariate analysis",
    "section": "Separating Size and Shape",
    "text": "Separating Size and Shape\nWill you want to do an analysis of the data along with a size-corrected dataset? If shape variation is interesting for your data (i.e., do they differ in shape when we control for differences in size, or are they relatively larger or smaller?), then you may want to find some sort of size-adjustment. Popular methods include\n\nRegressing against a size variable, and using residuals\n\nPCA analysis excluding PC1 (PC1 is considered size),\n\nShear or Procrustes methods, and\n\nRatios with size.\nThere is a huge wealth of literature on size and how to analyze shape."
  },
  {
    "objectID": "posts/2023-03-02-multivariate/index.html#linear-regression",
    "href": "posts/2023-03-02-multivariate/index.html#linear-regression",
    "title": "A small tour of multivariate analysis",
    "section": "Linear Regression",
    "text": "Linear Regression\nLinear regression asks whether there is a relationship between X and Y, that is if you know X can you predict the value of Y?\n\nwith(iris, plot(Sepal.Width ~ Sepal.Length))\nlm.fit <- with(iris, lm(Sepal.Width ~ Sepal.Length))\nabline(lm.fit, col=\"blue\")\n\n\n\n\nLinear regression results in two parameters, the best-fit slope and intercept:\n\nlm.fit \n\n\nCall:\nlm(formula = Sepal.Width ~ Sepal.Length)\n\nCoefficients:\n (Intercept)  Sepal.Length  \n     3.41895      -0.06188  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sepal.Width ~ Sepal.Length)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1095 -0.2454 -0.0167  0.2763  1.3338 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   3.41895    0.25356   13.48   <2e-16 ***\nSepal.Length -0.06188    0.04297   -1.44    0.152    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4343 on 148 degrees of freedom\nMultiple R-squared:  0.01382,   Adjusted R-squared:  0.007159 \nF-statistic: 2.074 on 1 and 148 DF,  p-value: 0.1519\n\n\nRegression minimizes the sum of squared errors (or deviations) from the line. The “errors” are the difference between where Y is, and where Y should be if it followed a perfect line.\nWe can illustrate what this means:\n\nx <- iris$Sepal.Length\ny <- iris$Sepal.Width\nyhat <- predict(lm.fit)\n\nwith(iris, plot(Sepal.Width ~ Sepal.Length))\nwith(iris, abline(lm.fit, col=\"blue\"))\nfor(i in 1:length(x))  lines(x[c(i,i)],c(y[i], yhat[i]), col=\"red\", lty=2)\n\n\n\n\nThe regression line is the best fit line that minimizes the sums of squared deviations from the regression. It turns out that the least-squares fit of the regression line is also provides the Maximum Likelihood fits of the parameters of the line (the slope and intercept)."
  },
  {
    "objectID": "posts/2023-03-02-multivariate/index.html#anova",
    "href": "posts/2023-03-02-multivariate/index.html#anova",
    "title": "A small tour of multivariate analysis",
    "section": "ANOVA",
    "text": "ANOVA\nAnalysis of variance is very closely related to regression. It also works by minimizing sums of squares, but it asks a different question.\nDoes the data better fit a model with one group (one regression line?) or multiple groups (multiple regression lines, one for each group)?\nAnother way to state ANOVA is - is at least one of these groups different than the others?\nIf we want to know whether species are different in sepal length, then we need to have lm fit the model by species. We do this like so:\n\nlm.fit <- with(iris, lm(Sepal.Length ~ Species))\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sepal.Length ~ Species)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6880 -0.3285 -0.0060  0.3120  1.3120 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         5.0060     0.0728  68.762  < 2e-16 ***\nSpeciesversicolor   0.9300     0.1030   9.033 8.77e-16 ***\nSpeciesvirginica    1.5820     0.1030  15.366  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5148 on 147 degrees of freedom\nMultiple R-squared:  0.6187,    Adjusted R-squared:  0.6135 \nF-statistic: 119.3 on 2 and 147 DF,  p-value: < 2.2e-16\n\n\nNotice that now have more parameters estimated. You can specify which parameter values and contrasts you want displayed. Often we just want an ANOVA table:\n\nanova(lm.fit)\n\nAnalysis of Variance Table\n\nResponse: Sepal.Length\n           Df Sum Sq Mean Sq F value    Pr(>F)    \nSpecies     2 63.212  31.606  119.26 < 2.2e-16 ***\nResiduals 147 38.956   0.265                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe can see that species are significantly different in sepal length. Can you make a plot that shows this?"
  },
  {
    "objectID": "posts/2023-03-02-multivariate/index.html#ancova",
    "href": "posts/2023-03-02-multivariate/index.html#ancova",
    "title": "A small tour of multivariate analysis",
    "section": "ANCOVA",
    "text": "ANCOVA\nThere are many forms of regession and ANOVA. For example, if you want to see if the relationship between Sepal.Length and Sepal.Width differs by species, you woul do an ANCOVA (analysis of covariance):\n\nlm.fit <- with(iris, lm(Sepal.Width ~ Sepal.Length + Species))\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sepal.Width ~ Sepal.Length + Species)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.95096 -0.16522  0.00171  0.18416  0.72918 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        1.67650    0.23536   7.123 4.46e-11 ***\nSepal.Length       0.34988    0.04630   7.557 4.19e-12 ***\nSpeciesversicolor -0.98339    0.07207 -13.644  < 2e-16 ***\nSpeciesvirginica  -1.00751    0.09331 -10.798  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.289 on 146 degrees of freedom\nMultiple R-squared:  0.5693,    Adjusted R-squared:  0.5604 \nF-statistic: 64.32 on 3 and 146 DF,  p-value: < 2.2e-16\n\n\nWhich would fit separate Y-intercepts for each species."
  },
  {
    "objectID": "posts/2023-03-02-multivariate/index.html#pca-loadings",
    "href": "posts/2023-03-02-multivariate/index.html#pca-loadings",
    "title": "A small tour of multivariate analysis",
    "section": "PCA Loadings",
    "text": "PCA Loadings\nSome things to look for in PC analysis: The loadings of the variables on the PC axes show how much each variable is correlated with that PC axis. The magnitude of the loading indicates how strong the correlation is, and the sign indicates the direction. The sign of the loading is only informative if variables load with different signs on the same PC axis. For example if variable A and B load positively with PC 2, and variable C loads negatively, this is often interpreted as varying along PC2 in an increasing direction indicating larger A and B but smaller C. In a morphological analysis, the first PC axis often loads positively and nearly equally on all variables, and is therefore considered to indicate size. PC1 also typically explains a large fraction of the variation.\nThe amount of variation each PC axis explains is called the proportion of variance explained. It is usually expressed as a percent or a fraction. It is not uncommon in morphological analysis for PC1 to explain 90% of the variation in the data.\nIt is important to note, however, that the amount of variation does not necessarily indicate it’s importance. Many ecological associations or functionally significant variation is reflected in shape variation, which as we said may be only 10% of the variation. However, this might be very functionally relevant. Size may vary a lot, but it might be whether or not you have very long legs relative to your size that tells us if you are a good runner. Long legs (in an absolute sense) may not make you a great runner if you are actually huge in size, so that relative to your body length, your legs are actually relatively short. So one thing to keep in mind is that you often will use only 3 axes, even though you have 10 or more variables. If you have managed to capture 90 or 95% of the variation with the first three variables (sometimes even more), you’re probably in great shape. It’s a tradeoff between keeping the analysis and interpretation manageable, and keeping all the variation in the data. Usually the minor axes have less than 1% of the variation, and are usually not interesting even if you were to keep them. Anyway, to conclude this paragraph, you may want to do a PC analysis on the data with size included, and then do a second analysis on the size-adjusted data (shape). Another strategy is to do a PC analysis on the data with size, and then leave out PC1 in downstream analyses of “shape”."
  },
  {
    "objectID": "posts/2023-01-17-the-big-picture/index.html#data-science",
    "href": "posts/2023-01-17-the-big-picture/index.html#data-science",
    "title": "Introduction and The Big Idea",
    "section": "Data Science",
    "text": "Data Science\nFrom R for Data Science 2e by Hadley Wickam, Garrett Grolemund, and Mine Çetinkaya-Rundel\n\n\n\nQuestion -> Gather Data -> Design\nObserve -> Record Data -> Data Table\nDocument -> Comment (annotate)\nProject -> Version Control -> Share"
  },
  {
    "objectID": "posts/2023-01-17-the-big-picture/index.html#learning-r---a-first-session",
    "href": "posts/2023-01-17-the-big-picture/index.html#learning-r---a-first-session",
    "title": "Introduction and The Big Idea",
    "section": "Learning R - A first session",
    "text": "Learning R - A first session\n\n\nThink about how R works as you try out commands\nhttps://www.r-project.org Go to manuals, click on An Introduction to R\nFollow Section 2.1\ninput -> R -> output\nWhat came out? What does it tell you about the rules R follows?\nComputers only do Exactly what you tell them to do\nJump to Appendix A - letʻs try to understand some rules of R together"
  },
  {
    "objectID": "posts/2023-01-17-the-big-picture/index.html#goals",
    "href": "posts/2023-01-17-the-big-picture/index.html#goals",
    "title": "Introduction and The Big Idea",
    "section": "Goals",
    "text": "Goals\n\nCreativity\nAccuracy\nAuthority\nRepeatability\nCommunication\n\nEase of dissemination to other researchers\nShow me, donʻt tell me"
  },
  {
    "objectID": "posts/2023-01-17-the-big-picture/index.html#tools",
    "href": "posts/2023-01-17-the-big-picture/index.html#tools",
    "title": "Introduction and The Big Idea",
    "section": "Tools",
    "text": "Tools\n\n\nOur senses\nNotebooks and Pens\nExcel\nCommand Line (UNIX/ Windows/ File Commands in your operating system)\nR\nGit/GitHub\nQuarto/Rmarkdown\nand many more…"
  },
  {
    "objectID": "posts/2023-01-17-the-big-picture/index.html#data-science-1",
    "href": "posts/2023-01-17-the-big-picture/index.html#data-science-1",
    "title": "Introduction and The Big Idea",
    "section": "Data Science",
    "text": "Data Science\n\n\n\n\n\nNeed\nTools\n\n\n\n\nObserve -> Record Data -> Data Table\nNotebooks\n\n\nDocument -> Comment (annotate)\nR\n\n\nProject -> Version Control -> Share\nGit/GitHub\n\n\nCommunicate\nR/ Quarto"
  },
  {
    "objectID": "posts/2023-01-17-the-big-picture/index.html#software",
    "href": "posts/2023-01-17-the-big-picture/index.html#software",
    "title": "Introduction and The Big Idea",
    "section": "Software",
    "text": "Software\n\nDo you have R installed and working? (R studio is optional)\nFor a longer walk-through here is another resource: Introduction to R/Rstudio\nGit/GitHub: Please Install. Letʻs follow Introduction to Git/GitHub"
  },
  {
    "objectID": "posts/2023-01-31-reading-data/index.html",
    "href": "posts/2023-01-31-reading-data/index.html",
    "title": "Data IO",
    "section": "",
    "text": "Material for this lecture was borrowed and adopted from\n\nhttp://rafalab.dfci.harvard.edu/dsbook/importing-data.html\nhttps://www.stephaniehicks.com/jhustatcomputing2022"
  },
  {
    "objectID": "posts/2023-01-31-reading-data/index.html#get-the-files-onto-your-computer",
    "href": "posts/2023-01-31-reading-data/index.html#get-the-files-onto-your-computer",
    "title": "Data IO",
    "section": "Get the files onto your computer",
    "text": "Get the files onto your computer\nFirst, get the data files from the rcassdatadata GitHub repo. Navigate to within the rclassdata folder on your computer and execute:\n\n\nTerminal\n\ngit pull origin main\n\nYou should probably make a R working directory for your class work. Maybe call it Rclass? Then move or drag rclassdata within it.\nNext, from within R, check which working directory you are in. You should be in your Rclass folder. If you are not, use setwd() to get there.\n\ngetwd()\nsetwd(\"~/Rclass\")  # my folder is at the top level of my user directory\n\nCopy the 2023-01-31-DataIO folder into Rclass to work for today."
  },
  {
    "objectID": "posts/2023-01-31-reading-data/index.html#read.csv",
    "href": "posts/2023-01-31-reading-data/index.html#read.csv",
    "title": "Data IO",
    "section": "read.csv",
    "text": "read.csv\nGetting the file in is easy. If it is in csv format, you just use:\n\nread.csv(\"anolisSSD.csv\")  # look for the file in the Data directory\n\nThis is an Anolis lizard sexual size dimorphism dataset. It has values of dimorphism by species for different ecomorphs, or microhabitat specialists.\nTo save the data, give it a name and save it:\n\nanolis <- read.csv(\"anolisSSD.csv\")\n\nIt is a good practice to always check that the data were read in properly. If it is a large file, you’ll want to at least check the beginning and end were read in properly:\n\nhead(anolis)\n\n  species   logSSD    ecomorph\n1      oc -0.00512        twig\n2      eq  0.08454 crown-giant\n3      co  0.24703 trunk-crown\n4     aln  0.24837 trunk-crown\n5      ol  0.09844  grass-bush\n6      in  0.06137        twig\n\ntail(anolis)\n\n   species  logSSD     ecomorph\n18      cr 0.39796 trunk-ground\n19      st 0.15737  trunk-crown\n20      cy 0.26024 trunk-ground\n21     alu 0.08216   grass-bush\n22      lo 0.13108        trunk\n23      an 0.13547         twig\n\n\nVoila! Now you can plot, take the mean, etc. Which prints out the first six and last six lines of the file.\nNow try reading in anolisSSDsemicolon.csv what did you get? Try reading it in with read.table() - check out the help page.\nR can read in many other formats as well, including database formats, excel native format (although it is easier in practice to save as .csv), fixed width formats, and scanning lines. For more information see the R manual “R Data Import/Export” which you can get from help.start() or at http://www.r-project.org."
  },
  {
    "objectID": "posts/2023-01-31-reading-data/index.html#input-files-generated-by-data-loggers",
    "href": "posts/2023-01-31-reading-data/index.html#input-files-generated-by-data-loggers",
    "title": "Data IO",
    "section": "Input files generated by data loggers",
    "text": "Input files generated by data loggers\nFiles that are generated by computer, even if they are not separated by commas (.csv) are not too bad to deal with. Take, for example, the file format generated from our hand-held Ocean Optics specroradiometer. It is very regular in structure, and we have tons of data files, so it is well worth the programming effort to code a script for automatic file input.\nFirst, you can open the file below in a text editor. If you’d rather open it in R, you can use:\n\nreadLines(\"20070725_01forirr.txt\")\n\nNotice that there is a very large header, in fact the first 17 lines. Notice also that the last line will cause a problem. Also, the delimiter in this file is tab (backslash t).\n\ntemp <- readLines(\"20070725_01forirr.txt\")\nhead(temp)\ntail(temp)\n\nWe can solve these issues using the skip and the comment.char arguments of read.table to ignore both types of lines, reading in only the “good stuff”. Also, the default delimiter in this function is the tab:\n\ndat <- read.table(file=\"20070725_01forirr.txt\", skip=17, comment.char=\">\")\nnames(dat) <- c(\"lambda\", \"intensity\")\nhead(dat)\n\n  lambda intensity\n1 177.33         0\n2 177.55         0\n3 177.77         0\n4 177.99         0\n5 178.21         0\n6 178.43         0\n\ntail(dat)\n\n     lambda intensity\n3643 888.21   0.29491\n3644 888.38   0.31306\n3645 888.54   0.28153\n3646 888.71   0.28245\n3647 888.87   0.18988\n3648 889.04   0.18988\n\n\nThe file produces (useless) rows of data outside of the range of accuracy of the spectraradiometer. We can get rid of these by subsetting the data, selecting only the range 300-750nm:\n\ndat <- dat[dat$lambda >= 300, ]  # cut off rows below 300nm\ndat <- dat[dat$lambda <= 750, ]  #cut off rows above 750nm\n\nOr do both at once:\n\ndat <- dat[dat$lambda >= 300 & dat$lambda <= 750,]\n\nIf we are going to be doing this subsetting over and over, we might want to save this as an index vector which tells us the position of the rows of data we want to keep in the dataframe (don’t worry, we’ll cover this again in the workhorse functions chapter).\n\noo <- dat$lambda >= 300 & dat$lambda <= 750\ndat <- dat[oo, ]   # same as longer version above\n\nWe can now save the cleaned up version of the irradiance data:\n\nwrite.csv(dat, \"20070725_01forirr.csv\")"
  },
  {
    "objectID": "posts/2023-01-31-reading-data/index.html#human-error",
    "href": "posts/2023-01-31-reading-data/index.html#human-error",
    "title": "Data IO",
    "section": "Human Error",
    "text": "Human Error\nIt is very easy to make a typo, and humans are really bad at catching our own typos in real time. Right? They creep in despite our best efforts. This is why even though it is possible to measure specimens and enter the numbers directly into the computer, itʻs not something I would do.\nI always write my measurements down into a notebook or a datasheet using pencil and paper. This has saved me many errors in three ways (1) sometimes my brain is still processing what I just wrote and I will catch a typo. (2) I write my data in rows for specimens and columns for the different measurements. As the dataset builds, it is easy to notice errors if some number is really off. (3) If you have any doubts, you can quickly look over your page and re-measure anything suspicous. If you have the person-power, you can also have one person taking the measurements and calling them out, and another writing them down and repeating them back. It is a very effective way to check on the spot.\nI had my first post-college job at an insurance rating board. This is a business that deals with reams and reams of data. We did work a lot with Fortran code and spreadsheets, but surprisingly, some of it does actually have to get manually checked. The protocol was simple, a paper printout of the old version was put next to the new version and a person went along with a ruler, literally putting a check mark after verifying the number. A second check was done as well, and then I finally understood the meaning of “double checking” 😝. When I got to grad school, it was eye-opening to find that checking any personʻs data entry was a rare practice 😲. Please, whenever possible check your data entry. So here are some tips:\n\nRecord data in a notebook (paper and pencil). It serves as a permanent record.\nWrite it down in a data table format in an order that minimizes error. For example, if it is convenient to take five measurements in a particular order, then organize your table that way and always take the measurements in the same order.\nOrganize your spreadsheet to mirror the hand-written data. This will minimize data entry errors.\nHave another person check your data entry against the notebook."
  },
  {
    "objectID": "posts/2023-01-31-reading-data/index.html#organizing-your-spreadhseet",
    "href": "posts/2023-01-31-reading-data/index.html#organizing-your-spreadhseet",
    "title": "Data IO",
    "section": "Organizing your spreadhseet",
    "text": "Organizing your spreadhseet\nWhen it comes time to enter your data in a spreadsheet, there are many things you can do to improve organization. Below is a summary of the recommendations made in paper by Karl Broman and Kara Woo (Broman and Woo 2018).\n\nBe Consistent - Have a plan before you start entering data. Be consistent and stick to it.\nChoose Good Names for Things - You want the names you pick for objects, files, and directories to be memorable, easy to spell, descriptive, but concise. This is actually a hard balance to achieve and it does require time and thought.\n\nOne important rule to follow is do not use spaces, use underscores _ or dashes instead -.\nAlso, avoid symbols; stick to letters and numbers.\n\nWrite Dates as YYYY-MM-DD - To avoid confusion, we strongly recommend using this global ISO 8601 standard.\nNo Empty Cells - Fill in all cells and use some common code for missing data.\nPut Just One Thing in a Cell - It is better to add columns to store the extra information rather than having more than one piece of information in one cell.\nMake It a Rectangle - The spreadsheet should be a rectangle.\nCreate a Data Dictionary - If you need to explain things, such as what the columns are or what the labels used for categorical variables are, do this in a separate file. This is an excellent use for a README.md file\nNo Calculations in the Raw Data Files - Excel permits you to perform calculations. Do not make this part of your spreadsheet. Code for calculations should be in a script.\nDo Not Use Font Color or Highlighting as Data - Most import functions are not able to import this information. Encode this information as a variable (a “comment” column) instead.\nMake Backups - Make regular backups of your data.\nUse Data Validation to Avoid Errors - Leverage the tools in your spreadsheet software so that the process is as error-free and repetitive-stress-injury-free as possible. Think of checks you can do for “reality checks”.\nSave the Data as Text Files - Save files for sharing in comma or tab delimited format. An unambiguous text format is the best for archiving your data."
  },
  {
    "objectID": "posts/2023-01-31-reading-data/index.html#text-versus-binary-files",
    "href": "posts/2023-01-31-reading-data/index.html#text-versus-binary-files",
    "title": "Data IO",
    "section": "Text versus binary files",
    "text": "Text versus binary files\nFor data science purposes, files can generally be classified into two categories: text files (also known as ASCII files) and binary files. You have already worked with text files. All your R scripts are text files and so are the Quarto files used to create this website. The .csv tables you have read are also text files. One big advantage of these files is that we can easily “look” at them using a plain text editor, without having to purchase any kind of special software.\nAny text editor can be used to examine a text file, including freely available editors such as R, RStudio, Atom, Notepad, TextEdit, vi, emacs, nano, and pico. However, if you try to open, say, an Excel xls file, jpg or png file, you will not be able to see anything immediately useful. These are binary files. Excel files are actually compressed folders with several text files inside. But the main distinction here is that text files can be easily examined.\nAlthough R includes tools for reading widely used binary files, such as xls files, in general you will want to find data sets stored in text files. If necessary, use the proprietary software to export the data into text files and go from there.\n\n\n\n\n\n\nWarning\n\n\n\nThe problem with using propietary formats for data management is that htey are poor formats for achival purposes. These formats may change or they donʻt copy well, so that eventually you can no longer open them.\n\n\nSimilarly, when sharing data you want to make it available as text files as long as storage is not an issue (binary files are much more efficient at saving space on your disk). In general, plain-text formats make it easier to share data since commercial software is not required for working with the data, and they are more reliable.\nTechnically, html and xml files are text files too, but they have complicated tags around the information. In the Data Wrangling part of the book we learn to extract data from more complex text files such as html files."
  },
  {
    "objectID": "posts/2023-01-31-reading-data/index.html#unicode-versus-ascii",
    "href": "posts/2023-01-31-reading-data/index.html#unicode-versus-ascii",
    "title": "Data IO",
    "section": "Unicode versus ASCII",
    "text": "Unicode versus ASCII\nA pitfall in data science is assuming a file is an ASCII text file when, in fact, it is something else that can look a lot like an ASCII text file, for example, a Unicode text file.\nTo understand the difference between these, remember that everything on a computer needs to eventually be converted to 0s and 1s (binary format). ASCII is an encoding that maps bits to characters that are easier for humans to read.\n\n\n\n\n\n\nBinary data can take on only two values - 0 or 1\n\n\n\n\na bit is the smallest unit, a single binary character (0 or 1).\na byte is eight bits.\na megabyte or MB is one million bytes.\na gigabyte or GB is one billion bytes.\nYou can roughly calculate the size of your data by the numbers of bytes per each observation.\n\n\n\nASCII uses 7 bits – seven variables that can be either 0 or 1 – which results in 27 = 128 unique items, enough to encode all the characters on an English language keyboard (all characters, numbers, and symbols, Figure 1, Figure 2). However, we need to expand the possibilities if we want to include support for other languages or additional characters.\n\n\n\nFigure 1: Binary encodings in ACII, with some of the most used escape sequences like \\t for tab and \\n for new line.\n\n\n\n\n\nFigure 2: Some examples of ASCII encodings for A-E and a-e.\n\n\nFor this reason, a new encoding, using more than 7 bits, was defined: Unicode. When using Unicode, one can chose between 8, 16, and 32 bits abbreviated UTF-8, UTF-16, and UTF-32 respectively. RStudio actually defaults to UTF-8 encoding.\nAlthough we do not go into the details of how to deal with the different encodings here, it is important that you know these different encodings exist so that you can better diagnose a problem if you encounter it. One way problems manifest themselves is when you see “weird looking” characters you were not expecting.\nMany plain text editors (Atom, Sublime, TextWrangler, Notepad++) will detect encodings and tell you what they are and may also convert between them. Also from the command line the file command will reveal the encoding (should also work on Windows if you have git installed:\n\n\nTerminal\n\nfile filename\n\nThis StackOverflow discussion is an example: https://stackoverflow.com/questions/18789330/r-on-windows-character-encoding-hell."
  },
  {
    "objectID": "posts/2023-01-31-reading-data/index.html#line-endings",
    "href": "posts/2023-01-31-reading-data/index.html#line-endings",
    "title": "Data IO",
    "section": "Line endings",
    "text": "Line endings\nOne last potential headache is the character used for line endings. The line ending is the invisible (to us) character that is added to your file when you press the return key.\nIt is all one stream of informatin to the computer, but the computer will interpret the information as a new line when it detects one of these characters. When software is provided a file with an unexpected line ending, it is not able to properly detect the lines of information (maybe it sees only one huge line). See?\nThere are two types of line endings in use today:\n\nOn UNIX and MacOS, text file line-endings are terminated with a newline character (ASCII 0x0a, represented by the \\n escape sequence in most languages), also referred to as a linefeed (LF).\nOn Windows, line-endings are terminated with a combination of a carriage return (ASCII 0x0d or \\r) and a newline(\\n), also referred to as CR/LF.\n\nIf your computer complains about line endings, the easiest thing to do is to open it in one of the good plain text editors and save it with the line endings it is expecting (usually as LF instead of CR/LF)."
  },
  {
    "objectID": "posts/2023-01-26-reproducible-research/index.html",
    "href": "posts/2023-01-26-reproducible-research/index.html",
    "title": "Reproducible Research",
    "section": "",
    "text": "The shocking assertion will be that most statistics in most scientific papers has errors. —Charles Geyer\n\n\nPre-lecture materials\n\nRead ahead\n\n\n\n\n\n\nRead ahead\n\n\n\nBefore class, you can prepare by reading the following materials:\n\nStatistical programming, Small mistakes, big impacts by Simon Schwab and Leonhard Held\nReproducible Research: A Retrospective by Roger Peng and Stephanie Hicks\n\n\n\n\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\n\nhttp://users.stat.umn.edu/~geyer/Sweave/\nhttp://users.stat.umn.edu/~geyer/repro.pdf\nhttps://rdpeng.github.io/Biostat776\nReproducible Research: A Retrospective by Roger Peng and Stephanie Hicks\n\n\n\n\nLearning objectives\n\n\n\n\n\n\nLearning objectives\n\n\n\nAt the end of this lesson you will:\n\nKnow the difference between replication and reproducibility\nIdentify valid reasons why replication and/or reproducibility is not always possible\nIdentify the type of reproducibility\nIdentify key components to enable reproducible data analyses\n\n\n\n\n\nIntroduction\nFrom a young age, we have learned that scientific conclusions should be reproducible. After all, isnʻt that what the methods section is for? We are taught to write methods sections so that any scientist could, in theory, repeat the experiment with the idea that if the phenomenon is true, they should obtain comparable results and more often than not should come to the same conclusions.\nBut how repeatable is modern science? Many experiments are now so complex and so expensive that repeating them is not practical. However, it is even worse than that. As datasets get larger and analyses become ever more complex, there is a growing concern that even given the data, we still cannot necessarily repeat the analysis. This is called “the reproducbility crisis”.\nRecently, there has been a lot of discussion of reproducibility in the media and in the scientific literature. The journal Science had a special issue on reproducibility and data replication.\n\nhttps://www.science.org/toc/science/334/6060\n\nTake for example a recent study by the Crowdsourced Replication Initiative (2022). It was a massive effort by 166 coauthors published in PNAS to test repeatability:\n\n73 research teams from around the world analyzed the same social science data.\nThey investigated the same hypothesis: that more immigration will reduce public support for government provision of social policies.\nTogether they fit 1261 statistical models and came to widely varying concluisons.\nA meta-analysis of the results by the PIs could not explain the variation in results. Even after accounting for the choices made by the research teams in designing their statistical tests, 95% of the total variation remained unexplained.\nThe authors claim that “a hidden universe of uncertainty remains.”\n\n\n\n\n\n\nSource: Breznau et al., 2022\nThis should be very disturbing. It was very disturbing to me! Greyer notes that the meta-analysis did not investigate how much of the variability of results was due to outright error. He furthermore notes that while the meta-analysis was done in a reproducibly, the original 73 analyses were not. What does he mean?\n\n\nSome of the issues from a statisticianʻs perspective\nGreyer provides nine ideas worth considering:\n\nMost scientific papers that need statistics have conclusions that are not actually supported by the statistical calculations done, because of\n\nmathematical or computational error,\nstatistical procedures inappropriate for the data, or\nstatistical procedures that do not lead to the inferences claimed.\n\nGood computing practices — version control, well thought out testing, code reviews, literate programming — are essential to correct computing.\nFailure to do all calculations from raw data to conclusions (every number or figure shown in a paper) in a way that is fully reproducible and available in a permanent public repository is, by itself, a questionable research practice.\nFailure to do statistics as if it could have been pre-registered is a questionable research practice.\nJournals that use P < 0.05 as a criterion of publication are not scientific journals (publishing only one side of a story is as unscientific as it is possible to be).\nStatistics should be adequately described, at least in the supplementary material.\nScientific papers whose conclusions depend on nontrivial statistics should have statistical referees, and those referees should be heeded.\nNot all errors are describable by statistics. There is also what physicists call systematic error that is the same in every replication of an experiment. Physicists regularly attempt to quantify this. Others should too.\n\nA reasonable ideal for reproducible research today - Research should be reproducible. Anything in a scientific paper should be reproducible by the reader. - Whatever may have been the case in low tech days, this ideal has long gone. Much scientific research in recent years is too complicated and the published details to scanty for anyone to reproduce it. - The lack of detail is not entirely the author’s fault. Journals have severe page pressure and no room for full explanations. - For many years, the only hope of reproducibility is old-fashioned person-to-person contact. Write the authors, ask for data, code, whatever. Some authors help, some don’t. If the authors are not cooperative, tough. - Even cooperative authors may be unable to help. If too much time has gone by and their archiving was not systematic enough and if their software was unportable, there may be no way to recreate the analysis. - Fortunately, the internet comes to the rescue. No page pressure there! - Nowadays, many scientific papers also point to supplementary materials on the internet. Data, computer programs, whatever should be there, permanently. Ideally with a permanent Document Identifier or DOI. There are complaints that many Supplmentary Materials are incomprehensible, but that can be improved with practices of reproducible reserach.\nTherefore, at the very least scientists should use in their statistical programming - version control, - software testing, - code reviews, - literate programming, and - all data and code available in a permanent public repository.\nSome journals have specific policies to promote reproducibility in manuscripts that are published in their journals. For example, the Journal of American Statistical Association (JASA) requires authors to submit their code and data to reproduce their analyses and a set of Associate Editors of Reproducibility review those materials as part of the review process:\n\nhttps://jasa-acs.github.io/repro-guide\n\n\n\nRecommendations\nDiscuss Table 1\n\nAuthors and Readers\nIt is important to realize that there are multiple players when you talk about reproducibility–there are different types of parties that have different types of interests. There are authors who produce research and they want to make their research reproducible. There are also readers of research and they want to reproduce that work. Everyone needs tools to make their lives easier.\nOne current challenge is that authors of research have to undergo considerable effort to make their results available to a wide audience.\n\nPublishing data and code today is not necessarily a trivial task. Although there are a number of resources available now, that were not available even five years ago, it is still a bit of a challenge to get things out on the web (or at least distributed widely).\nResources like GitHub, kipoi, and RPubs and various data repositories have made a big difference, but there is still a ways to go with respect to building up the public reproducibility infrastructure.\n\nFurthermore, even when data and code are available, readers often have to download the data, download the code, and then they have to piece everything together, usually by hand. It’s not always an easy task to put the data and code together.\n\nReaders may not have the same computational resources that the original authors did.\nIf the original authors used an enormous computing cluster, for example, to do their analysis, the readers may not have that same enormous computing cluster at their disposal. It may be difficult for readers to reproduce the same results.\n\nGenerally, the toolbox for doing reproducible research is small, although it’s definitely growing.\n\nIn practice, authors often just throw things up on the web. There are journals and supplementary materials, but they are famously disorganized.\nThere are only a few central databases that authors can take advantage of to post their data and make it available. So if you are working in a field that has a central database that everyone uses, that is great. If you are not, then you have to assemble your own resources.\n\n\n\n\n\n\n\nSummary\n\n\n\n\nThe process of conducting and disseminating research can be depicted as a “data science pipeline”\nReaders and consumers of data science research are typically not privy to the details of the data science pipeline\nOne view of reproducibility is that it gives research consumers partial access to the raw pipeline elements.\n\n\n\n\n\n\nPost-lecture materials\n\nFinal Questions\nHere are some post-lecture questions to help you think about the material discussed.\n\n\n\n\n\n\nQuestions\n\n\n\n\nWhy can replication be difficult to achieve? Why is reproducibility a reasonable minimum standard when replication is not possible?\nWhat is needed to reproduce the results of a data analysis?\n\n\n\n\n\nAdditional Resources\n\n\n\n\n\n\nTip\n\n\n\n\nReproducibility and Error by Charles J. Geyer"
  },
  {
    "objectID": "posts/2023-02-16-plotting-systems/index.html",
    "href": "posts/2023-02-16-plotting-systems/index.html",
    "title": "Plotting Systems",
    "section": "",
    "text": "The data may not contain the answer. And, if you torture the data long enough, it will tell you anything. —John W. Tukey\n#✏️"
  },
  {
    "objectID": "posts/2023-02-16-plotting-systems/index.html#the-base-plotting-system",
    "href": "posts/2023-02-16-plotting-systems/index.html#the-base-plotting-system",
    "title": "Plotting Systems",
    "section": "The Base Plotting System",
    "text": "The Base Plotting System\nThe base plotting system is the original plotting system for R. The basic model is sometimes referred to as the “artist’s palette” model, because you start with a blank canvas and add to it, element by element.\nWe begin with an R plotting function that creates a new plot window typically the plot() function, and we can add annotations to the plot with functions such as (text, lines, points, axis).\nIf you get an error\nError in plot.xy(xy.coords(x, y), type = type, ...) : \n  plot.new has not been called yet\nIt is probably because you tried to add an annotation before you created a plot first.\nThe base plotting system is often the most convenient plotting system to because it mirrors the cartesian coordinate (X,Y) syntax that we sometimes think of when building plots and analyzing data.\nIt is also the most customizablbe because there are many options that users can specify. See the help page for ?plot.default and ?par for graphical parameters that once can set. It does take some learning, but it is possible to make multi-figure plots while precisely controlling their placement, margins, and any annotations including points, lines, text etc.\nThe plot system is also very useful at the very beginning of a data analysis When we donʻt precisely know what we want to plot. We can start by “throwing some data on the page” and then slowly modifying or adding more information to it as our thought process evolves.\n\n\n\n\n\n\nExample\n\n\n\nWe might look at a simple scatterplot and then decide to add a linear regression line or a smoother to it to highlight the trends.\n\ndata(airquality)\nwith(airquality, {\n        plot(Temp, Ozone)\n        lines(loess.smooth(Temp, Ozone))\n})\n\n\n\n\nScatterplot with loess curve\n\n\n\n\n\n\nIn the code above:\n\nThe plot() function creates the initial plot and draws the points on the canvas.\nThe lines function is used to annotate or add to the plot (in this case it adds a loess smoother to the scatterplot).\nR has many other types of smoothing functions as well.\n\nNext, we use the plot() function to draw the points on the scatterplot and then use the main argument to add a main title to the plot.\n\ndata(airquality)\nwith(airquality, {\n        plot(Temp, Ozone, main = \"my plot\")\n        lines(loess.smooth(Temp, Ozone))\n})\n\n\n\n\nScatterplot with loess curve\n\n\n\n\nOne downside with constructing base plots is that annotations can only be added. If your annotations are crashing, or you run out of room, etc., you will have to rerun the code starting from plot().\nAnd while the base plotting system is nice in that it gives you the flexibility to specify these kinds of details to painstaking accuracy, sometimes it would be nice if the system could just figure it out for you. Thatʻs where lattice and ggplot2 have contributed.\nAnother downside of the customizability of the base plotting system is that it is difficult to describe or translate a plot to others because there is no clear graphical language or grammar. In other words, you cannot paint the format of one onto another plot, unless you design the code (maybe by using the same variable names, etc.) to do it yourself.\nThe only real way to describe what you have done in a base plot is to just list the series of commands/functions that you have executed, which is not a particularly compact way of communicating things. The ggplot2 package has developed a grammar of graphics that make transferring formats easy.\n\n\n\n\n\n\nExample\n\n\n\nAnother typical base plot for a linear regression is constructed with the following code. Here we are using formula representation. Instead of plot(x,y) we use plot(y~x) which is read “plot y as a function of x”. These styles are equivalent, but formula is in the style of regression models.\n\ndata(cars)\n\n## Create the plot / draw canvas\nwith(cars, plot(dist ~ speed))\n\n\n\n\nBase plot with title\n\n\n\n## Fit a linear model and save it\nlm.fit <- with(cars, lm( dist ~ speed))\n\nWe can add annotations as before. This time, to add the regression line to the plot, we use the abline() function which adds straight lines, specified by slope and intercept.\n\nwith(cars, plot(dist ~ speed))\n\n## Add annotations\ntitle(\"Linear model of speed vs. stopping distance\")\nabline(lm.fit, col=\"red\")\ntext(7, 120, paste(\"slope =\", round(coef(lm.fit)[2], digits=2)))\ntext(7, 110, paste(\"intercept =\", round(coef(lm.fit)[1], digits=2)))\n\n\n\n\nUsing text() we added the slope and intercept from the linear model fit. That information is returned by the coef() function. We used round() to make it prettier (8 decimal places is not necessary!)"
  },
  {
    "objectID": "posts/2023-02-16-plotting-systems/index.html#the-lattice-system",
    "href": "posts/2023-02-16-plotting-systems/index.html#the-lattice-system",
    "title": "Plotting Systems",
    "section": "The Lattice System",
    "text": "The Lattice System\nThe lattice plotting system is ideal for visualization of multivariate data, and is implemented in the lattice R package which comes with every installation of R (although it is not loaded by default).\nTo use the lattice plotting functions, you must first load the lattice package.\n\nrequire(lattice)\n\nLoading required package: lattice\n\n\nWith the lattice system, plots are created with a single function call, such as xyplot() or bwplot(), and the plot is delivered in a predefined format.\nThere is no real distinction between functions that create or initiate plots and functions that annotate plots because it all happens at once.\nLattice plots tend to be most useful for conditioning types of plots, i.e. looking at how y changes with x across levels of z.\n\ne.g. these types of plots are useful for looking at multi-dimensional data and often allow you to squeeze a lot of information into a single window or page.\n\nAnother aspect of lattice that makes it different from base plotting is that things like margins and spacing are set automatically. The downside is that it is not very customizable.\n\n\n\n\n\n\nExample\n\n\n\nHere is a lattice plot that looks at the relationship between life expectancy and income and how that relationship varies by region in the United States.\n\nstate <- data.frame(state.x77, region = state.region)\nhead(state)\n\n           Population Income Illiteracy Life.Exp Murder HS.Grad Frost   Area\nAlabama          3615   3624        2.1    69.05   15.1    41.3    20  50708\nAlaska            365   6315        1.5    69.31   11.3    66.7   152 566432\nArizona          2212   4530        1.8    70.55    7.8    58.1    15 113417\nArkansas         2110   3378        1.9    70.66   10.1    39.9    65  51945\nCalifornia      21198   5114        1.1    71.71   10.3    62.6    20 156361\nColorado         2541   4884        0.7    72.06    6.8    63.9   166 103766\n           region\nAlabama     South\nAlaska       West\nArizona      West\nArkansas    South\nCalifornia   West\nColorado     West\n\nxyplot(Life.Exp ~ Income | region, data = state, layout = c(4, 1))\n\n\n\n\nLattice plot\n\n\n\n\n\n\nYou can see that the entire plot was generated by the call to xyplot() and all of the data for the plot were specified to come from the state data frame.\nThe layout specifies four panels —one for each region— within each panel is a scatterplot of life expectancy and income. Note that in this case, layout is columns, rows. Try changing the numbers or leaving out the layout argument and see what happens.\nThe notion of panels comes up a lot with lattice plots because you typically have many panels in a lattice plot (each panel typically represents a factor level, like “region”).\n\n\n\n\n\n\nNote\n\n\n\nDownsides with the lattice system\n\nIt can sometimes be very awkward to specify an entire plot in a single function call (you end up with functions with many many arguments).\nAnnotation in panels in plots is not especially intuitive and can be difficult to explain. In particular, the use of custom panel functions and subscripts can be difficult to wield and requires a lot of trial and error."
  },
  {
    "objectID": "posts/2023-02-16-plotting-systems/index.html#the-ggplot2-system",
    "href": "posts/2023-02-16-plotting-systems/index.html#the-ggplot2-system",
    "title": "Plotting Systems",
    "section": "The ggplot2 System",
    "text": "The ggplot2 System\nThe ggplot2 plotting system attempts to split the difference between base and lattice in a number of ways.\n\n\n\n\n\n\nNote\n\n\n\nTaking cues from lattice, the ggplot2 system automatically deals with spacings, text, titles but also allows you to annotate by “adding” to a plot, with annotations added in layers.\n\n\nThe ggplot2 system is implemented in the ggplot2 package (part of the tidyverse package), which is available from CRAN (it does not come with R).\nYou can install it from CRAN via\n\ninstall.packages(\"ggplot2\")\n\nand then load it into R.\n\nrequire(ggplot2)\n\nLoading required package: ggplot2\n\n\nSuperficially, the ggplot2 functions are similar to lattice, but the system is generally easier and more intuitive to use once you learn the syntax.\nThe defaults used in ggplot2 make many choices for you, but you can still customize plots.\n\n\n\n\n\n\nExample\n\n\n\nA typical plot with the ggplot2 package looks like the code below.\nNote the use of the pipe operator %>% from the magrittr package in the tidyverse, which sends the dataframe mpg as input into the function ggplot. People love the pipe operator because you can just pass output along from one function to the next (as long as the function is written for piping). All of the tidyverse allows piping, and some base R functions do as well.\n\nlibrary(tidyverse)\ndata(mpg)\nmpg %>%\n  ggplot(aes(displ, hwy)) + \n  geom_point()\n\n\n\n\nggplot2 plot\n\n\n\n\n\n\nIn ggplot, elements of the plot are specified as aesthetics, and layers can be added onto the plot with + anotherfunction(). Try running the first part, and then running the whole thing together. Then try adding + geom_smooth()\nThere are additional functions in ggplot2 that allow you to make arbitrarily sophisticated plots.\nWe will discuss more about this in the next lecture."
  },
  {
    "objectID": "posts/2023-02-14-tidying-exploring-data/index.html",
    "href": "posts/2023-02-14-tidying-exploring-data/index.html",
    "title": "Tidying and Exploring Data",
    "section": "",
    "text": "✏️"
  },
  {
    "objectID": "posts/2023-02-14-tidying-exploring-data/index.html#acknowledgements",
    "href": "posts/2023-02-14-tidying-exploring-data/index.html#acknowledgements",
    "title": "Tidying and Exploring Data",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nMaterial for this lecture was borrowed and adopted from"
  },
  {
    "objectID": "posts/2023-02-14-tidying-exploring-data/index.html#vectors",
    "href": "posts/2023-02-14-tidying-exploring-data/index.html#vectors",
    "title": "Tidying and Exploring Data",
    "section": "Vectors",
    "text": "Vectors\nThe index of a vector is it’s number in the array. Each and every element in any data object has at least one index (if vector, it is one dimensional so it is its position along the vector, if a matrix or data frame, which are two-dimensional, it’s the row and column number, etc.)\nLet’s create a vector:\n\nxx <- c(1, 5, 2, 3, 5)\nxx\n\n[1] 1 5 2 3 5\n\n\nAccess specific values of xx by number:\n\nxx[1]\n\n[1] 1\n\nxx[3]\n\n[1] 2\n\n\nYou can use a function to generate an index. Get the last element (without knowing how many there are) by:\n\nxx[length(xx)]\n\n[1] 5\n\n\nRetrieve multiple elements of xx by using a vector as an argument:\n\nxx[c(1, 3, 4)]\n\n[1] 1 2 3\n\nxx[1:3]\n\n[1] 1 5 2\n\nxx[c(1, length(xx))]  # first and last\n\n[1] 1 5\n\n\nExclude elements by using a negative index:\n\nxx\n\n[1] 1 5 2 3 5\n\nxx[-1]  # exclude first\n\n[1] 5 2 3 5\n\nxx[-2] # exclude second\n\n[1] 1 2 3 5\n\nxx[-(1:3)] # exclude first through third\n\n[1] 3 5\n\nxx[-c(2, 4)] # exclude second and fourth, etc. \n\n[1] 1 2 5\n\n\nUse a logical vector:\n\nxx[ c( T, F, T, F, T) ]  # T is the same as TRUE\n\n[1] 1 2 5\n\n\n\nxx > 2\n\n[1] FALSE  TRUE FALSE  TRUE  TRUE\n\nxx[ xx > 2 ]\n\n[1] 5 3 5\n\nxx > 2 & xx < 5\n\n[1] FALSE FALSE FALSE  TRUE FALSE\n\nxx[ xx>2 & xx<5]\n\n[1] 3\n\n\nSubsetting (picking particular observations out of an R object) is something that you will have to do all the time. It’s worth the time to understand it clearly.\n\nsubset_xx <-  xx[ xx > 2 ]\nsubset_xx2 <- subset(xx, xx>2)  # using subset function\nsubset_xx == subset_xx2   # check if the same\n\n[1] TRUE TRUE TRUE\n\n\nThe subset function is just another way of subsetting by index, just in function form with arguments. It can be more clear to use for dataframes, but it is really a matter of personal preference as you develop your style. Whichever way you go, it is important to be aware of the different ways to achieve the same goals."
  },
  {
    "objectID": "posts/2023-02-14-tidying-exploring-data/index.html#matrices-and-dataframes",
    "href": "posts/2023-02-14-tidying-exploring-data/index.html#matrices-and-dataframes",
    "title": "Tidying and Exploring Data",
    "section": "Matrices and Dataframes",
    "text": "Matrices and Dataframes\nMatrices and dataframes are both rectangular having two dimensions, and are handled very similarly for indexing and subsetting.\nLet’s work with a dataframe that is provided with the geiger package called geospiza. It is a list with a tree and a dataframe. The dataframe contains five morphological measurements for 13 species. First, let’s clear the workspace (or clear and start a new R session):\n\ninstall.packages(\"geiger\")  # if you need to install geiger\n\nGet the built-in dataset this way:\n\nrm(list=ls())\nrequire(geiger)\n\nLoading required package: geiger\n\n\nLoading required package: ape\n\ndata(geospiza)   # load the dataset into the workspace\nls()               # list the objects in the workspace\n\n[1] \"geospiza\"\n\n\nLet’s find out some basic information about this object:\n\nclass(geospiza)\n\n[1] \"list\"\n\nattributes(geospiza)\n\n$names\n[1] \"geospiza.tree\" \"geospiza.data\" \"phy\"           \"dat\"          \n\nstr(geospiza)\n\nList of 4\n $ geospiza.tree:List of 4\n  ..$ edge       : num [1:26, 1:2] 15 16 17 18 19 20 21 22 23 24 ...\n  ..$ edge.length: num [1:26] 0.2974 0.0492 0.0686 0.134 0.1035 ...\n  ..$ Nnode      : int 13\n  ..$ tip.label  : chr [1:14] \"fuliginosa\" \"fortis\" \"magnirostris\" \"conirostris\" ...\n  ..- attr(*, \"class\")= chr \"phylo\"\n $ geospiza.data: num [1:13, 1:5] 4.4 4.35 4.22 4.26 4.24 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:13] \"magnirostris\" \"conirostris\" \"difficilis\" \"scandens\" ...\n  .. ..$ : chr [1:5] \"wingL\" \"tarsusL\" \"culmenL\" \"beakD\" ...\n $ phy          :List of 4\n  ..$ edge       : num [1:26, 1:2] 15 16 17 18 19 20 21 22 23 24 ...\n  ..$ edge.length: num [1:26] 0.2974 0.0492 0.0686 0.134 0.1035 ...\n  ..$ Nnode      : int 13\n  ..$ tip.label  : chr [1:14] \"fuliginosa\" \"fortis\" \"magnirostris\" \"conirostris\" ...\n  ..- attr(*, \"class\")= chr \"phylo\"\n $ dat          : num [1:13, 1:5] 4.4 4.35 4.22 4.26 4.24 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:13] \"magnirostris\" \"conirostris\" \"difficilis\" \"scandens\" ...\n  .. ..$ : chr [1:5] \"wingL\" \"tarsusL\" \"culmenL\" \"beakD\" ...\n\n\nIt is a list with four elements. Here we want the data\n\ngeo <- as.data.frame(geospiza$geospiza.data)\ndim(geo)\n\n[1] 13  5\n\n\nIt is a dataframe with 13 rows and 5 columns. If we want to know all the attributes of geo:\n\nattributes(geo)\n\n$names\n[1] \"wingL\"   \"tarsusL\" \"culmenL\" \"beakD\"   \"gonysW\" \n\n$class\n[1] \"data.frame\"\n\n$row.names\n [1] \"magnirostris\" \"conirostris\"  \"difficilis\"   \"scandens\"     \"fortis\"      \n [6] \"fuliginosa\"   \"pallida\"      \"fusca\"        \"parvulus\"     \"pauper\"      \n[11] \"Pinaroloxias\" \"Platyspiza\"   \"psittacula\"  \n\n\nWe see that it has a “names” attribute, which refers to column names in a dataframe. Typically, the columns of a dataframe are the variables in the dataset. It also has “rownames” which contains the species names (so it does not have a separate column for species names).\nDataframes have two dimensions which we can use to index with: dataframe[row, column].\n\ngeo     # the entire object, same as geo[] or geo[,]\ngeo[c(1, 3), ]   # select the first and third rows, all columns\ngeo[, 3:5]   # all rows, third through fifth columns\ngeo[1, 5]  # first row, fifth column (a single number)\ngeo[1:2, c(3, 1)]  # first and second row, third and first column (2x2 matrix)\ngeo[-c(1:3, 10:13), ]  # everything but the first three and last three rows\ngeo[ 1:3, 5:1]  # first three species, but variables in reverse order\n\nTo prove to ourselves that we can access matrices in the same way, let’s coerce geo to be a matrix:\n\ngeom <- as.matrix( geo ) \nclass(geom)\n\n[1] \"matrix\" \"array\" \n\nclass(geo)\n\n[1] \"data.frame\"\n\ngeo[1,5]  # try a few more from the choices above to test\n\n[1] 2.675983\n\n\nSince geo and geom have row and column names, we can access by name (show that this works for geom too):\n\ngeo[\"pauper\", \"wingL\"]  # row pauper, column wingL\n\n[1] 4.2325\n\ngeo[\"pauper\", ]  # row pauper, all columns \n\n        wingL tarsusL culmenL  beakD gonysW\npauper 4.2325  3.0359   2.187 2.0734 1.9621\n\n\nWe can also use the names (or rownames) attribute if we are lazy. Suppose we wanted all the species which began with “pa”. we could find which position they hold in the dataframe by looking at the rownames, saving them to a vector, and then indexing by them:\n\nsp <- rownames(geo)\nsp                            # a vector of the species names\n\n [1] \"magnirostris\" \"conirostris\"  \"difficilis\"   \"scandens\"     \"fortis\"      \n [6] \"fuliginosa\"   \"pallida\"      \"fusca\"        \"parvulus\"     \"pauper\"      \n[11] \"Pinaroloxias\" \"Platyspiza\"   \"psittacula\"  \n\nsp[c(7,8,10)]     # the ones we want are #7,8, and 10\n\n[1] \"pallida\" \"fusca\"   \"pauper\" \n\ngeo[ sp[c(7,8,10)], ]  # rows 7,8 and 10, same as geo[c(7, 8, 10)]\n\n           wingL  tarsusL  culmenL    beakD   gonysW\npallida 4.265425 3.089450 2.430250 2.016350 1.949125\nfusca   3.975393 2.936536 2.051843 1.191264 1.401186\npauper  4.232500 3.035900 2.187000 2.073400 1.962100\n\n\nOne difference between dataframes and matrices is that Indexing a data frame by a single vector (meaning, no comma separating) selects an entire column. This can be done by name or by number:\n\ngeo[3]   # third column\ngeo[\"culmenL\"]  # same\ngeo[c(3,5)]  # third and fifth column\ngeo[c(\"culmenL\", \"gonysW\")]  # same\n\nProve to yourself that selecting by a single index has a different behavior for matrices (and sometimes produces an error.\n\n\n\n\n\n\nWhy?\n\n\n\n\nBecause internally, a dataframe is actually a list of vectors. Thus a single name or number refers to the column, rather than a coordinate in a cartesian-coordinate-like system.\nHowever, a matrix is actually a vector with breaks in it. So a single number refers to a position along the single vector.\nA single name could work, but only if the individual elements of the matrix have names (like naming the individual elements of a vector).\n\n\n\nAnother difference is that dataframes (and lists below) can be accessed by the $ operator. It means indicates a column within a dataframe, so dataframe$column. This is another way to select by column:\n\ngeo$culmenL\n\n [1] 2.724667 2.654400 2.277183 2.621789 2.407025 2.094971 2.430250 2.051843\n [9] 1.974420 2.187000 2.311100 2.331471 2.259640\n\n\nAn equivalent way to index is by using the subset function. Some people prefer it because you have explicit parameters for what to select and which variables to include. See help page ?subset."
  },
  {
    "objectID": "posts/2023-02-14-tidying-exploring-data/index.html#lists",
    "href": "posts/2023-02-14-tidying-exploring-data/index.html#lists",
    "title": "Tidying and Exploring Data",
    "section": "Lists",
    "text": "Lists\nA list is a vector, except that whereas an ordinary vector has the same type of data (numeric, character, factor) in each slot, a list can have different types in different slots. They are sort of like expandable containers, flexibly accommodating any group of objects that the user wants to keep together.\nThey are accessed by numeric index or by name (if they are named), but they are accessed by double square brackets. Also, you can’t access multiple elements of lists by using vectors of indices:\n\nmylist <- list( vec = 2*1:10, mat = matrix(1:10, nrow=2), cvec = c(\"frogs\", \"birds\"))\nmylist\n\n$vec\n [1]  2  4  6  8 10 12 14 16 18 20\n\n$mat\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    3    5    7    9\n[2,]    2    4    6    8   10\n\n$cvec\n[1] \"frogs\" \"birds\"\n\nmylist[[2]]\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    3    5    7    9\n[2,]    2    4    6    8   10\n\nmylist[[\"vec\"]]\n\n [1]  2  4  6  8 10 12 14 16 18 20\n\n# mylist[[1:3]]  # gives an error if you uncomment it\nmylist$cvec\n\n[1] \"frogs\" \"birds\""
  },
  {
    "objectID": "posts/2023-02-14-tidying-exploring-data/index.html#missing-values",
    "href": "posts/2023-02-14-tidying-exploring-data/index.html#missing-values",
    "title": "Tidying and Exploring Data",
    "section": "Missing Values",
    "text": "Missing Values\nMissing values compared to anything else will return a missing value (so NA == NA returns NA, which is usually not what you want). You must test it with is.na function. You can also test multiple conditions with and (&) and or (|)\n\n!is.na(geo$gonysW) \n\n [1]  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE  TRUE\n[13]  TRUE\n\ngeo[!is.na(geo$gonysW) & geo$wingL > 4, ]  # element by element \"and\"\n\n                wingL  tarsusL  culmenL    beakD   gonysW\nconirostris  4.349867 2.984200 2.654400 2.513800 2.360167\nfortis       4.244008 2.894717 2.407025 2.362658 2.221867\nmagnirostris 4.404200 3.038950 2.724667 2.823767 2.675983\npsittacula   4.235020 3.049120 2.259640 2.230040 2.073940\nplatyspiza   4.419686 3.270543 2.331471 2.347471 2.282443\nscandens     4.261222 2.929033 2.621789 2.144700 2.036944\n\ngeo[!is.na(geo$gonysW) | geo$wingL > 4, ]   # element by element \"or\"\n\n                wingL  tarsusL  culmenL    beakD   gonysW\nconirostris  4.349867 2.984200 2.654400 2.513800 2.360167\ndifficilis   4.224067 2.898917 2.277183 2.011100       NA\nfuliginosa   4.132957 2.806514 2.094971       NA       NA\nfortis       4.244008 2.894717 2.407025 2.362658 2.221867\nmagnirostris 4.404200 3.038950 2.724667 2.823767 2.675983\nparvulus     4.131600 2.973060       NA       NA       NA\npinaroloxias 4.188600 2.980200 2.311100       NA       NA\npauper       4.232500 3.035900 2.187000 2.073400       NA\npsittacula   4.235020 3.049120 2.259640 2.230040 2.073940\npallida      4.265425 3.089450 2.430250 2.016350       NA\nplatyspiza   4.419686 3.270543 2.331471 2.347471 2.282443\nscandens     4.261222 2.929033 2.621789 2.144700 2.036944\n\n!is.na(geo$gonysW) && geo$wingL > 4   # vectorwise \"and\"\n\nWarning in !is.na(geo$gonysW) && geo$wingL > 4: 'length(x) = 13 > 1' in coercion\nto 'logical(1)'\n\nWarning in !is.na(geo$gonysW) && geo$wingL > 4: 'length(x) = 13 > 1' in coercion\nto 'logical(1)'\n\n\n[1] TRUE\n\n\nMatching works on strings also:\n\ngeo[rownames(geo) == \"pauper\",]   # same as   geo[\"pauper\", ]\ngeo[rownames(geo) < \"pauper\",]\n\nThere are even better functions for strings, though. In the expression A %in% B, the %in% operator compares two vectors of strings, and tells us which elements of A are present in B.\n\nnewsp <- c(\"clarkii\", \"pauper\", \"garmani\")\nnewsp[newsp  %in% rownames(geo)]     # which new species are in geo?\n\nWe can define the “without” operator:\n\n\"%w/o%\" <- function(x, y) x[!x %in% y]\nnewsp  %w/o% rownames(geo)   # which new species are not in geo?"
  },
  {
    "objectID": "posts/2023-02-14-tidying-exploring-data/index.html#plot",
    "href": "posts/2023-02-14-tidying-exploring-data/index.html#plot",
    "title": "Tidying and Exploring Data",
    "section": "Plot()",
    "text": "Plot()\nFor a generic x-y plot use plot(). It will also start a graphical device.\nHere we are using the with() function to specify which dataframe to look into for our named variables. We could instead do penguins$body_mass_g, etc.\n\nwith(penguins, plot( body_mass_g, bill_length_mm))\n\n\n\n\nTo add points to an existing plot, use points()\n\nwith(penguins, plot( body_mass_g, bill_length_mm))\nwith(penguins[penguins$species==\"Adelie\",], points( body_mass_g, bill_length_mm, col=\"red\"))\n\n\n\n\nOne could fit linear models, for example, and use lines() to overlay the line on the plot."
  },
  {
    "objectID": "posts/2023-02-14-tidying-exploring-data/index.html#distribution-using-hist-and-density",
    "href": "posts/2023-02-14-tidying-exploring-data/index.html#distribution-using-hist-and-density",
    "title": "Tidying and Exploring Data",
    "section": "Distribution using hist() and density()",
    "text": "Distribution using hist() and density()\nTo see a histogram, use hist(). You can change the breaks and many other features by checking out the help page ?hist\n\nwith(penguins, hist( body_mass_g ))\n\n\n\n\nTo see a density plot use density() to create the density, then plot it.\n\ndens <- with(penguins, density( body_mass_g, na.rm=T ))\nplot(dens)"
  },
  {
    "objectID": "posts/2023-02-21-ggplot2/index.html",
    "href": "posts/2023-02-21-ggplot2/index.html",
    "title": "The ggplot2 package",
    "section": "",
    "text": "For more details see\n\n\n\n\nWonderful Window shopping in the R graph gallery (with code): https://r-graph-gallery.com\nThe “grammar of graphics” explained in Hadley Wickamʻs article: http://vita.had.co.nz/papers/layered-grammar.pdf\nVery gentle intro for beginners: https://posit.cloud/learn/primers/3\nHadley Wickamʻs overview: https://r4ds.had.co.nz/data-visualisation\nCedric Schererʻs Step-by-step tutorial: https://www.cedricscherer.com/2019/08/05/a-ggplot2-tutorial-for-beautiful-plotting-in-r/\nFor in-depth reading, Hadley Wickamʻs ggplot2 book: https://ggplot2-book.org"
  },
  {
    "objectID": "posts/2023-02-21-ggplot2/index.html#basic-components-of-a-ggplot2-plot",
    "href": "posts/2023-02-21-ggplot2/index.html#basic-components-of-a-ggplot2-plot",
    "title": "The ggplot2 package",
    "section": "Basic components of a ggplot2 plot",
    "text": "Basic components of a ggplot2 plot\n\n\n\n\n\n\nKey components\n\n\n\nA ggplot2 plot consists of a number of key components.\n\nData: In the form of a dataframe or tibble, containing all of the data that will be displayed on the plot.\nGeometry: The geometry or geoms define the style of the plot such as scatterplot, barplot, histogram, violin plots, smooth densities, qqplot, boxplot, and others.\nAesthetic mapping: Aesthetic mappings describe how data are mapped to color, size, shape, location, or to legend elements. How we define the mapping depends on what geometry we are using.\n\nNearly all plots drawn with ggplot2 will have the above compoents. In addition you may want to have specify additional elements:\n\nFacets: When used, facets describe how panel plots based on partions of the data should be drawn.\nStatistical Transformations: Or stats are transformations of the data such as log-transformation, binning, quantiles, smoothing.\nScales: Scales are used to indicate which factors are associated with the levels of the aesthetic mapping. Use manual scales to specify each level.\nCoordinate System: ggplot2 will use a default coordinate system drawn from the data, but you can customize the coordinate system in which the locations of the geoms will be drawn\n\n\n\nPlots are built up in layers, with the typical ordering being\n\nPlot the data\nOverlay a summary that reveals the relationship\nAdd metadata and annotation"
  },
  {
    "objectID": "posts/2023-02-21-ggplot2/index.html#legend",
    "href": "posts/2023-02-21-ggplot2/index.html#legend",
    "title": "The ggplot2 package",
    "section": "Legend",
    "text": "Legend\nFor example, we can make changes to the legend title via the scale_color_discrete function():\n\np + geom_point(aes(col=Species), size = 3) + \n  scale_color_discrete(name = \"Iris Varieties\")"
  },
  {
    "objectID": "posts/2023-02-21-ggplot2/index.html#no-legend",
    "href": "posts/2023-02-21-ggplot2/index.html#no-legend",
    "title": "The ggplot2 package",
    "section": "No legend",
    "text": "No legend\nggplot2 automatically adds a legend that maps color to species. To remove the legend we set the geom_point() argument show.legend = FALSE.\n\np + geom_point(aes(col=Species), size = 3, show.legend=FALSE)"
  },
  {
    "objectID": "posts/2023-02-21-ggplot2/index.html#themes",
    "href": "posts/2023-02-21-ggplot2/index.html#themes",
    "title": "The ggplot2 package",
    "section": "Themes",
    "text": "Themes\nThe default theme for ggplot2 uses the gray background with white grid lines.\nIf you don’t like this, you can use the black and white theme by using the theme_bw() function.\nThe theme_bw() function also allows you to set the typeface for the plot, in case you don’t want the default Helvetica. Here we change the typeface to Times.\n\n\n\n\n\n\nNote\n\n\n\nFor things that only make sense globally, use theme(), i.e. theme(legend.position = \"none\"). Two standard appearance themes are included\n\ntheme_gray(): The default theme (gray background)\ntheme_bw(): More stark/plain\n\n\n\n\np + \n  geom_point(aes(color = Species)) + \n  theme_bw(base_family = \"Times\")\n\n\n\n\nModifying the theme for a plot"
  },
  {
    "objectID": "posts/2023-02-21-ggplot2/index.html#ggthemes",
    "href": "posts/2023-02-21-ggplot2/index.html#ggthemes",
    "title": "The ggplot2 package",
    "section": "ggthemes",
    "text": "ggthemes\nThe style of a ggplot2 graph can be changed using the theme functions. Several themes are included as part of the ggplot2 package.\nMany other themes are added by the package ggthemes. Among those are the theme_economist theme that we used. After installing the package, you can change the style by adding a layer like this:\n\nrequire(ggthemes)\n\nLoading required package: ggthemes\n\n\n\nAttaching package: 'ggthemes'\n\n\nThe following object is masked from 'package:cowplot':\n\n    theme_map\n\np + geom_point(aes(col=Species, size = 3, alpha = 1/2)) +\n  scale_color_manual(values=cols) + \n  theme_economist()\n\n\n\n\nYou can see how some of the other themes look by simply changing the function. For instance, you might try the theme_fivethirtyeight() theme instead.\nggrepel\nThe final difference has to do with the position of the labels. In our plot, some of the labels fall on top of each other. The package ggrepel includes a geometry that adds labels while ensuring that they don’t fall on top of each other. We simply change geom_text with geom_text_repel.\n\nrequire(ggrepel)\n\nLoading required package: ggrepel\n\np + geom_point(aes(col=Species, size = 3, alpha = 1/2)) +\n  scale_color_manual(values=cols) + \n  theme_economist() +\n  geom_text_repel(aes(Petal.Length, Petal.Width, label = id))\n\nWarning: ggrepel: 90 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps"
  },
  {
    "objectID": "posts/2023-01-26-intro-quarto/index.html",
    "href": "posts/2023-01-26-intro-quarto/index.html",
    "title": "Literate Statistical Programming and Quarto",
    "section": "",
    "text": "Read ahead\n\n\n\nBefore class, you can prepare by reading the following materials:\n\nhttps://rss.onlinelibrary.wiley.com/doi/epdf/10.1111/1740-9713.01522\nCreating a Website in Quarto quickstart up to and including Render https://quarto.org/docs/websites/\nPublishing to GitHub up to and including Render to docs https://quarto.org/docs/publishing/github-pages.html\n\n\n\n\n\n\nMaterial for this lecture was borrowed and adopted from\n\nhttps://www.stephaniehicks.com/jhustatcomputing2022/posts/2022-09-01-literate-programming/\nhttp://users.stat.umn.edu/~geyer/Sweave/\nhttps://rdpeng.github.io/Biostat776/lecture-literate-statistical-programming.html\nhttps://statsandr.com/blog/tips-and-tricks-in-rstudio-and-r-markdown"
  },
  {
    "objectID": "posts/2023-01-26-intro-quarto/index.html#weaving-and-tangling",
    "href": "posts/2023-01-26-intro-quarto/index.html#weaving-and-tangling",
    "title": "Literate Statistical Programming and Quarto",
    "section": "Weaving and Tangling",
    "text": "Weaving and Tangling\nLiterate programs by themselves are a bit difficult to work with, but they can be processed in two important ways.\nLiterate programs can be weaved to produce human readable documents like PDFs or HTML web pages, and they can tangled to produce machine-readable “documents”, or in other words, machine readable code.\nIn order to use a system like this you need a documentational language, that’s human readable, and you need a programming language that’s machine readable (or can be compiled/interpreted into something that’s machine readable)."
  },
  {
    "objectID": "posts/2023-01-26-intro-quarto/index.html#sweave",
    "href": "posts/2023-01-26-intro-quarto/index.html#sweave",
    "title": "Literate Statistical Programming and Quarto",
    "section": "Sweave",
    "text": "Sweave\nOne of the original literate programming systems in R that was designed to do this was called Sweave written by Friedrich Leisch. Sweave enables users to combine R code with a documentation program called LaTeX. Sweave revolutionized coding, and has become part of the R base code. Leisch is on the R Core Development Team and the BioConductor Project.\nSweave files ends a .Rnw and have R code weaved through the document:\n<<plot1, height=4, width=5, eval=FALSE>>=\ndata(airquality)\nplot(airquality$Ozone ~ airquality$Wind)\n@\nOnce you have created your .Rnw file, Sweave will process the file, executing the R chunks and replacing them with output as appropriate before creating the PDF document.\nSweaveʻs main limitation is that it requires knowledge of LaTeX\n\nLaTeX is very powerful for laying out mathematical equations and fine-tuned control of formatting, but is not a documentation language that is widely used outside of mathematics.\nTherefore, there is a steep learning curve.\nSweave also lacks a lot of features that people find useful like caching, and multiple plots per page and mixing programming languages.\n\nInstead, folks have moved towards using something called knitr, which offers everything Sweave does, plus it extends it to much simpler Markdown documents."
  },
  {
    "objectID": "posts/2023-01-26-intro-quarto/index.html#rmarkdown",
    "href": "posts/2023-01-26-intro-quarto/index.html#rmarkdown",
    "title": "Literate Statistical Programming and Quarto",
    "section": "rmarkdown",
    "text": "rmarkdown\nAnother choice for literate programming is to build documents based on Markdown language. A markdown file is a plain text file that is typically given the extension .md. The rmarkdown R package takes a R Markdown file (.Rmd) and weaves together R code chunks Figure 1, producing a large number of user-specified outputs.\n\n\n\nFigure 1: R markdown translates text and code to many different formats\n\n\nR chunks surrounded by text looks like this:\n```{r plot1, height=4, width=5, eval=FALSE, echo=TRUE}\ndata(airquality)\nplot(airquality$Ozone ~ airquality$Wind)\n```\n\n\n\n\n\n\nTip\n\n\n\nThe best resource for learning about R Markdown this by Yihui Xie, J. J. Allaire, and Garrett Grolemund:\n\nhttps://bookdown.org/yihui/rmarkdown\n\nThe R Markdown Cookbook by Yihui Xie, Christophe Dervieux, and Emily Riederer is really good too:\n\nhttps://bookdown.org/yihui/rmarkdown-cookbook\n\nThe authors of the 2nd book describe the motivation for the 2nd book as:\n\n“However, we have received comments from our readers and publisher that it would be beneficial to provide more practical and relatively short examples to show the interesting and useful usage of R Markdown, because it can be daunting to find out how to achieve a certain task from the aforementioned reference book (put another way, that book is too dry to read). As a result, this cookbook was born.”\n\n\n\nBecause this is lecture is built in a .qmd file (which is very similar to a .Rmd file), let’s demonstrate how this work. I am going to change eval=FALSE to eval=TRUE.\n\ndata(airquality)\nplot(airquality$Ozone ~ airquality$Wind)\n\n\n\n\n\n\n\n\n\n\nQuestions\n\n\n\n\nWhy do we not see the back ticks ``` anymore in the code chunk above that made the plot?\nWhat do you think we should do if we want to have the code executed, but we want to hide the code that made it?\n\n\n\nBefore we leave this section, I find that there is quite a bit of terminology to understand the magic behind rmarkdown that can be confusing, so let’s break it down:\n\nPandoc. Pandoc is a command line tool with no GUI that converts documents (e.g. from number of different markup formats to many other formats, such as .doc, .pdf etc). It is completely independent from R (but does come bundled with RStudio). If you donʻt have Rstudio installed, you will have to install pandoc.\nMarkdown (markup language). Markdown is a lightweight markup language with plain text formatting syntax designed so that it can be converted to HTML and many other formats. A markdown file is a plain text file that is typically given the extension .md. It is completely independent from R.\nR Markdown (markup language). R Markdown is an extension of the markdown syntax for weaving together text with R code. R Markdown files are plain text files that typically have the file extension .Rmd.\nrmarkdown (R package). The R package rmarkdown is a library that uses pandoc to process and convert text and R code written in .Rmd files into a number of different formats. This core function is rmarkdown::render(). Note: this package only deals with the markdown language. If the input file is e.g. .Rhtml or .Rnw, then you need to use knitr prior to calling pandoc (see below).\n\n\n\n\n\n\n\nTip\n\n\n\nCheck out the R Markdown Quick Tour for more:\n\nhttps://rmarkdown.rstudio.com/authoring_quick_tour.html\n\n\n\n\n\n\nArtwork by Allison Horst on RMarkdown\n\n\n\nknitr\nOne of the alternative that has come up in recent times is something called knitr.\n\nThe knitr package for R takes a lot of these ideas of literate programming and updates and improves upon them.\nknitr still uses R as its programming language, but it allows you to mix other programming languages in.\nYou can also use a variety of documentation languages now, such as LaTeX, markdown and HTML.\nknitr was developed by Yihui Xie while he was a graduate student at Iowa State and it has become a very popular package for writing literate statistical programs.\n\nKnitr takes a plain text document with embedded code, executes the code and ‘knits’ the results back into the document.\nFor for example, it converts\n\nAn R Markdown (.Rmd) file into a standard markdown file (.md)\nAn .Rnw (Sweave) file into to .tex format.\nAn .Rhtml file into to .html.\n\nThe core function is knitr::knit() and by default this will look at the input document and try and guess what type it is e.g. Rnw, Rmd etc.\nThis core function performs three roles:\n\nA source parser, which looks at the input document and detects which parts are code that the user wants to be evaluated.\nA code evaluator, which evaluates this code\nAn output renderer, which writes the results of evaluation back to the document in a format which is interpretable by the raw output type. For instance, if the input file is an .Rmd, the output render marks up the output of code evaluation in .md format.\n\n\n\n\n\n\nConverting a Rmd file to many outputs using knitr and pandoc\n\n\n\n\n[Source]\nAs seen in the figure above, from there pandoc is used to convert e.g. a .md file into many other types of file formats into a .html, etc.\nSo in summary:\n\n“R Markdown stands on the shoulders of knitr and Pandoc. The former executes the computer code embedded in Markdown, and converts R Markdown to Markdown. The latter renders Markdown to the output format you want (such as PDF, HTML, Word, and so on).”\n\n[Source]"
  },
  {
    "objectID": "posts/2023-01-26-intro-quarto/index.html#create-your-website-locally-with-quarto",
    "href": "posts/2023-01-26-intro-quarto/index.html#create-your-website-locally-with-quarto",
    "title": "Literate Statistical Programming and Quarto",
    "section": "Create your website locally with Quarto",
    "text": "Create your website locally with Quarto\nIn this section, I am adding a bit more explanation to the Quarto quickstart guide up to and including Render. If something is not clear, please consult https://quarto.org/docs/websites/\nThere are three main quarto commands we will use:\n\nquarto create-project: Make a website project template\nquarto preview: Take a look at what the webite will look like\nquarto render: Render your qmd to html\n\n\nMake your website directory and template\nCreate your website (here called mysite) using the following command. It will make a directory of the same name and put the website contents within it.\n\n\nTerminal\n\nquarto create-project mysite --type website\n\nYou should now see the following files in your mysite directory (Figure 2):\n\n\n\nFigure 2: Website files from the Terminal view\n\n\nThis is the bare-bones version of your website. Check that the code is functional by looking at a preview:\n\n\nTerminal\n\nquarto preview\n\nThis should open up a browser window showing a temporary file made by quarto by rendering your website files.\n\n\n\n\n\n\nTip\n\n\n\n\nquarto preview will refresh the preview every time you save your index.qmd (or any) website files. So itʻs a good idea to keep the preview open as you make edits and saves.\nCheck every edit, it is easier to debug in small steps.\nTerminate quarto preview with Control-c\n\n\n\n\n\nRender your website to html\nUse quarto to render your content to html, the format used by browsers. First navigate into your website directory then render:\n\n\nTerminal\n\ncd mysite\nquarto render\n\nTake a look at the mysite contents after rendering, you should see a new directory _site (Figure 3). The html was rendered and put in there (go ahead, open up the files and check it out):\n\n\n\nFigure 3: Website files after rendering\n\n\n\n\nPersonalize your content\nWhat is really nice is that you can personalize your website by simply editing the quarto markdown and yaml files.\n\nWeb content goes in .qmd\nUsing any text editor, edit the index.qmd to personalize your website.\nThe first section of your index.qmd is the header. You can change the title and add additional header information, including any cover images and website templates.\nFor example this is what I have in my course website index.qmd header. Note that my cover image is in a folder called images within at the top level of my website directory. If you want to try this out substitute or remove the image line and change the twitter/github handles.\n\n\nindex.qmd\n\n---\ntitle: \"Welcome to Introduction to Data Science in R for Biologists!\"\nimage: images/mycoolimage.png\nabout:\n  template: jolla\n  links:\n    - icon: twitter\n      text: Twitter\n      href: https://twitter.com/mbutler808\n    - icon: github\n      text: Github\n      href: https://github.com/mbutler808\n---\n\nYou should edit the body of your website as well. You simply edit the text.\nThe quarto markdown page has great examples showing how to format your content. Take a look at how to specify header sizes, lists, figures and tables.\nTry editing the about.qmd file as well. You will notice that this is another tab in your website. YOu can add more tabs by adding .qmd files.\nWith each addition, be sure to quarto preview your changes to make sure it works. When you are satisfied with your website, quarto render to render to html.\n\n\n\n\n\n\nTip\n\n\n\n\nWhen editing markdown, take care to note spaces and indents as they are interpreted for formatting.\nIndentations are really important for formatting lists.\nFor example in a hyperlink, there is no space between the square brackets and parentheses. [This is a cool link](http://mycoollink.com)\n\n\n\n\n\nWebsite-wide settings go in _quarto.yml\nAll Quarto projects include a _quarto.yml configuration file that sets the global options that apply across the entire website.\nYAML started off as “Yet Another Markup Language” 😜. It is clean, clear, and widely used. You can edit your YAML to add options or change the format of your website. Take a look at your _quarto.yml.\nHere is an example for a simple website. title: is the parameter to set the websiteʻs title. navbar: sets the menu, in this case on the left sidebar. By default tabs will be named based on the names of the .qmd files, but you can set them manually. There are many themes you can choose from too, check them out. For something different try cyborg.\n\n\n_quarto.yml\n\nproject:\n  type: website\n\nwebsite:\n  title: \"today\"\n  navbar:\n    left:\n      - href: index.qmd\n        text: Home\n      - about.qmd\n\nformat:\n  html:\n    theme: minty\n    css: styles.css\n    toc: true\n\nAgain, after saving your edits, quarto preview to see the effects. When you are satisfied with your website, quarto render to render to html.\n\n\nTerminal\n\nquarto render"
  },
  {
    "objectID": "posts/2023-01-26-intro-quarto/index.html#publishing-your-website-to-github",
    "href": "posts/2023-01-26-intro-quarto/index.html#publishing-your-website-to-github",
    "title": "Literate Statistical Programming and Quarto",
    "section": "Publishing your website to GitHub",
    "text": "Publishing your website to GitHub\nYou can publish your website for free on GitHub, which is a very cool feature. In his section I am adding a bit more explanation to the Quarto quickstart guide up to and including Render to docs https://quarto.org/docs/publishing/github-pages.html. I describe the most important stpes below:\n\nRender your html to a docs directory\nSupress GitHub jekyll html processing by creating a .nojekyll file\nMake your website directory into a repo, and link it to a GitHub repo\nEdit the GitHub repo settings to publish your website\n\n\nRender your html to docs\nEdit the _quarto.yml file at the top level of your website to send output to docs. This will also create the docs folder.\n\n\n_quarto.yml\n\nproject:\n  type: website\n  output-dir: docs\n\nThe next time you quarto render it will create docs and all of its contents.\n\n\nSupress GitHub jekyll html processing\nGitHub uses a sofware called jekyll to render html from markdown. Since weʻre using quarto, we want to supress that. Create an empty file named .nojekyll at the top level of your website directory to supress default jekyll processing.\n\n\n\n\n\n\n\nMac/Linux\n\n\nTerminal\n\ntouch .nojekyll\n\n\n\nWindows\n\n\nTerminal\n\ncopy NUL .nojekyll\n\n\n\n\n\n\nSetup a GitHub repo for your website\n\nTurn your website directory into a git repo:\n\n\n\nTerminal\n\ngit init\ngit add .\ngit commit -m \"first commit\"\n\n\nCreate a GitHub repo by the same name\n\nFor example, mine might be github.com/mbutler808/mysite.\n\nLink your local repo and GitHub repo together\n\nIf you forgot how to do this, go back here\n\nCheck your GitHub repo. Are your files there?\n\n\n\nGitHub settings to serve your webpage\nAlmost there! A couple more steps.\nFrom your GitHub repo, click on Settings in the top menu, and Pages on the left menu.\nYour website should deploy from branch. Under Select branch choose main and under Select folder choose docs.\nAfter clicking save GitHub will trigger a deployment of your website. After a few minutes, your URL will appear near the top at Your site is live at...:\n\nCongratulations! ⚡️ Your website is now live 🎉🎊😍"
  },
  {
    "objectID": "posts/2023-01-26-intro-quarto/index.html#now-make-more-changes",
    "href": "posts/2023-01-26-intro-quarto/index.html#now-make-more-changes",
    "title": "Literate Statistical Programming and Quarto",
    "section": "Now make more changes!",
    "text": "Now make more changes!\n\n\n\n\n\n\nThe Quarto Workflow is\n\n\n\n\nEdit the content in .qmd\nFrom the Command line:\n\nquarto preview to check that edits are correct\nquarto render to render .qmd to .html\ngit add .\ngit commit -m \"message\"\ngit push origin main\n\nCheck your website (this may take a beat)"
  },
  {
    "objectID": "posts/2023-01-26-intro-quarto/index.html#for-fun",
    "href": "posts/2023-01-26-intro-quarto/index.html#for-fun",
    "title": "Literate Statistical Programming and Quarto",
    "section": "For fun",
    "text": "For fun\nYou can have fun with emoji! Guangchuang Yu wrote the package emojifont (this is the same person who wrote the widely used ggtree package) and now you can bring your emoji out of your phone and into your quarto documents! Install the R package emojifont:\n\ninstall.packages(\"emojifont\")\n\nThen anywhere you want an emoji in the markdown file, you just type:\n\n`r emojifont::emoji('palm_tree')`\n\n🌴\nOr if you want several, just line them up:\n\n`r emojifont::emoji('balloon')``r emojifont::emoji('tada')``r emojifont::emoji('smiley')`\n\n🎈🎉😃\nThere is a handy cheat sheet of emoji names here https://gist.github.com/rxaviers/7360908"
  },
  {
    "objectID": "posts/2023-01-26-intro-quarto/index.html#final-tips",
    "href": "posts/2023-01-26-intro-quarto/index.html#final-tips",
    "title": "Literate Statistical Programming and Quarto",
    "section": "Final tips",
    "text": "Final tips\n\n\n\n\n\n\nTip\n\n\n\n\nAlways always quarto render before you push up your changes to GitHub!\nIf your changes are not appearing, try quarto preview and check that your changes appear in the preview. Then quarto render before you use git to add, commit, and push\nNote: It can take a few minutes to render on GitHub before your changes appear on your website\n\n\n\nPlease see Stephanie Hicksʻ lecture for more literate programming examples and tips."
  },
  {
    "objectID": "posts/2023-01-19-your-computer-filesystems/index.html",
    "href": "posts/2023-01-19-your-computer-filesystems/index.html",
    "title": "Introduction to your computerʻs terminal utilities",
    "section": "",
    "text": "Read ahead\n\n\n\nFor future lectures, Iʻll give you some reading or podcasts to prepare\n\n\n\n\n\nMaterial for this lecture was borrowed and adopted from\n\nhttps://academind.com/tutorials/terminal-zsh-basics"
  },
  {
    "objectID": "posts/2023-01-19-your-computer-filesystems/index.html#the-kernel",
    "href": "posts/2023-01-19-your-computer-filesystems/index.html#the-kernel",
    "title": "Introduction to your computerʻs terminal utilities",
    "section": "The kernel",
    "text": "The kernel\nThe kernel is the part of your computerʻs operating system that loads first once you start up. It is kind of like your computerʻs autonomic nervous system. It recognizes all of the physical hardware attached to it, enables communication between components (and device drivers), and monitors maintenance functions like turning on the fan when it gets hot, manages virtual memory, gives warnings when the hard drive gets full, manages multitasking, and manages security and file permissions. In the mac this is the XNU kernel, in modern Windows machines it is the Windows NT kernel.\n\n\n\n\n\n[Source: Map of MacOS: the heart of everything is called Darwin; and within it, we have separate system utilities (the shell) and the XNU kernel, which is composed in parts by the Mach kernel and by the BSD kernel.]"
  },
  {
    "objectID": "posts/2023-01-19-your-computer-filesystems/index.html#the-shell",
    "href": "posts/2023-01-19-your-computer-filesystems/index.html#the-shell",
    "title": "Introduction to your computerʻs terminal utilities",
    "section": "The shell",
    "text": "The shell\nThe shell is another key part of the core operating system (note in the diagram above it is part of the System Utilities, and the partner of the kernel). The shell is a software (an app) that allows humans to control the computer. You are already familiar with the GUI interface, or the Graphical User Interface. It is important that you are comfortable using the Command Line Interface as well.\n\n\n\n\n\n\nThere are many reasons to be proficient in the shell:\n\n\n\n\nData analysis increasingly uses many files. The shell provides a simple but very powerful means to do all kinds of operations on files: move, delete, organize, combine, rename, etc.\nUsing the shell encourages you to understand your computerʻs filesystem, and helps you to more precisely control input and output to any place along your file paths.\nShell operations are fast.\nYou can use wildcards to control matching or excluding many files.\nThe shell can be used to execute (run) software.\nThe shell is probably the oldest app, so it is very stable with lasting power.\nIt is part of the OS, so when your apps fail or you are having some issues, you would turn to the shell to kill troublesome processes (programs) or diagnose and fix the issues.\n\n\n\nMacs use the same terminal utilities as UNIX/Linux systems. On the Mac, the command line interface app is called Terminal, which you will find in your Application folder, in the Utilities subfolder (here is a screentshot of our GUI Interface).\n\n\n\n\n\nOn a PC you would use the Command Prompt otherwise known as the Windows Command Processor or CMD. If you used a pre-Windows machine, you would be familiar with MS-DOS. To open CMD:\n\nOpen the Start Menu and type “command prompt” or\nPress Win + R and type “cmd” in the run box or\nPress Win + X and select Command Prompt from the menu.\n\nNote: you may see Windows PowerShell or Windows Terminal instead, these are similar apps."
  },
  {
    "objectID": "posts/2023-01-19-your-computer-filesystems/index.html#the-dots",
    "href": "posts/2023-01-19-your-computer-filesystems/index.html#the-dots",
    "title": "Introduction to your computerʻs terminal utilities",
    "section": "The dots",
    "text": "The dots\n\n“.” is the current working directory (where you are currently)\n“..” is the directory one level up\n“./foldername” will take you to the folder one level down, for example “./Data”\n\nYou can use these paths to move directories using cd or to list the contents of the directories using ls or to make new directories using mkdir\nls .\nls ./Data\nmkdir ./Data/A\nMake multiple directories:\nmkdir ./Data/B ./Data/C\nList the files one level up or two levels up:\nls ..  # for PC use dir ..\nls ../..\nUp one level, and over to another directory:\nls ../AnotherDirectory\nYou can wander anywhere along your computerʻs file directory! Just add more steps to the path."
  },
  {
    "objectID": "posts/2023-01-19-your-computer-filesystems/index.html#rtistry",
    "href": "posts/2023-01-19-your-computer-filesystems/index.html#rtistry",
    "title": "Introduction to your computerʻs terminal utilities",
    "section": "rtistry",
    "text": "rtistry\n\n\n\n\n\n[‘Flametree’ from Danielle Navarro https://art.djnavarro.net]"
  },
  {
    "objectID": "posts/2023-01-24-intro-git-github/index.html",
    "href": "posts/2023-01-24-intro-git-github/index.html",
    "title": "Introduction to git/GitHub",
    "section": "",
    "text": "Helpful references for this lecture\n\n\n\n\nHappy Git with R from Jenny Bryan\nChapter on git and GitHub in dsbook from Rafael Irizarry\nGitHub introduction in module 1 from Andreas Handel\n\n\n\n\n\nMaterial for this lecture was borrowed and adopted from\n\nhttps://www.stephaniehicks.com/jhustatcomputing2022/\nhttps://andreashandel.github.io/MADAcourse/Tools_Github_Introduction.html"
  },
  {
    "objectID": "posts/2023-01-24-intro-git-github/index.html#git",
    "href": "posts/2023-01-24-intro-git-github/index.html#git",
    "title": "Introduction to git/GitHub",
    "section": "git",
    "text": "git\nGit is software that implements what is called a version control system for a repository of files (also known as a repo). The main idea is that as you (and your collaborators) work on a project, the git software tracks, and records any changes made by anyone.\nGitHub is an online server and user interface that provides powerful tools for distribution of your repository, bug tracking, collaboration, and also allows you to create easy websites for each repository.\nGit and GitHub together provide an organized way to track your projects, and all of the tools you will need in this course are free.\n\n\n\n\n\nFigure 1. Whether working solo (A) or collaborating in a group (B) version tracking by naming files is a mess when you want to retrace the steps of the analysis (C). Git and GitHub track all changes and the complete branching tree of the project (D). The commit history is a powerful tool to retrace the development of the project or can be used to roll back to any prior version.\n\n\n\n\n[Source: Jenny Bryan]\nIt is very well suited for collaborative work, as it was developed by Linus Torvalds (in about 10 days of coding!) for collaborative software development of the Linux kernel pretty interesting interview with Torvalds. What it did really well was distributed control, and allowing everyone to have their own copy of the repository.\nGit/GitHub is now the dominant version control system with GitHub hosting over 200 million repositories worldwide! It is used very broadly for all kinds of repos now including data science projects, book projects, courses, and anything that needs collaborative management of mostly text files.\nAnother great and fun read about Git/GitHub and why it is a great tool for data analysis is in this article by Jenny Bryan."
  },
  {
    "objectID": "posts/2023-01-24-intro-git-github/index.html#what-to-not-use-gitgithub-for",
    "href": "posts/2023-01-24-intro-git-github/index.html#what-to-not-use-gitgithub-for",
    "title": "Introduction to git/GitHub",
    "section": "What to (not) use Git/GitHub for",
    "text": "What to (not) use Git/GitHub for\nGitHub is ideal if you have a project with (possibly many) smallish files, and most of those files are text files (such as R code, LaTeX, Quarto/(R)Markdown, etc.) and different people work on different parts of the project.\nGitHub is less useful if you have a lot of non-text files (e.g. Word or PowerPoint) and different team members might want to edit the same document at the same time. In that instance, a solution like Google Docs, Word+Dropbox, Word+Onedrive, etc. might be better.\nGitHub also has a problem with large files. Anything above around 50MB can lead to very slow syncing and sometimes outright failure. Unfortunately, once GitHub has choked on a large file, it can be quite tricky to fix the repository to work again. This is because the ENTIRE history is saved, including the addition of the huge file. Therefore keep large (>50MB) files out of your GitHub repositories. If you have to work with such files, try to reduce them first before placing into the GitHub repository. Or as alternative, place those files in another sync service (e.g. Dropbox, OneDrive, GoogleDrive) and load them from there.\nFinally, if you have data, you need to be careful since by default, GitHub repositories are public. You can set the repository to private, but you need to be careful that you don’t accidentally expose confidential data to the public. It is in general not a good idea to have confidential data on GitHub. First anonymize your data (ensure that it is not at risk of identifiability), then you can place it in a private repository. If you put it in a public repo, be very careful!! (And you may need IRB approval, check with your institutional research office.)\n\n\n\n\n\n\nTip\n\n\n\n\nGit/GitHub has version control features like a turbo-charged version of “track changes” but more rigorous, powerful, and scaled up to multiple files\nGreat for solo or collaborative work\nSaves the entire history of every change made\nAllows for multiple verisions or “branches” to be developed and later merged\nGitHub allows distributed collaboration (potentially among complete strangers) and has greatly promoted open-source software development, collaboration, distribution, and bug/issue tracking for users to get help\nGitHub allows webpages for your projects or repositories\n\n\n\nNote that other interfaces to Git exist, e.g., Bitbucket, but GitHub is the most widely used one."
  },
  {
    "objectID": "posts/2023-01-24-intro-git-github/index.html#why-use-gitgithub",
    "href": "posts/2023-01-24-intro-git-github/index.html#why-use-gitgithub",
    "title": "Introduction to git/GitHub",
    "section": "Why use git/GitHub?",
    "text": "Why use git/GitHub?\nYou want to use GitHub to avoid this:\n\n\n\n\n\nHow not to use GitHub image from PhD Comics\n\n\n\n\nTo learn a bit more about Git/GitHub and why you might want to use it, read this article by Jenny Bryan.\nNote her explanation of what’s special with the README.md file on GitHub."
  },
  {
    "objectID": "posts/2023-01-24-intro-git-github/index.html#how-to-use-gitgithub",
    "href": "posts/2023-01-24-intro-git-github/index.html#how-to-use-gitgithub",
    "title": "Introduction to git/GitHub",
    "section": "How to use Git/GitHub",
    "text": "How to use Git/GitHub\nGit/GitHub is fundamentally based on commands you type into the command line. Lots of online resources show you how to use the command line. This is the most powerful, and the way I almost always interact with git/GitHub. However, many folks find this the most confusing way to use git/GitHub. Alternatively, there are graphical interfaces.\n\nGitHub itself provides a grapical interface with basic functionality.\nRStudio also has Git/GitHub integration. Of course this only works for R project GitHub integration.\nThere are also third party GitHub clients with many advanced features, most of which you won’t need initially, but might eventually.\n\nNote: As student, you can (and should) upgrade to the Pro version of GitHub for free (i.e. access to unlimited private repositories is one benefit), see the GitHub student developer pack on how to do this."
  },
  {
    "objectID": "posts/2023-01-24-intro-git-github/index.html#sec-profile",
    "href": "posts/2023-01-24-intro-git-github/index.html#sec-profile",
    "title": "Introduction to git/GitHub",
    "section": "Set up your profile in git on your computer",
    "text": "Set up your profile in git on your computer\nBefore making changes to your repository, GitHub will want to verify your identity.\nIn order for your computer to talk to GitHub smoothly, you will need to set up your username and email in the git profile stored on your computer.\n\n\n\n\n\n\nWarning\n\n\n\nBe sure to match your GitHub account username and email! Otherwise GitHub wonʻt know who you are\n\n\n\n\nTerminal\n\ngit config --global user.name 'GitHubUsername'\ngit config --global user.email 'GitHub_email@example.com'\ngit config --global --list\n\nThat last line will show all of your current git config settings.\nIf you are using Rstudio, easy directions are provided here https://happygitwithr.com/hello-git.html"
  },
  {
    "objectID": "posts/2023-01-24-intro-git-github/index.html#sec-token",
    "href": "posts/2023-01-24-intro-git-github/index.html#sec-token",
    "title": "Introduction to git/GitHub",
    "section": "Set up your Personal Authentication Token on your computer",
    "text": "Set up your Personal Authentication Token on your computer\nGitHub will also want to check your credentials to authenticate you are really you before writing changes to your repo.\nThere are several ways to do this, but the easiest is the protocol for HTTP authentication. You will generate a Personal Access Token for HTTPS from your GitHub account which will be stored on your personal machine.\nI prefer the GithHub command line interface or gh to do this. To install the CLI, follow the instructions here for your operating system. For Mac users, I suggest that you install homebrew, it is a command-line general software manager for many different software packages.\n\n\n\n\n\n\nStoring your Personal Access Token\n\n\n\n\nFrom GitHub: Generate your personal access token instructions\n\nFind the token generator on GitHub under your User Icon > settings > developer settings (left side bar) > Personal access tokens > Tokens (classic) > Generate new Token > classic note this menu may change\nSelect at least these scopes: “admin:org”,“repo”, “user”, “gist”, and “workflow”\n\nFrom your Command Line: Use gh auth login to store your token and follow the prompts.\n\nselect HTTPS for your preferred protocol\nselect Y for authenticate with GitHub credentials\nAlternatively if you want to do this all from the command line you can run the following line (if your token is saved in mytoken.txt):\n\n\n\nTerminal\n\ngh auth login --with-token < mytoken.txt\n\n\n\n\nMany more details are explained nicely here https://happygitwithr.com/https-pat.html\nYou only have to store your credentials once for each computer (or PAT expiration date), then you can push and pull from GitHub to your heartʻs content. It really is a nice way to do things securely."
  },
  {
    "objectID": "posts/2023-01-24-intro-git-github/index.html#configuring-your-default-git-editor",
    "href": "posts/2023-01-24-intro-git-github/index.html#configuring-your-default-git-editor",
    "title": "Introduction to git/GitHub",
    "section": "Configuring your default git editor",
    "text": "Configuring your default git editor\nYou may want to set your default git editor to something you know how to use (it will come up when you have a merge conflict).\nFor example the nano editor is easy to use on the command line for a Unix shell:\n\n\nMac/Linux\n\n$ git config --global core.editor \"nano -w\"\n\nThe Carpentries provide a full list of editors by operating system, a great resource."
  },
  {
    "objectID": "posts/2023-02-09-data/index.html",
    "href": "posts/2023-02-09-data/index.html",
    "title": "Types of Data",
    "section": "",
    "text": "✏️"
  },
  {
    "objectID": "posts/2023-02-09-data/index.html#acknowledgements",
    "href": "posts/2023-02-09-data/index.html#acknowledgements",
    "title": "Types of Data",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nMaterial for this lecture was borrowed and adopted from\n\nhttps://andreashandel.github.io/MADAcourse/Data_Types.html\nhttps://r-coder.com/data-types-r/#Raw_data_type_in_R\nhttps://www.stat.auckland.ac.nz/~paul/ItDT/HTML/node76.html # Learning objectives\n\n\n\n\n\n\n\nLearning objectives\n\n\n\nAt the end of this lesson you will:\n\nUnderstand different types of data and how they are represented computationally\nUnderstand that different data types require different analysis approaches\nRecognize different base data types in R and know how to work with them\nRecognize the base data structures or objects in R and how to use them to do what you want"
  },
  {
    "objectID": "posts/2023-02-09-data/index.html#basic-data-types",
    "href": "posts/2023-02-09-data/index.html#basic-data-types",
    "title": "Types of Data",
    "section": "Basic data types",
    "text": "Basic data types\n\n\n\n\n\n\nR has six basic (atomic) types of data:\n\n\n\n\n\n\nAtomic Type\nShort Description\nSize in bytes\n\n\n\n\nstring (or character)\ntext\n1 (per character)\n\n\ninteger\ncountable numbers\n4\n\n\nreal\nreal numbers\n8\n\n\nlogical\nTRUE or FALSE\n4\n\n\ncomplex\nnumbers with imaginary component\n\n\n\nraw\nraw bytes\n1\n\n\n\nAll other data types are derived from the atomic types\n\n\nNote: Most computers use 64-bit operating systems these days, so the sizes above are for 64-bit software.\nFor a quick tour of the data types, see https://r-coder.com/data-types-r\nString/character: Character values are alphanumeric values (plus whitespace, punctuation, etc.). A string is a collection of characters, in other words “text”.\n\nstrings can be pasted together using the paste() function.\nR has powerful tools for string manipulation, including searching, replacing, and customized partial matching (with or without replacement) using wildcards and perl-like regular expressions (or regex) using base functions such as\n\ngrep()\nsub()\ngsub()\nsubstr()\n\nThere are also packages specific for string manipulation including the stringr package which is part of the tidyverse.\n\nIt is very likely that you will need to work with strings at some point during a data analysis, even if it is only to find specific values, clean up variable names, etc.\nThese problems can be quite the headache! But instead of editing them by hand and possibly making an error, it is better to do this with code. It also makes it easier to keep a record of the original data and all of the changes made to it, improving the reproducibility of your analysis.\nThere is a learning curve to using these tools, especially regex syntax, but they are very powerful and well worth your time.\n\n\n\n\n\n\nGood sources for practice manipulating strings:\n\n\n\n\nFor beginners: Review the Strings chapter (14) of R4DS, and do the exercises.\nThe string processing chapter (25) of IDS\nthe Character Vectors chapter in the STAT 545 book by Jenny Bryan\n\nDecide which one is right for your level and work through some examples. I think youʻll agree that it is worth your time.\n\n\nNumeric (double or integer): Variables of type numeric in R are either integers or double precision (representing real numbers).\n\nIntegers and real values are different, but in practice most R users donʻt pay attention to this distinction. Integer values tend to be coerced (converted) to real values if any mathematical operations are done to them.\nIf an integer is explicity needed, you can create them using functions such as as.integer().\nNote that when you type an integer value, e.g. x <- 2, into R, this is considered numeric by default.\nIf you want to make sure a value is treated as integer, add an L, e.g. x <- 2L.\n\nLogical: Logical variables are binary and can take on only two values, TRUE or FALSE (which are reserved words that only take on these meanings in R).\n\nIn R, logical values are treated as integers, and interpreted as 1 for TRUE and 0 for FALSE. It is possible to sum(TRUE) or a vector of logicals, for example.\n\nR also understands T, True, and true for TRUE, and the corresponding representations for FALSE.\nImportantly, logical comparisons are used for indexing. You will use logical comparisons when cleaning and checking your data, or running analyses, e.g., if you want to see if your variable x is greater than 5, then the R command x > 5 will return either TRUE or FALSE, based on the value of x.\nNote: reserved words are understood as constants and should not be “quoted”."
  },
  {
    "objectID": "posts/2023-02-09-data/index.html#derived-data-types",
    "href": "posts/2023-02-09-data/index.html#derived-data-types",
    "title": "Types of Data",
    "section": "Derived data types",
    "text": "Derived data types\nR also allows derived data types called classes that are built up from atomic data types. There are R base classes as well as new classes that can be defined as needed by programmers (maybe you?).\nFactors: Are Rʻs class for categorical variables.\n\nFactors have names and values.\nFor example, a size factor may have names (or levels) of small, medium. and large with values 0,1,2. Here, the values simply indicate the different categories, with the names being the human-friendly labels for the values.\n\nFactors can be ordered/ordinal or not.\nFactors could be numeric values, e.g., the number of offspring.\nOr it could be a factor coding for 3 types of habitat (unordered),\nOr 3 levels of life history stage (ordered).\nAn excellent package to work with factors is the forcats package.\n\nFor more about factors, work through the Factors chapter of R4DS, and do the exercises.\nDate/time: Dates in base R are of the class Date (and are called POSIX variables). The lubridate package is a tidyverse package to work with dates, which many people find easier. There are other packages as well.\nAdditional resources are the Dates and times chapter of R4DS and the Parsing Dates and Times chapter of IDS.\nProgrammer-defined classes Many packages define their own classes. For example class phylo is used to represent phylogenetic trees in the ape package.\n\nThere are several functions that can show you the data type of an R object\nsuch as typeof(), mode(), storage.mode(), class() and str()."
  },
  {
    "objectID": "posts/2023-02-09-data/index.html#other-derived-data-types",
    "href": "posts/2023-02-09-data/index.html#other-derived-data-types",
    "title": "Types of Data",
    "section": "Other derived data types",
    "text": "Other derived data types\nTimeseries: A very useful set of tools for times-series analysis in R is the set of packages called the tidyverts. CRAN also has a Task View for Time Series Analysis. (A Task View on CRAN is a site that tries to combine and summarize various R packages for a specific topic). Another task view that deals with longitudinal/time-series data is the Survival Analysis Task View.\nOmics: The bioconductor website is your source for (almost) all tools and resources related to omics-type data analyses in R.\nText: Working with and analyzing larger sections of text is different from the simple string manipulation discussed above. These days, analysis of text often goes by the term natural language processing. Such text analysis will continue to increase in importance, given the increasing data streams of that type. If you are interested in doing full analyses of text data, the tidytext R package and the Text mining with R book are great resources. A short introduction to this topic is The text mining chapter (27) of IDS.\nImages: Images are generally converted into multiple matrices of values for different pixels of an image. For instance, one could divide an image into a 100x100 grid of pixels, and assign each pixel a RGB values and intensity. That means one would have 4 matrices of numeric values, each of size 100x100. One would then perform operations on those values. We won’t do anything with images here, there are some R packages for analyzing image data.\nVideos: Are a time-series of images. Analysis of videos therefore has an extra layer of complexity."
  },
  {
    "objectID": "posts/2023-02-09-data/index.html#for-sequences",
    "href": "posts/2023-02-09-data/index.html#for-sequences",
    "title": "Types of Data",
    "section": "For sequences",
    "text": "For sequences\n\n\n\nFunctions\nActions\n\n\n\n\nseq()\ngenerate a sequence of numbers\n\n\n1:10\nsequence from 1 to 10 by 1\n\n\nrep(x, times)\nreplicates x\n\n\nsample(x, size, replace=FALSE)\nsample size elements from x\n\n\nrnorm(n, mean=0, sd=1)\ndraw n samples from normal distribution"
  },
  {
    "objectID": "posts/2023-02-09-data/index.html#creating-or-coercing-objects-to-different-class",
    "href": "posts/2023-02-09-data/index.html#creating-or-coercing-objects-to-different-class",
    "title": "Types of Data",
    "section": "Creating or Coercing objects to different class",
    "text": "Creating or Coercing objects to different class\n\n\n\nFunctions\nActions\n\n\n\n\nvector()\ncreate a vector\n\n\nmatrix()\ncreate a matrix\n\n\ndata.frame()\ncreate a data frame\n\n\nas.vector(x)\ncoerces x to vector\n\n\nas.matrix(x)\ncoerces to matrix\n\n\nas.data.frame(x)\ncoerces to data frame\n\n\nas.character(x)\ncoerces to character\n\n\nas.numeric(x)\ncoerces to numeric\n\n\nfactor(x)\ncreates factor levels for elements of x\n\n\nlevels()\norders the factor levels as specified"
  },
  {
    "objectID": "posts/2023-02-01-reference-management/index.html",
    "href": "posts/2023-02-01-reference-management/index.html",
    "title": "Reference management",
    "section": "",
    "text": "Read ahead\n\n\n\nBefore class, you can prepare by reading the following materials:\n\nAuthoring in R Markdown from RStudio\nCitations from Reproducible Research in R from the Monash Data Fluency initiative\nBibliography from R Markdown Cookbook\n\n\n\n\n\n\nMaterial for this lecture was borrowed and adopted from\n\nhttps://www.stephaniehicks.com/jhustatcomputing2022/\nhttps://andreashandel.github.io/MADAcourse\nhttps://rmarkdown.rstudio.com/authoring_bibliographies_and_citations.html\nhttps://bookdown.org/yihui/rmarkdown-cookbook/bibliography.html\nhttps://monashdatafluency.github.io/r-rep-res/citations.html"
  },
  {
    "objectID": "posts/2023-02-01-reference-management/index.html#the-parts",
    "href": "posts/2023-02-01-reference-management/index.html#the-parts",
    "title": "Reference management",
    "section": "The Parts",
    "text": "The Parts\nThere are three basic parts:\n\nThe citation data which is stored in the .bib file Section 4, and\nIn-text citations included by the author into the .qmd document Section 7, and\nLinking the ʻ.bibʻ file in the YAML header Section 6.\n\nFrom this, both the in-text citations as well as the citation list at the end of the document will be rendered.\nThere are additional customization options. You can change the style of the bibliography using style and class files Section 8. This is at the level of the entire document, it is easy to switch. Within the document, there are also many options for in-text citation styles Section 9. You can also customize the style file by editing the LaTeX.\nReference managers are helper software (independent of BibTeX) that are wonderful tools to help you collect and organize your citation data Section 5."
  },
  {
    "objectID": "posts/2023-02-01-reference-management/index.html#citation-management-software",
    "href": "posts/2023-02-01-reference-management/index.html#citation-management-software",
    "title": "Reference management",
    "section": "Citation management software",
    "text": "Citation management software\nIn addition to .bib (BibTeX) there are a lot of file formats in use including .medline (MEDLINE), .ris (RIS), and .enl (EndNote), among others. You can generally download the results of your literature search in the format of your choice (some citation manager software can convert formats as well).\nIf you recall the output from citation(\"rmarkdown\") above, one option is to copy and paste the BibTeX output into a text file labeled .bib or into citation management software, but instead we can use Rʻs write_bib() function from the knitr package to create a bibliography file.\nLet’s run the following code in order to generate a my-refs.bib file\n\nknitr::write_bib(\"rmarkdown\", file = \"my-refs.bib\")\n\nYou can output multiple citations by passing a vector of package names:\n\nknitr::write_bib(c(\"rmarkdown\",\"base\"), file = \"my-refs.bib\")\n\nNow we can see we have the file saved locally.\n\nlist.files()\n\n[1] \"ButlerPapers.bib\" \"evolution.csl\"    \"index 2.html\"     \"index.qmd\"       \n[5] \"index.rmarkdown\"  \"my-refs.bib\"     \n\n\nIf you open up the my-refs.bib file, you will see\n@Manual{R-base,\n  title = {R: A Language and Environment for Statistical Computing},\n  author = {{R Core Team}},\n  organization = {R Foundation for Statistical Computing},\n  address = {Vienna, Austria},\n  year = {2022},\n  url = {https://www.R-project.org/},\n}\n\n@Manual{R-rmarkdown,\n  title = {rmarkdown: Dynamic Documents for R},\n  author = {JJ Allaire and Yihui Xie and Jonathan McPherson and Javier Luraschi and Kevin Ushey and Aron Atkins and Hadley Wickham and Joe Cheng and Winston Chang and Richard Iannone},\n  year = {2021},\n  note = {R package version 2.8},\n  url = {https://CRAN.R-project.org/package=rmarkdown},\n}\n\n@Book{rmarkdown2018,\n  title = {R Markdown: The Definitive Guide},\n  author = {Yihui Xie and J.J. Allaire and Garrett Grolemund},\n  publisher = {Chapman and Hall/CRC},\n  address = {Boca Raton, Florida},\n  year = {2018},\n  note = {ISBN 9781138359338},\n  url = {https://bookdown.org/yihui/rmarkdown},\n}\n\n@Book{rmarkdown2020,\n  title = {R Markdown Cookbook},\n  author = {Yihui Xie and Christophe Dervieux and Emily Riederer},\n  publisher = {Chapman and Hall/CRC},\n  address = {Boca Raton, Florida},\n  year = {2020},\n  note = {ISBN 9780367563837},\n  url = {https://bookdown.org/yihui/rmarkdown-cookbook},\n}\n\nNote there are three keys that we will use later on:\n\nR-rmarkdown\nrmarkdown2018\nrmarkdown2020"
  },
  {
    "objectID": "posts/2023-02-07-scripts/index.html",
    "href": "posts/2023-02-07-scripts/index.html",
    "title": "Saving your work as R scripts",
    "section": "",
    "text": "✏️"
  },
  {
    "objectID": "posts/2023-02-07-scripts/index.html#make-sure-you-can-see-file-endings",
    "href": "posts/2023-02-07-scripts/index.html#make-sure-you-can-see-file-endings",
    "title": "Saving your work as R scripts",
    "section": "Make sure you can see file endings",
    "text": "Make sure you can see file endings\nDoes your MacOS or Windows environment show you the file endings (i.e., .R, .pdf, .csv, etc.)? If not be sure to turn them on. Try the instructions below or you can google for “show all file exensions in” (Mac or Windows, etc.).\n\nMac OS\nThis is a Finder preference. From any Finder window, click on the menu bar: Finder >  Preferences > Advanced > Click on Show all finename extensions.\n\n\nWindows\nThis is in File Explorer (Windows key + E). Click on the menu: View > Show > File Name Extensions. You can also choose to show all hidden files if you wish. For pictures see here"
  },
  {
    "objectID": "posts/2023-02-07-scripts/index.html#text-editor-environment",
    "href": "posts/2023-02-07-scripts/index.html#text-editor-environment",
    "title": "Saving your work as R scripts",
    "section": "Text editor environment",
    "text": "Text editor environment\nWhile I love the R text editor for writing R scripts, for working with multiple .qmd and other files I find it helpful to have a full-featured plain text editor. A new tool that I discovered is the Sublime text editor. If youʻd like to try it out, you can download it here: https://www.sublimetext.com\nA couple of features I like is that you can have multiple panes open. For example if you want to copy text from an old script to a new script, you can easily see and do that.\nIt also allows you to organize Projects, various folders that will appear on the sidebar to preserve your workspace. This helps when you are writing text documents across folders. So for example if you have your Rclass folder in one place and your website folder in another, you can have both open within the Sublime project. When you finish working on it you can save the project and reopen it later.\nTo create a project start by opening a new file. Then choose the Project > Add folder to Project... on the menu bar. You can load mulitple folders.\nIt has contextual highlighting for Quarto as well as GitHub markdown.\nIt also has integration with command line R https://bishwarup-paul.medium.com/a-guide-to-using-r-in-sublime-text-27f78b33f872. You can run R commands in a lower terminal pane, sent directly from your text document in sublime."
  },
  {
    "objectID": "posts/2023-02-07-scripts/index.html#different-desktop-windows",
    "href": "posts/2023-02-07-scripts/index.html#different-desktop-windows",
    "title": "Saving your work as R scripts",
    "section": "Different Desktop Windows",
    "text": "Different Desktop Windows\nItʻs also nice to have multiple desktops to organize your work. - It makes it easier to find your different apps. - You may have one workspace for your text editor, and another for your Terminal or CMD prompt, for example. - If I am working with multiple git repos, I might have one desktop just for my Terminal windows with a separate Terminal open for each one.\nTo use multiple desktops: On Windows On Mac\nOne the Mac, you can open new desktops by using three fingers to swipe up on the trackpad. Switch between them by swiping left or right with three fingers."
  },
  {
    "objectID": "posts/2023-02-07-scripts/index.html#organize-your-projects-into-folders",
    "href": "posts/2023-02-07-scripts/index.html#organize-your-projects-into-folders",
    "title": "Saving your work as R scripts",
    "section": "Organize your Projects into Folders",
    "text": "Organize your Projects into Folders\nWeʻve been learning about reproducibility. One important aspect is file organization. Each project should be organized into one folder that contains:\n\nAll input data (usually in a Data folder)\nAll code and documentation\nAll output\n\nThe idea is to keep everything complete, self-contained, and clear. Move old versions into a “Trash” folder. If you donʻt end up looking back at it, then delete it! (Or if you are bold, delete it right away!)\nA really useful UNIX/CMD command is tree. It shows you the directory structure contained within any folder. It works on both MacOS and Windows.\nThis is in ASCII – so you can copy and paste it into your README.md file!\n\n\nTerminal/CMD\n\ntree myfolder\n\nIf it is not pre-installed on your mac, you may need to install it with homebrew:\n\n\nTerminal\n\nbrew install tree"
  },
  {
    "objectID": "posts/2023-02-28-dplyr/index.html",
    "href": "posts/2023-02-28-dplyr/index.html",
    "title": "Getting data in shape with dplyr",
    "section": "",
    "text": "Pre-lecture materials\n\n🌴\n\nRead ahead\n\n\n\n\n\n\nRead ahead\n\n\n\nBefore class, you can prepare by reading the following materials:\n\nhttps://r4ds.had.co.nz/tibbles\nhttps://jhudatascience.org/tidyversecourse/wrangle-data.html#data-wrangling\ndplyr cheat sheet from RStudio\n\n\n\n\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\n\nhttps://www.stephaniehicks.com/jhustatcomputing2022/posts/2022-09-06-managing-data-frames-with-tidyverse/\nhttps://rdpeng.github.io/Biostat776/lecture-managing-data-frames-with-the-tidyverse\nhttps://jhudatascience.org/tidyversecourse/get-data.html#tibbles\n\n\n\n\nLearning objectives\n\n\n\n\n\n\nLearning objectives\n\n\n\nAt the end of this lesson you will:\n\nUnderstand the tools available to get data into the proper structure and shape for downstream analyses\nLearn about the dplyr R package to manage data frames\nRecognize the key verbs (functions) to manage data frames in dplyr\nUse the “pipe” operator to combine verbs together\n\n\n\n\n\nOverview\nIt is still important to understand base R manipulations, particularly for things such as cleaning raw data, troubleshooting, and writing custom functions. But the tidyverse provides many useful tools for data manipuation and analysis of cleaned data. In this session we will learn about dplyr and friends.\n\n\nTidy data\nThe tidyverse has many slogans. A particularly good one for all data analysis is the notion of tidy data.\nAs defined by Hadley Wickham in his 2014 paper published in the Journal of Statistical Software, a tidy dataset has the following properties:\n\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a table.\n\n\n\n\nArtwork by Allison Horst on tidy data\n\n\n[Source: Artwork by Allison Horst]\n\n\nWhat shapes does the data need to be in?\nBeyond the data being tidy, however, we also need to think about what shape it needs to be in. Weʻll review concepts and tools in the next two lessons.\nNow that we have had some experience plotting our data, we can see the value of having rectangular dataframes. We can also see that for particular graphics and analyses, we need to have the data arranged in particular ways.\nFor example, take a look at this elegant graphic below. This single graphic is packed with information on fat, BMR, TEE, and activity levels, all for mulitple species. Is it more effective that individual bar plots? This arrangement is so helpful because you can imagine questions that can be answered with it by comparing the different aspects of the data.\n\n\n\n\n\n\nA very informative figure!\n\n\n\n Source: Gibbons, 2022 based on data of H. Ponzer et al., NATURE, 533:90, 2016\n\n\n\n\n\n\n\n\nCan you imagine what this dataset looks like in terms of organization?\n\n\n\n\nFirst imagine what it would look like variable by variable.\nHow might you intially plot the data?\nWhat organization would you need to make a single figure such as this?\n\n\n\nWe often do not know exactly what we need at the start of a data analysis. We have to play around with different data structures, rearrange the data, look for interesting plots to try, rerrange to fit the input requirements of new functions weʻve discovered, and so on.\n\n\nTibbles\nThe tidyverse uses as its central data structure, the tibble or tbl_df. Tibbles are a variation on data frames, claimed to be lazy and surly:\n\nThey don’t change variable names or types when you construct the tibble.\nDon’t convert strings to factors (the default behavior in data.frame()).\nComplain more when a variable doesnʻt exist.\nNo row.names() in a tibble. Instead, you must create a new variable.\nDisplay a different summary style for its print() method.\nAllows non-standard R names for variables\nAllows columns to be lists.\n\nHowever, most tidyverse functions also work on data frames. Itʻs up to you.\n\ntibble() constructor\nJust as with data frames, there is a tibble() constructor function, which functions in many ways with similar syntax as the data.frame() constructor.\nIf you havenʻt already done so, install the tidyverse:\n\ninstall.packages(\"tidyverse\")\n\n\nrequire(tibble)\n\nLoading required package: tibble\n\ntibble( iris[1:4,] )  # the first few rows of iris\n\n# A tibble: 4 × 5\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n         <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n1          5.1         3.5          1.4         0.2 setosa \n2          4.9         3            1.4         0.2 setosa \n3          4.7         3.2          1.3         0.2 setosa \n4          4.6         3.1          1.5         0.2 setosa \n\nx <- 1:3\ntibble( x, x * 2 )  # name assigned at construction\n\n# A tibble: 3 × 2\n      x `x * 2`\n  <int>   <dbl>\n1     1       2\n2     2       4\n3     3       6\n\nsilly <- tibble(      # an example of a non-standard names\n  `one - 3` = 1:3,  # name = value syntax\n  `12` = \"numeric\",\n  `:)` = \"smile\",\n)\nsilly\n\n# A tibble: 3 × 3\n  `one - 3` `12`    `:)` \n      <int> <chr>   <chr>\n1         1 numeric smile\n2         2 numeric smile\n3         3 numeric smile\n\n\n\n\nas_tibble() coersion\nas_tibble() converts an existing object, such as a data frame or matrix, into a tibble.\n\nas_tibble( iris[1:4,] )  # coercing a dataframe to tibble\n\n# A tibble: 4 × 5\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n         <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n1          5.1         3.5          1.4         0.2 setosa \n2          4.9         3            1.4         0.2 setosa \n3          4.7         3.2          1.3         0.2 setosa \n4          4.6         3.1          1.5         0.2 setosa \n\n\n\n\nAs output\nMost often we will get tibbles returned from tidyverse functions such as read_csv() from the readr package.\n\n\n\nThe dplyr package\nThe dplyr package, which is part of the tidyverse was written to supply a grammar for data manipulation, with verbs for the most common data manipulation tasks.\n\n\n\nArtwork by Allison Horst on the dplyr package\n\n\n[Source: Artwork by Allison Horst]\n\ndplyr functions\n\nselect(): return a subset of the data frame, using a flexible notation\nfilter(): extract a subset of rows from a data frame using logical conditions\narrange(): reorder rows of a data frame\nrename(): rename columns in a data frame\nmutate(): add new columns or transform existing variables\nsummarize(): generate summary statistics of the variables in the data frame, by strata if data are hierarchical\n%>%: the “pipe” operator (from magrittr) connects multiple verbs together into a data wrangling pipeline (kind of like making a compound sentence)\n\nNote: Everything dplyr does could already be done with base R. What is different is a new syntax, which allows for more clarity of the data manipulations and the order, and perhaps makes the code more readable.\nInstead of the nested syntax, or typing the dataframe name over and over, we can pipe one operation into the next.\nAnother useful contribution is that dplyr functions are very fast, as many key operations are coded in C++. This will be important for very large datasets or repeated manipulations (say in a simulation study).\n\n\nstarwars dataset\nWe will use the starwars dataset included with dplyr. You should check out the help page for this dataset ?starwars.\nLetʻs start by using the skim() function to check out the dataset:\n\nrequire(dplyr)\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nclass(starwars)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nskimr::skim(starwars)\n\n\nData summary\n\n\nName\nstarwars\n\n\nNumber of rows\n87\n\n\nNumber of columns\n14\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n8\n\n\nlist\n3\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nname\n0\n1.00\n3\n21\n0\n87\n0\n\n\nhair_color\n5\n0.94\n4\n13\n0\n12\n0\n\n\nskin_color\n0\n1.00\n3\n19\n0\n31\n0\n\n\neye_color\n0\n1.00\n3\n13\n0\n15\n0\n\n\nsex\n4\n0.95\n4\n14\n0\n4\n0\n\n\ngender\n4\n0.95\n8\n9\n0\n2\n0\n\n\nhomeworld\n10\n0.89\n4\n14\n0\n48\n0\n\n\nspecies\n4\n0.95\n3\n14\n0\n37\n0\n\n\n\nVariable type: list\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nn_unique\nmin_length\nmax_length\n\n\n\n\nfilms\n0\n1\n24\n1\n7\n\n\nvehicles\n0\n1\n11\n0\n2\n\n\nstarships\n0\n1\n17\n0\n5\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nheight\n6\n0.93\n174.36\n34.77\n66\n167.0\n180\n191.0\n264\n▁▁▇▅▁\n\n\nmass\n28\n0.68\n97.31\n169.46\n15\n55.6\n79\n84.5\n1358\n▇▁▁▁▁\n\n\nbirth_year\n44\n0.49\n87.57\n154.69\n8\n35.0\n52\n72.0\n896\n▇▁▁▁▁\n\n\n\n\n\n\n\nSelecting columns with select()\n\n\n\n\n\n\nExample\n\n\n\nSuppose we wanted to take the first 3 columns only. There are a few ways to do this.\nWe could for example use numerical indices:\n\nnames(starwars)[1:3]\n\n[1] \"name\"   \"height\" \"mass\"  \n\n\nBut we can also use the names directly:\n\nsubset <- select(starwars, c(name, sex:species))\nhead(subset)\n\n# A tibble: 6 × 5\n  name           sex    gender    homeworld species\n  <chr>          <chr>  <chr>     <chr>     <chr>  \n1 Luke Skywalker male   masculine Tatooine  Human  \n2 C-3PO          none   masculine Tatooine  Droid  \n3 R2-D2          none   masculine Naboo     Droid  \n4 Darth Vader    male   masculine Tatooine  Human  \n5 Leia Organa    female feminine  Alderaan  Human  \n6 Owen Lars      male   masculine Tatooine  Human  \n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe : normally cannot be used with names or strings, but inside the select() function you can use it to specify a range of variable names.\n\n\n\nBy exclusion\nVariables can be omited using the negative sign withing select():\n\nselect( starwars, -(sex:species))\n\nThe select() function also has several helper functions that allow matching on patterns. So, for example, if you wanted to keep every variable that ends with “color”:\n\nsubset <- select(starwars, ends_with(\"color\"))\nstr(subset)\n\ntibble [87 × 3] (S3: tbl_df/tbl/data.frame)\n $ hair_color: chr [1:87] \"blond\" NA NA \"none\" ...\n $ skin_color: chr [1:87] \"fair\" \"gold\" \"white, blue\" \"white\" ...\n $ eye_color : chr [1:87] \"blue\" \"yellow\" \"red\" \"yellow\" ...\n\n\nOr all variables that start with n or m:\n\nsubset <- select(starwars, starts_with(\"n\") | starts_with(\"m\"))\nstr(subset)\n\ntibble [87 × 2] (S3: tbl_df/tbl/data.frame)\n $ name: chr [1:87] \"Luke Skywalker\" \"C-3PO\" \"R2-D2\" \"Darth Vader\" ...\n $ mass: num [1:87] 77 75 32 136 49 120 75 32 84 77 ...\n\n\nYou can also use more general regular expressions. See the help page (?select) for more details.\n\n\n\nSubsetting with filter()\nThe filter() function is used to extract subsets of rows or observations from a data frame. This function is similar to the existing subset() function in base R, or indexing by logical comparisons.\n\n\n\nArtwork by Allison Horst on filter() function\n\n\n[Source: Artwork by Allison Horst]\n\n\n\n\n\n\nExample\n\n\n\nSuppose we wanted to extract the rows of the starwars data frame where the birthyear is greater than 100:\n\nage100 <- filter(starwars, birth_year > 100)\nhead(age100)\n\n# A tibble: 5 × 14\n  name         height  mass hair_…¹ skin_…² eye_c…³ birth…⁴ sex   gender homew…⁵\n  <chr>         <int> <dbl> <chr>   <chr>   <chr>     <dbl> <chr> <chr>  <chr>  \n1 C-3PO           167    75 <NA>    gold    yellow      112 none  mascu… Tatooi…\n2 Chewbacca       228   112 brown   unknown blue        200 male  mascu… Kashyy…\n3 Jabba Desil…    175  1358 <NA>    green-… orange      600 herm… mascu… Nal Hu…\n4 Yoda             66    17 white   green   brown       896 male  mascu… <NA>   \n5 Dooku           193    80 white   fair    brown       102 male  mascu… Serenno\n# … with 4 more variables: species <chr>, films <list>, vehicles <list>,\n#   starships <list>, and abbreviated variable names ¹​hair_color, ²​skin_color,\n#   ³​eye_color, ⁴​birth_year, ⁵​homeworld\n\n\n\n\nYou can see that there are now only 5 rows in the data frame and the distribution of the birth_year values is.\n\nsummary(age100$birth_year)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    102     112     200     382     600     896 \n\n\nWe can also filter on multiple conditions: and requires both conditions to be true, whereas or requires only one to be true. This time letʻs choose birth_year < 100 and homeworld == \"Tatooine:\n\nage_tat <- filter(starwars, birth_year < 100 & homeworld == \"Tatooine\")\nselect(age_tat, name, height, mass, birth_year, sex)\n\n# A tibble: 8 × 5\n  name               height  mass birth_year sex   \n  <chr>               <int> <dbl>      <dbl> <chr> \n1 Luke Skywalker        172    77       19   male  \n2 Darth Vader           202   136       41.9 male  \n3 Owen Lars             178   120       52   male  \n4 Beru Whitesun lars    165    75       47   female\n5 Biggs Darklighter     183    84       24   male  \n6 Anakin Skywalker      188    84       41.9 male  \n7 Shmi Skywalker        163    NA       72   female\n8 Cliegg Lars           183    NA       82   male  \n\n\nOther logical operators you should be aware of include:\n\n\n\n\n\n\n\n\nOperator\nMeaning\nExample\n\n\n\n\n==\nEquals\nhomeworld == Tatooine\n\n\n!=\nDoes not equal\nhomeworld != Tatooine\n\n\n>\nGreater than\nheight > 170.0\n\n\n>=\nGreater than or equal to\nheight >= 170.0\n\n\n<\nLess than\nheight < 170.0\n\n\n<=\nLess than or equal to\nheight <= 170.0\n\n\n%in%\nIncluded in\nhomeworld %in% c(\"Tatooine\", \"Naboo\")\n\n\nis.na()\nIs a missing value\nis.na(mass)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you are ever unsure of how to write a logical statement, but know how to write its opposite, you can use the ! operator to negate the whole statement.\nA common use of this is to identify observations with non-missing data (e.g., !(is.na(homweworld))).\n\n\n\n\nSorting data with arrange()\narrange() is like the sort function in a spreadsheet, or order() in base R. arrange() reorders rows of a data frame according to one of the columns. Think of this as sorting your rows on the value of a column.\nHere we can order the rows of the data frame by birth_year, so that the first row is the earliest (oldest) observation and the last row is the latest (most recent) observation.\n\nstarwars <- arrange(starwars, birth_year)\n\nWe can now check the first few rows\n\nhead(select(starwars, name, birth_year), 3)\n\n# A tibble: 3 × 2\n  name                  birth_year\n  <chr>                      <dbl>\n1 Wicket Systri Warrick          8\n2 IG-88                         15\n3 Luke Skywalker                19\n\n\nand the last few rows.\n\ntail(select(starwars, name, birth_year), 3)\n\n# A tibble: 3 × 2\n  name           birth_year\n  <chr>               <dbl>\n1 Poe Dameron            NA\n2 BB8                    NA\n3 Captain Phasma         NA\n\n\nColumns can be arranged in descending order using the helper function desc().\n\nstarwars <- arrange(starwars, desc(birth_year))\n\nLooking at the first three and last three rows shows the dates in descending order.\n\nhead(select(starwars, name, birth_year), 3)\n\n# A tibble: 3 × 2\n  name                  birth_year\n  <chr>                      <dbl>\n1 Yoda                         896\n2 Jabba Desilijic Tiure        600\n3 Chewbacca                    200\n\ntail(select(starwars, name, birth_year), 3)\n\n# A tibble: 3 × 2\n  name           birth_year\n  <chr>               <dbl>\n1 Poe Dameron            NA\n2 BB8                    NA\n3 Captain Phasma         NA\n\n\n\n\nRenaming columns with rename()\nRenaming a variable in a data frame in R is accomplished using the names() function. The rename() function is designed to make this process easier.\nHere you can see the names of the first six variables in the starwars data frame.\n\nhead(starwars[, 1:6], 3)\n\n# A tibble: 3 × 6\n  name                  height  mass hair_color skin_color       eye_color\n  <chr>                  <int> <dbl> <chr>      <chr>            <chr>    \n1 Yoda                      66    17 white      green            brown    \n2 Jabba Desilijic Tiure    175  1358 <NA>       green-tan, brown orange   \n3 Chewbacca                228   112 brown      unknown          blue     \n\n\nSuppose we wanted to drop the _color. The syntax is newname = oldname:\n\nstarwars <- rename(starwars, hair = hair_color, skin = skin_color, eye = eye_color)\nhead(starwars[, 1:6], 3)\n\n# A tibble: 3 × 6\n  name                  height  mass hair  skin             eye   \n  <chr>                  <int> <dbl> <chr> <chr>            <chr> \n1 Yoda                      66    17 white green            brown \n2 Jabba Desilijic Tiure    175  1358 <NA>  green-tan, brown orange\n3 Chewbacca                228   112 brown unknown          blue  \n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow would you do the equivalent in base R without dplyr?\n\n\n\n\nAdding columns with mutate()\nThe mutate() function computes transformations of variables in a data frame.\n\n\n\nArtwork by Allison Horst on mutate() function\n\n\n[Source: Artwork by Allison Horst]\nFor example, we may want to adjust height for mass:\n\nstarwars <- mutate(starwars, heightsize = height / mass )\nhead(starwars)\n\n# A tibble: 6 × 15\n  name       height  mass hair  skin  eye   birth…¹ sex   gender homew…² species\n  <chr>       <int> <dbl> <chr> <chr> <chr>   <dbl> <chr> <chr>  <chr>   <chr>  \n1 Yoda           66    17 white green brown     896 male  mascu… <NA>    Yoda's…\n2 Jabba Des…    175  1358 <NA>  gree… oran…     600 herm… mascu… Nal Hu… Hutt   \n3 Chewbacca     228   112 brown unkn… blue      200 male  mascu… Kashyy… Wookiee\n4 C-3PO         167    75 <NA>  gold  yell…     112 none  mascu… Tatooi… Droid  \n5 Dooku         193    80 white fair  brown     102 male  mascu… Serenno Human  \n6 Qui-Gon J…    193    89 brown fair  blue       92 male  mascu… <NA>    Human  \n# … with 4 more variables: films <list>, vehicles <list>, starships <list>,\n#   heightsize <dbl>, and abbreviated variable names ¹​birth_year, ²​homeworld\n\n\nThere is also the related transmute() function, which mutate()s and keeps only the transformed variables. Therefore, the result is only two columns in the transmuted data frame.\n\n\nPerform functions on groups using group_by()\nThe group_by() function is used to indicate groups within the data.\nFor example, what is the average height by homeworld?\nIn conjunction with the group_by() function, we often use the summarize() function.\n\n\n\n\n\n\nNote\n\n\n\nThe general operation here is a combination of\n\nSplitting a data frame by group defined by a variable or group of variables (group_by())\nsummarize() across those subsets\n\n\n\n\n\n\n\n\n\nExample\n\n\n\nWe can create a separate data frame that splits the original data frame by homeworld.\n\nworlds <- group_by(starwars, homeworld)\n\nCompute summary statistics by planet (just showing mean and median here, almost any summary stat is available):\n\nsummarize(worlds, height = mean(height, na.rm = TRUE), \n          maxheight = max(height, na.rm = TRUE),\n          mass = median(mass, na.rm = TRUE))\n\n# A tibble: 49 × 4\n   homeworld      height maxheight  mass\n   <chr>           <dbl>     <dbl> <dbl>\n 1 Alderaan         176.      176.  64  \n 2 Aleen Minor       79        79   15  \n 3 Bespin           175       175   79  \n 4 Bestine IV       180       180  110  \n 5 Cato Neimoidia   191       191   90  \n 6 Cerea            198       198   82  \n 7 Champala         196       196   NA  \n 8 Chandrila        150       150   NA  \n 9 Concord Dawn     183       183   79  \n10 Corellia         175       175   78.5\n# … with 39 more rows\n\n\n\n\nsummarize() returns a data frame with homeworld as the first column, followed by the requested summary statistics. This is similar to the base R function aggregate().\n\n\n\n\n\n\nMore complicated example\n\n\n\nIn a slightly more complicated example, we might want to know what are the average masses within quintiles of height:\nFirst, we can create a categorical variable of height5 divided into quintiles\n\nqq <- quantile(starwars$height, seq(0, 1, 0.2), na.rm = TRUE)\nstarwars <- mutate(starwars, height.quint = cut(height, qq))\n\nNow we can group the data frame by the height.quint variable.\n\nquint <- group_by(starwars, height.quint)\n\nFinally, we can compute the mean of mass within quintiles of height.\n\nsummarize(quint, mquint = mean(mass, na.rm = TRUE))\n\n# A tibble: 6 × 2\n  height.quint mquint\n  <fct>         <dbl>\n1 (66,165]       44.2\n2 (165,175]     187. \n3 (175,183]      79.2\n4 (183,193]      80.2\n5 (193,264]     106. \n6 <NA>           17  \n\n\n\n\nOddly enough there is a maximum mass in the second height quintile of Starwars characters. The biologist in me thinks maybe outliers?\n\n\nPiping multiple functions using %>%\nThe pipe operator %>% is very handy for stringing together multiple dplyr functions in a sequence of operations. It comes from the magritter package.\n Source:\nIn base R, there are two styles of applying multiple functions. The first is the resave the object after each operation.\nThe second is to nest functions, with the first at the deepest level (the heart of the onion), then working our way out:\n\nthird(second(first(x)))\n\nThe %>% operator allows you to string operations in a left-to-right fashion, where the output of one flows into the next, i.e.:\n\nfirst(x) %>% second %>% third\n\n\n\n\n\n\n\nExample\n\n\n\nTake the example that we just did in the last section.\nThat can be done with the following sequence:\n\nstarwars %>% \n  group_by(homeworld) %>% \n  summarize(height = mean(height, na.rm = TRUE), \n          maxheight = max(height, na.rm = TRUE),\n          mass = median(mass, na.rm = TRUE))\n\n# A tibble: 49 × 4\n   homeworld      height maxheight  mass\n   <chr>           <dbl>     <dbl> <dbl>\n 1 Alderaan         176.      176.  64  \n 2 Aleen Minor       79        79   15  \n 3 Bespin           175       175   79  \n 4 Bestine IV       180       180  110  \n 5 Cato Neimoidia   191       191   90  \n 6 Cerea            198       198   82  \n 7 Champala         196       196   NA  \n 8 Chandrila        150       150   NA  \n 9 Concord Dawn     183       183   79  \n10 Corellia         175       175   78.5\n# … with 39 more rows\n\n\n\n\n\nData masking\nNotice that we did not have to specify the dataframe. This is because dplyr functions are built on a data masking syntax. From the dplyr data-masking help page:\n\nData masking allows you to refer to variables in the “current” data frame (usually supplied in the .data argument), without any other prefix. It’s what allows you to type (e.g.) filter(diamonds, x == 0 & y == 0 & z == 0) instead of diamonds[diamonds$x == 0 & diamonds$y == 0 & diamonds$z == 0, ]\n\nWhen you look at the help page for ?mutate for example, you will see a function definition like so:\n\nmutate(.data, ...)\n\nNote the .data, Which means that the data can be supplied as usual, or it can be inherited from the “current” data frame which is passed to it via a pipe.\n\n\n\nSample rows of data with slice_*()\nThe slice_sample() function will randomly sample rows of data.\nThe number of rows to show is specified by the n argument.\n\nThis can be useful if you do not want to print the entire tibble, but you want to get a greater sense of the variation.\n\n\n\n\n\n\n\nExample\n\n\n\n\nslice_sample(starwars, n = 10)\n\n# A tibble: 10 × 16\n   name      height  mass hair  skin  eye   birth…¹ sex   gender homew…² species\n   <chr>      <int> <dbl> <chr> <chr> <chr>   <dbl> <chr> <chr>  <chr>   <chr>  \n 1 Jek Tono…    180 110   brown fair  blue       NA male  mascu… Bestin… Human  \n 2 Ric Olié     183  NA   brown fair  blue       NA <NA>  <NA>   Naboo   <NA>   \n 3 Poggle t…    183  80   none  green yell…      NA male  mascu… Geonos… Geonos…\n 4 Luminara…    170  56.2 black yell… blue       58 fema… femin… Mirial  Mirial…\n 5 Lama Su      229  88   none  grey  black      NA male  mascu… Kamino  Kamino…\n 6 Nien Nunb    160  68   none  grey  black      NA male  mascu… Sullust Sullus…\n 7 Jango Fe…    183  79   black tan   brown      66 male  mascu… Concor… Human  \n 8 Gregar T…    185  85   black dark  brown      NA male  mascu… Naboo   Human  \n 9 Roos Tar…    224  82   none  grey  oran…      NA male  mascu… Naboo   Gungan \n10 Ben Quad…    163  65   none  grey… oran…      NA male  mascu… Tund    Toong  \n# … with 5 more variables: films <list>, vehicles <list>, starships <list>,\n#   heightsize <dbl>, height.quint <fct>, and abbreviated variable names\n#   ¹​birth_year, ²​homeworld\n\n\n\n\nYou can also use slice_head() or slice_tail() to take a look at the top rows or bottom rows of your tibble. Again the number of rows can be specified with the n argument.\nThis will show the first 5 rows.\n\nslice_head(starwars, n = 5)\n\n# A tibble: 5 × 16\n  name       height  mass hair  skin  eye   birth…¹ sex   gender homew…² species\n  <chr>       <int> <dbl> <chr> <chr> <chr>   <dbl> <chr> <chr>  <chr>   <chr>  \n1 Yoda           66    17 white green brown     896 male  mascu… <NA>    Yoda's…\n2 Jabba Des…    175  1358 <NA>  gree… oran…     600 herm… mascu… Nal Hu… Hutt   \n3 Chewbacca     228   112 brown unkn… blue      200 male  mascu… Kashyy… Wookiee\n4 C-3PO         167    75 <NA>  gold  yell…     112 none  mascu… Tatooi… Droid  \n5 Dooku         193    80 white fair  brown     102 male  mascu… Serenno Human  \n# … with 5 more variables: films <list>, vehicles <list>, starships <list>,\n#   heightsize <dbl>, height.quint <fct>, and abbreviated variable names\n#   ¹​birth_year, ²​homeworld\n\n\nThis will show the last 5 rows.\n\nslice_tail(starwars, n = 5)\n\n# A tibble: 5 × 16\n  name       height  mass hair  skin  eye   birth…¹ sex   gender homew…² species\n  <chr>       <int> <dbl> <chr> <chr> <chr>   <dbl> <chr> <chr>  <chr>   <chr>  \n1 Finn           NA    NA black dark  dark       NA male  mascu… <NA>    Human  \n2 Rey            NA    NA brown light hazel      NA fema… femin… <NA>    Human  \n3 Poe Damer…     NA    NA brown light brown      NA male  mascu… <NA>    Human  \n4 BB8            NA    NA none  none  black      NA none  mascu… <NA>    Droid  \n5 Captain P…     NA    NA unkn… unkn… unkn…      NA <NA>  <NA>   <NA>    <NA>   \n# … with 5 more variables: films <list>, vehicles <list>, starships <list>,\n#   heightsize <dbl>, height.quint <fct>, and abbreviated variable names\n#   ¹​birth_year, ²​homeworld\n\n\n\n\n\nSummary\nThe dplyr package provides an alternative syntax for manipulating data frames. In particular, we can often conduct the beginnings of an exploratory analysis with the powerful combination of group_by() and summarize().\nOnce you learn the dplyr grammar there are a few additional benefits\n\ndplyr can work with other data frame “back ends” such as SQL databases. There is an SQL interface for relational databases via the DBI package\ndplyr can be integrated with the data.table package for large fast tables\nMany people like the piping syntax for readability and clarity\n\n\n\nPost-lecture materials\n\nFinal Questions\n\n\n\n\n\n\nQuestions\n\n\n\n\nHow can you tell if an object is a tibble?\nUsing the trees dataset in base R (this dataset stores the girth, height, and volume for Black Cherry Trees) and using the pipe operator:\n\nconvert the data.frame to a tibble.\nfilter for rows with a tree height of greater than 70, and\norder rows by Volume (smallest to largest).\n\n\n\nhead(trees)\n\n  Girth Height Volume\n1   8.3     70   10.3\n2   8.6     65   10.3\n3   8.8     63   10.2\n4  10.5     72   16.4\n5  10.7     81   18.8\n6  10.8     83   19.7\n\n\n\n\n\n\nAdditional Resources\n\n\n\n\n\n\nTip\n\n\n\n\nhttps://jhudatascience.org/tidyversecourse/wrangle-data.html#data-wrangling\ndplyr cheat sheet from RStudio"
  },
  {
    "objectID": "projects/2023-01-24-project-0/index.html",
    "href": "projects/2023-01-24-project-0/index.html",
    "title": "Project 0 (optional)",
    "section": "",
    "text": "This exercise is modified from material developed by Stephanie Hicks."
  },
  {
    "objectID": "projects/2023-02-16-project-1/index.html",
    "href": "projects/2023-02-16-project-1/index.html",
    "title": "Project 1",
    "section": "",
    "text": "This exercise is modified from material developed by Andreas Handel."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Project 0 (optional)\n\n\n\n\n\n\n\nproject 0\n\n\nprojects\n\n\n\n\nInformation for Project 0 (entirely optional, but hopefully useful and fun!)\n\n\n\n\n\n\nJan 23, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n\n\nProject 1\n\n\n\n\n\n\n\nproject 1\n\n\nprojects\n\n\n\n\nInformation for Project 1\n\n\n\n\n\n\nJan 23, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Introduction to Data Science in R for Biologists!",
    "section": "",
    "text": "Welcome to Data Science for Biologists at the University of Hawaiʻi!"
  },
  {
    "objectID": "index.html#what-is-this-course",
    "href": "index.html#what-is-this-course",
    "title": "Welcome to Introduction to Data Science in R for Biologists!",
    "section": "What is this course?",
    "text": "What is this course?\nThis course covers the basics of computational and programming skills required for research in biological sciences and related disciplines. We will cover practical issues in data organization and management as well as programming in R and the tidyverse. Some of the topics will include: data ethics, best practices for coding and reproducible research, introduction to data visualizations, best practices for working with special data types (dates/times, text data, etc), best practices for storing data, basics of debugging, organizing and commenting code, basics of interacting with other computational resources from R. Topics in statistical data analysis, morphometrics, phylogenetic tree visualization, and other practical examples provide working examples."
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Welcome to Introduction to Data Science in R for Biologists!",
    "section": "Getting started",
    "text": "Getting started\nPlease look over the Syllabus and Schedule under General Information. Lectures are provided under the course materials tab."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Welcome to Introduction to Data Science in R for Biologists!",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis course was developed and is maintained by Marguerite Butler.\nA big thank you to Stephanie Hicks for generously sharing the beautifully designed quarto template for this course.\nMaterials have been adapted from courses developed by the following individuals (more to come): Stephanie Hicks.\nThe course materials are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Linked and embedded materials are governed by their own licenses. I assume that all external materials used or embedded here are covered under the educational fair use policy. If this is not the case and any material displayed here violates copyright, please let me know and I will remove it."
  },
  {
    "objectID": "index.html#useful-free-r-resources",
    "href": "index.html#useful-free-r-resources",
    "title": "Welcome to Introduction to Data Science in R for Biologists!",
    "section": "Useful (Free) R Resources",
    "text": "Useful (Free) R Resources\nIntro to R (by the R Core Group): https://cran.r-project.org/doc/manuals/r-release/R-intro.html R for Data Science: http://r4ds.had.co.nz/ Intro to Data Science: http://rafalab.dfci.harvard.edu/dsbook/ Various “Cheat Sheets”: https://www.rstudio.com/resources/cheatsheets/ DataCamp: http://www.datacamp.com R reference card: http://cran.r-project.org/doc/contrib/Short-refcard.pdfUCLA R Data Import/Export (by the R Core Group): https://cran.r-project.org/doc/manuals/r-release/R-data.html"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "lectures.html",
    "href": "lectures.html",
    "title": "Lectures",
    "section": "",
    "text": "A small review of univariate parametric statistics\n\n\n\n\n\n\n\nmodule 3\n\n\nweek 8\n\n\nunivariate\n\n\nstatistics\n\n\nggplot2\n\n\ndplyr\n\n\n\n\nAdd\n\n\n\n\n\n\nMar 7, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n  \n\n\n\n\nA small tour of multivariate analysis\n\n\n\n\n\n\n\nmodule 3\n\n\nweek 7\n\n\nmultivariate\n\n\nstatistics\n\n\nR\n\n\n\n\nAdd\n\n\n\n\n\n\nFeb 28, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n  \n\n\n\n\nGetting data in shape with dplyr\n\n\n\n\n\n\n\nmodule 3\n\n\nweek 7\n\n\ntidyr\n\n\ntidyverse\n\n\ndplyr\n\n\ntibble\n\n\npipe\n\n\n\n\nAdd\n\n\n\n\n\n\nFeb 28, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n  \n\n\n\n\nThe ggplot2 package\n\n\n\n\n\n\n\nmodule 3\n\n\nweek 7\n\n\nR\n\n\nprogramming\n\n\nplotting\n\n\nggplot2\n\n\ndata visualization\n\n\n\n\nIntroduction to the gplot2 grammar of graphics\n\n\n\n\n\n\nFeb 21, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting Systems\n\n\n\n\n\n\n\nmodule 3\n\n\nweek 6\n\n\nplotting\n\n\nggplot2\n\n\nlattice\n\n\ndata visualization\n\n\n\n\nShowing you base plotting, lattice, and ggplot2\n\n\n\n\n\n\nFeb 16, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n  \n\n\n\n\nTidying and Exploring Data\n\n\n\n\n\n\n\nmodule 2\n\n\nweek 4\n\n\ndata\n\n\ndata structures\n\n\nobjects\n\n\n\n\nAll about data and how it is represented in R\n\n\n\n\n\n\nFeb 14, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTypes of Data\n\n\n\n\n\n\n\nmodule 2\n\n\nweek 4\n\n\ndata\n\n\ndata structures\n\n\nobjects\n\n\n\n\nAll about data and how it is represented in R\n\n\n\n\n\n\nFeb 9, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSaving your work as R scripts\n\n\n\n\n\n\n\nmodule 1\n\n\nweek 3\n\n\nR\n\n\nscripts\n\n\nreproducibility\n\n\n\n\nAdd\n\n\n\n\n\n\nFeb 7, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n  \n\n\n\n\nWhat is the question?\n\n\n\n\n\n\n\nmodule 2\n\n\nweek 3\n\n\ndata\n\n\nquestions\n\n\ndata mining\n\n\n\n\nLetʻs talk about scientific excellence & what data can and cannot do\n\n\n\n\n\n\nJan 31, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n  \n\n\n\n\nData IO\n\n\n\n\n\n\n\nmodule 2\n\n\nweek 3\n\n\ndata\n\n\ninput\n\n\noutput\n\n\nformats\n\n\n\n\nSo many ways to get data into R\n\n\n\n\n\n\nJan 31, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReference management\n\n\n\n\n\n\n\nmodule 1\n\n\nweek 3\n\n\nQuarto\n\n\nauthoring\n\n\nBibTeX\n\n\nprogramming\n\n\n\n\nHow to use citations and include your bibliography in R Quarto.\n\n\n\n\n\n\nJan 31, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReproducible Research\n\n\n\n\n\n\n\nmodule 1\n\n\nweek 2\n\n\nR\n\n\nreproducibility\n\n\n\n\nIntroduction to reproducible research\n\n\n\n\n\n\nJan 26, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n  \n\n\n\n\nLiterate Statistical Programming and Quarto\n\n\n\n\n\n\n\nmodule 1\n\n\nweek 2\n\n\nMarkdown\n\n\nQuarto\n\n\nprogramming\n\n\n\n\nIntroduction to literate statistical programming tools including Quarto Markdown\n\n\n\n\n\n\nJan 26, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to git/GitHub\n\n\n\n\n\n\n\nmodule 1\n\n\nweek 2\n\n\nprogramming\n\n\nversion control\n\n\ngit\n\n\nGitHub\n\n\n\n\nVersion control is a game changer; or how I learned to love git/GitHub\n\n\n\n\n\n\nJan 24, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to your computerʻs terminal utilities\n\n\n\n\n\n\n\nmodule 1\n\n\nweek 1\n\n\nprogramming\n\n\nfilesystem\n\n\nshell\n\n\n\n\nSo much power; or how I got my computer to do my bidding\n\n\n\n\n\n\nJan 19, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction and The Big Idea\n\n\n\n\n\n\n\nmodule 1\n\n\nweek 1\n\n\nintroduction\n\n\n\n\nThe big idea\n\n\n\n\n\n\nJan 17, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "",
    "text": "Delivery: In person\nCourse time: Tuesdays and Thursdays from 10:30-11:45am\nCourse location: BIL 319A\nAssignments: Weekly small quizzes, four projects\n\n\n\n\nTo add the course: Let me know so I can give you an override\nRegister for ZOOL710 CRN 85424 3 credits.\nAttendance is highly recommended for group work. Lectures will be recorded on request.\nPlease contact course instructor if interested in auditing.\nUndergraduates are welcome to join with approval.\n\n\n\n\n\nMarguerite A. Butler (https://butlerlab.org)\n\nOffice Location: Edmondson 318\nEmail: mbutler808 at gmail.com\nOffice Hours: After class and by appointment\n\n\n\n\n\nIn order of preference, here is a preferred list of ways to get help:\n\nI strongly encourage you to use the course SLACK channel, before joining office hours. You can get your answers faster, and other students in the class (who likely have similar questions) can also benefit from the questions and answers given. Everyone is encouraged to participate.\nYou are welcome to join office hours to get more group interactive feedback.\nIf you are not able to make the office hours, appointments can be made by email."
  },
  {
    "objectID": "syllabus.html#important-links",
    "href": "syllabus.html#important-links",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "Important Links",
    "text": "Important Links\n\nCourse website: coming soon.\nGitHub repository with all course material: coming soon."
  },
  {
    "objectID": "syllabus.html#learning-objectives",
    "href": "syllabus.html#learning-objectives",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "Learning Objectives:",
    "text": "Learning Objectives:\nUpon successfully completing this course, students will be able to:\n\nInstall and configure software necessary for a statistical programming environment\nDiscuss generic programming language concepts as they are implemented in a high-level statistical language\nWrite and debug code in base R and the tidyverse\nBuild basic data visualizations using R and the tidyverse\nDiscuss best practices for coding and reproducible research, basics of data ethics and management, basics of working with special data types, and basics of storing data"
  },
  {
    "objectID": "syllabus.html#lectures",
    "href": "syllabus.html#lectures",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "Lectures",
    "text": "Lectures\nIn Fall 2023, we will have in person lectures. If requested, I will attempt to record them and post to the website."
  },
  {
    "objectID": "syllabus.html#textbook-and-other-course-material",
    "href": "syllabus.html#textbook-and-other-course-material",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "Textbook and Other Course Material",
    "text": "Textbook and Other Course Material\nThere is no required textbook. We will make use of several freely available textbooks and other materials. All course materials will be provided. We will use the R software for data analysis, and git for version control and data sharing, all of which is freely available for download."
  },
  {
    "objectID": "syllabus.html#software",
    "href": "syllabus.html#software",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "Software",
    "text": "Software\nPlease install R onto your laptop. You can obtain R from the Comprehensive R Archive Network. There are versions available for Mac, Windows, and Unix/Linux. This software is required for this course.\nIt is important that you have the latest version of R installed. For this course we will be using R version 4.2.1 or higher. You can determine what version of R you have by starting up R and typing into the console R.version.string and hitting the return/enter key. If you do not have the proper version of R installed, go to CRAN and download and install the latest version.\nSome students like to use the Rstudio interface, but this is optional. The RStudio interactive development environment (IDE) requires that R be installed, and so is an “add-on” to R. You can obtain the RStudio Desktop for free from the RStudio web site. You can determine the version of RStudio by looking at menu item Help > About RStudio. You should be using RStudio version 1.4.1106 or higher."
  },
  {
    "objectID": "syllabus.html#quizzes",
    "href": "syllabus.html#quizzes",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "Quizzes",
    "text": "Quizzes\nThere will be weekly (short) quizzes on Laulima. These are intended to be low-stakes to assist you in checking your understanding."
  },
  {
    "objectID": "syllabus.html#projects",
    "href": "syllabus.html#projects",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "Projects",
    "text": "Projects\nThere will be one optional assignment and 4 graded assignments, due every 2–3 weeks. Projects will be submitted electronically via GitHub (more on this later).\nThe project assignments will be due on\n\nProject 0: February 3, 11:59pm (optional and not graded but hopefully useful and fun)\nProject 1: February 17, 11:59pm\nProject 2: March 10, 11:59pm\nProject 3: April 7, 11:59pm\nProject 4: May 5, 11:59pm\n\n\nProject collaboration\nPlease feel free to study together and talk to one another about project assignments. The mutual instruction that students give each other is among the most valuable that can be achieved.\nHowever, it is expected that project assignments will be implemented and written up independently unless otherwise specified. Specifically, please do not share analytic code or output. Please do not collaborate on write-up and interpretation. Please do not access or use solutions from any source before your project assignment is submitted for grading."
  },
  {
    "objectID": "syllabus.html#discussion-forum",
    "href": "syllabus.html#discussion-forum",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "Discussion Forum",
    "text": "Discussion Forum\nThe course will make use of SLACK to ask and answer questions and discuss any of the course materials. Please engage and provide answers as well as questions. The Instructor will monitor SLACK and answer questions when appropriate."
  },
  {
    "objectID": "syllabus.html#exams",
    "href": "syllabus.html#exams",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "Exams",
    "text": "Exams\nThere are no exams in this course."
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "Grading",
    "text": "Grading\nGrades in the course will be based on weekly quizzes (20%), participation (10%) and projects (70%). Each of Projects 1–3 counts approximately equally in the final grade. Grades will be posted on Laulima."
  },
  {
    "objectID": "syllabus.html#policy-for-submitted-projects-late",
    "href": "syllabus.html#policy-for-submitted-projects-late",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "Policy for submitted projects late",
    "text": "Policy for submitted projects late\nThe policy for late submissions is as follows:\n\nEach student will be given two free “late days” for the rest of the course.\nA late day extends the individual project deadline by 24 hours without penalty.\nThe late days can be applied to just one project (e.g. two late days for Project 2), or they can be split across the two projects (one late day for Project 2 and one late day for Project 3). This is entirely left up to the discretion of the student.\nLate days are intended to give you flexibility: you can use them for any reason no questions asked.\nYou do not get any bonus points for not using your late days, and they are not transferrable.\n\nFor students who exceed their free late days:\n\nI will be deducting 5% for each extra late day. For example, if you have already used all of your late days for the term, we will deduct 5% for the assignment that is <24 hours late, 10% points for the assignment that is 24-48 hours late, and 15% points for the assignment that is 48-72 hours late, etc.\nI will not grade assignments that are more than 3 days past the original due date.\n\n\nRegrading Policy\nIt is very important to me that all assignments are properly graded. If you believe there is an error in your assignment grading, please send an email within 7 days of receiving the grade explaining the issue. No re-grade requests will be accepted orally, and no regrade requests will be accepted more than 7 days after you receive the grade for the assignment."
  },
  {
    "objectID": "syllabus.html#academic-ethics-and-student-conduct-code",
    "href": "syllabus.html#academic-ethics-and-student-conduct-code",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "Academic Ethics and Student Conduct Code",
    "text": "Academic Ethics and Student Conduct Code\nThe faculty, staff, and students participating in courses of the School of Life Sciences assume a responsibility to uphold the Universityʻs missions of academic excellence and social responsibility as appropriate for an institute of higher education. Violations of the UH Systemwide Student Conduct Code includes but is not limited to: cheating; plagiarism; providing copies of your work to other students which is submitted as their own; obtaining copies of said work by others; using copies of said work or representing any portion of another person’s work as your own (i.e., plagiarism); misconduct. While we encourage you to discuss strategies for problem solving, and even collaborate by working through the problems/strategies together, giving someone all the answers is cheating. If you are unsure please ask.\nPlagiarism is when you use information or present ideas, whether by paraphrase or direct quote, from a source (be it published or a classmate) without giving proper credit to that source. Cheating in any way will be reported to the attention of UH Office of Judicial Affairs, and result in an F in this course. Students should be familiar with the policies and procedures specified under the Systemwide Student Conduct Code portal."
  },
  {
    "objectID": "syllabus.html#disability-support-service",
    "href": "syllabus.html#disability-support-service",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "Disability Support Service",
    "text": "Disability Support Service\nStudents requiring accommodations for disabilities should register with the Kokua program at Student Disability Services. It is the responsibility of the student to register for accommodations. The Kokua office will send me a notification once you are registered, however, they often do not share information regarding the specifics. If the accommodations are not sufficient to ensure your success, please contact me as soon as possible so that we may work together on providing for an effective learning environment."
  },
  {
    "objectID": "syllabus.html#prerequisites",
    "href": "syllabus.html#prerequisites",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "Prerequisites",
    "text": "Prerequisites\nThis is an applied quantitative course. I will not discuss the mathematical details of specific data analysis approaches, however some statistical background and being comfortable with quantitative thinking is useful. Previous experience with writing computer programs in general and R in particular is also helpful, but not necessary. If you have no programming experience, expect to spend extra time getting yourself familiar with R. As long as you are willing to invest the time to learn the programming and you do not mind thinking quantitatively, you should be able to take the course, independent of your background.\n\nGetting set up\nYou must install R and RStudio on your computer in order to complete this course. These are two different applications that must be installed separately before they can be used together:\n\nR is the core underlying programming language and computing engine that we will be learning in this course\nRStudio is an interface into R that makes many aspects of using and programming R simpler\n\nBoth R and RStudio are available for Windows, macOS, and most flavors of Unix and Linux. Please download the version that is suitable for your computing setup.\nThroughout the course, we will make use of numerous R add-on packages that must be installed over the Internet. Packages can be installed using the install.packages() function in R. For example, to install the tidyverse package, you can run\n\ninstall.packages(\"tidyverse\")\n\nin the R console.\n\nHow to Download R for Windows\nGo to https://cran.r-project.org and\n\nClick the link to “Download R for Windows”\nClick on “base”\nClick on “Download R 4.2.1 for Windows”\n\n\n\n\n\n\n\nWarning\n\n\n\nFor all software, please download the latest version.\n\n\n\n\nHow to Download R for the Mac\nGoto https://cran.r-project.org and\n\nClick the link to “Download R for (Mac) OS X”.\nClick on “R-4.2.1.pkg” (or the latest version)\n\n\n\nHow to Download RStudio\nGoto https://rstudio.com and\n\nClick on “Products” in the top menu\nThen click on “RStudio” in the drop down menu\nClick on “RStudio Desktop”\nClick the button that says “DOWNLOAD RSTUDIO DESKTOP”\nClick the button under “RStudio Desktop” Free\nUnder the section “All Installers” choose the file that is appropriate for your operating system."
  },
  {
    "objectID": "syllabus.html#general-disclaimers",
    "href": "syllabus.html#general-disclaimers",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "General Disclaimers",
    "text": "General Disclaimers\n\nThis syllabus is a general plan, deviations announced to the class by the instructor may be necessary."
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Learning R\n\nBig Book of R: https://www.bigbookofr.com\nList of resources to learn R (but also Python, SQL, Javascript): https://github.com/delabj/datacamp_alternatives/blob/master/index.md\nlearnr4free. Resources (books, videos, interactive websites, papers) to learn R. Some of the resources are beginner-friendly and start with the installation process: https://www.learnr4free.com/en\nData Science with R by Danielle Navarro: https://robust-tools.djnavarro.net"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "For Qmd files (markdown document with Quarto which are cross-language executable code), go to the course GitHub repository and navigate the directories, or best of all clone the repo and navigate within RStudio or your text browser and R console.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDates\nTopics\nProjects\n\n\n\n\n\nModule 1\n\nComputational Tools for Data Science\n\n\n\n\n\n\n\n\n\n\n\nWeek 1\nJan 17\n👋 Installing R/RStudio, GitHub [html] [Qmd]\n\n\n\n\n\nJan 19\n👩‍💻 Shell scripts, File Organization [html] [Qmd]\n\n\n\n\nWeek 2\nJan 24\n🐙 Intro to Git and GitHub [html] [Qmd]\n🌴 Project 0 [html] [Qmd]\n\n\n\n\n\n\n\n\n\n\nModule 2\n\nReproducible Research\n\n\n\n\n\n\n\n\n\n\n\n\nJan 26\n🐙 Reproducible Research [html] [Qmd]\n\n\n\n\n\nJan 26\n🐙 Literate Programming / Intro to Quarto [html] [Qmd]\n\n\n\n\nWeek 3\nJan 31\n🔬 What is the Question? [html] [Qmd]\n\n\n\n\n\n\n\n\n\n\n\nModule 3\n\nData Analysis in R \n\n\n\n\n\n\n\n\n\n\n\n\nJan 31\n👓 Reading/Writing data [html] [Qmd]\n\n\n\n\n\nFeb 2\n👓 Reference Management [html] [Qmd]\n🍂 Project 0 due\n\n\n\nWeek 4\nFeb 7\n🐙 Scripts [html] [Qmd]\n\n\n\n\n\nFeb 9\n🔩 Data [html] [Qmd]\n\n\n\n\n\n\n\n\n\n\n\nModule 3\n\nData Visulization in R\n\n\n\n\n\n\n\n\n\n\n\nWeek 5\nFeb 14\n🔪 Tidying Data [html] [Qmd]\n\n\n\n\n\nFeb 16\n📊 Plotting Systems (Base R) [html] [Qmd]\n🌱 Project 1 assigned\n\n\n\n\n\n\n\n\n\n\nWeek 6\nFeb 21\n📊 Plotting with ggplot2 [html] [Qmd]\n\n\n\n\n\nFeb 23\n📊 ggplot2 continued [html] [Qmd]\n\n\n\n\n\n\n\n\n\n\n\nModule 4\n\nNuts and Bolts of R\n\n\n\n\n\n\n\n\n\n\n\nWeek 7\nFeb 28\n🐒 Tidyverse and data wrangling [html] [Qmd]\n\n\n\n\n\nMar 2\n🐎 A tour of some multivariate methods in R [html] [Qmd]\nemoji('seedling') Project 1 due\n\n\n\n\n\n\n\n\n\n\nModule 5\n\nSpecial Data Types\n\n\n\n\n\n\n\n\n\n\n\nWeek 8\nMar 7\n📆 Dates and Times\n\n\n\n\n\nMar 9\n🐎 Regular Expressions\n🍂 Project 2 due\n\n\n\n\n\n\n\n\n\n\nWeek 9\nMar 14\n🤝 Spring Break\n\n\n\n\n\nMar 16\n☀️ Spring Break\n\n\n\n\n\n\n\n\n\n\n\nWeek 10\nMar 21\n🍄 Factors\n\n\n\n\n\n\n\n\n\n\n\n\nMar 23\n🌵 Text Data and Strings\n\n\n\n\n\n\n\n\n\n\n\nModule 6\n\nIntro to Morphometrics\n\n\n\n\n\n\n\n\n\n\n\nWeek 11\nMar 28\n📏 Linear Morphometrics, Size, Shape\n\n\n\n\n\n\n\n\n\n\n\n\nMar 30\n📐 Landmark-based Morphometrics\n\n\n\n\n\n\n\n\n\n\n\nWeek 12\nApr 4\n💀 3D Morphometrics\n\n\n\n\n\nApr 6\n🐾 3D Morphometrics\n\n\n\n\n\n\n\n\n\n\n\nModule 7\n\nPhylogenetic Trees\n\n\n\n\n\n\n\n\n\n\n\nWeek 13\nApr 11\n🌱 Trees as Data Objects\n🍂 Project 3 due\n\n\n\n\nApr 13\n🌴 Reading and writing trees, ape, ggtree, treeio\n\n\n\n\n\n\n\n\n\n\n\nWeek 14\nApr 18\n🌳 Reshaping Trees\n\n\n\n\n\n\n\n\n\n\n\n\nApr 20\n🌺 Annotating Tree Plots\n\n\n\n\n\n\n\n\n\n\n\nModule 8\n\nOther Topics\n\n\n\n\n\n\n\n\n\n\n\nWeek 15\nApr 25\n🐍 Parsing Data from any Format\n\n\n\n\n\n\n\n\n\n\n\n\nApr 27\n🌰 Student choice topic\n\n\n\n\n\n\n\n\n\n\n\nWeek 16\nMay 2\n🏆 Final Presentations\n🎊 Project 4 due"
  },
  {
    "objectID": "posts/2023-03-07-univariate/index.html",
    "href": "posts/2023-03-07-univariate/index.html",
    "title": "A small review of univariate parametric statistics",
    "section": "",
    "text": "Learning objectives\n\n\n\nAt the end of this lesson you will:\n\nBe able to perform basic univariate statistics\nBe able to design plots to display univariate comparisons\nBe able to relate questions to graphical representations of data"
  },
  {
    "objectID": "posts/2023-03-07-univariate/index.html#linear-regression",
    "href": "posts/2023-03-07-univariate/index.html#linear-regression",
    "title": "A small review of univariate parametric statistics",
    "section": "Linear Regression",
    "text": "Linear Regression\nLinear regression asks whether there is a relationship between X and Y, that is if you know X can you predict the value of Y?\n\nwith(iris, plot(Sepal.Width ~ Sepal.Length))\nlm.fit <- with(iris, lm(Sepal.Width ~ Sepal.Length))\nabline(lm.fit, col=\"blue\")\n\n\n\n\nLinear regression results in two parameters, the best-fit slope and intercept:\n\nlm.fit \n\n\nCall:\nlm(formula = Sepal.Width ~ Sepal.Length)\n\nCoefficients:\n (Intercept)  Sepal.Length  \n     3.41895      -0.06188  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sepal.Width ~ Sepal.Length)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1095 -0.2454 -0.0167  0.2763  1.3338 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   3.41895    0.25356   13.48   <2e-16 ***\nSepal.Length -0.06188    0.04297   -1.44    0.152    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4343 on 148 degrees of freedom\nMultiple R-squared:  0.01382,   Adjusted R-squared:  0.007159 \nF-statistic: 2.074 on 1 and 148 DF,  p-value: 0.1519\n\n\nRegression minimizes the sum of squared errors (or deviations) from the line. The “errors” are the difference between where Y is, and where Y should be if it followed a perfect line.\nWe can illustrate what this means:\n\nx <- iris$Sepal.Length\ny <- iris$Sepal.Width\nyhat <- predict(lm.fit)\n\nwith(iris, plot(Sepal.Width ~ Sepal.Length))\nwith(iris, abline(lm.fit, col=\"blue\"))\nfor(i in 1:length(x))  lines(x[c(i,i)],c(y[i], yhat[i]), col=\"red\", lty=2)\n\n\n\n\nThe regression line is the best fit line that minimizes the sums of squared deviations from the regression. It turns out that the least-squares fit of the regression line is also provides the Maximum Likelihood fits of the parameters of the line (the slope and intercept)."
  },
  {
    "objectID": "posts/2023-03-07-univariate/index.html#anova",
    "href": "posts/2023-03-07-univariate/index.html#anova",
    "title": "A small review of univariate parametric statistics",
    "section": "ANOVA",
    "text": "ANOVA\nAnalysis of variance is very closely related to regression. It also works by minimizing sums of squares, but it asks a different question.\nDoes the data better fit a model with one group (one regression line?) or multiple groups (multiple regression lines, one for each group)?\nGraphically, it looks like the plot below with the question Is the data explained better by a single group with a grand mean? or with separate means for each Species?\n\npar(mfrow=c(1,2))\nwith(iris, boxplot(Sepal.Length))\nwith(iris, plot(Sepal.Length ~ Species))\n\n\n\n\n\nIs One of these Groups not like the others?\nStatistically, this is asking whether the sums of squares is minimized by assuming there is only one group (one mean)? Or three groups?\nFor this plot we will add an index column (1 to number of observations), and use ggplot2, dplyr, and the pipe from magrittr\n\nrequire(dplyr)\nrequire(magrittr)\nrequire(ggplot2)\n\ndat <- cbind(iris, id = 1:length(iris$Species))\nyhat <- mean(iris$Sepal.Length)  # grand mean of Sepal Length\n\np <- dat %>% ggplot(aes( x = id, y = Sepal.Length, group=Species)) \n\nq <- p + geom_point( size=2) + \n  geom_hline( aes(yintercept = mean(iris$Sepal.Length)) ) + \n  geom_segment( data=dat, aes( x = id, y = Sepal.Length, xend = id, yend = yhat), color=\"red\", lty = 3)\n\nq  \n\n\n\n\nWe added two new ggplot2 functions:\n\ngeom_hline() which adds a horizontal line used for the grand mean. This is similar to base R abline()\n\ngeom_segment() which plots line segments indicated by x,y start points and xend,yend end points (this is based on the base R segment() function)\n\nTo add in the group structure, we need to compute means by species, and know where one species ends and the other begins in the data vector. We can do this with group_by() and summarize():\n\nspmeans  <- dat %>% group_by(Species) %>% \n        summarise(\n          sl = mean(Sepal.Length),\n          n = length(Sepal.Length),\n          minid = min(id),\n          maxid = max(id)\n        )\n\nspmeans\n\n# A tibble: 3 × 5\n  Species       sl     n minid maxid\n  <fct>      <dbl> <int> <int> <int>\n1 setosa      5.01    50     1    50\n2 versicolor  5.94    50    51   100\n3 virginica   6.59    50   101   150\n\n\n(You should always check that minid and maxid is what you intended)\nWe want to include this mean information in the dataframe, so to add it as a vector, we can merge():\n\ndat <- merge(dat, spmeans)\nhead(dat)\n\n  Species Sepal.Length Sepal.Width Petal.Length Petal.Width id    sl  n minid\n1  setosa          5.1         3.5          1.4         0.2  1 5.006 50     1\n2  setosa          4.9         3.0          1.4         0.2  2 5.006 50     1\n3  setosa          4.7         3.2          1.3         0.2  3 5.006 50     1\n4  setosa          4.6         3.1          1.5         0.2  4 5.006 50     1\n5  setosa          5.0         3.6          1.4         0.2  5 5.006 50     1\n6  setosa          5.4         3.9          1.7         0.4  6 5.006 50     1\n  maxid\n1    50\n2    50\n3    50\n4    50\n5    50\n6    50\n\ndat[45:55,]\n\n      Species Sepal.Length Sepal.Width Petal.Length Petal.Width id    sl  n\n45     setosa          5.1         3.8          1.9         0.4 45 5.006 50\n46     setosa          4.8         3.0          1.4         0.3 46 5.006 50\n47     setosa          5.1         3.8          1.6         0.2 47 5.006 50\n48     setosa          4.6         3.2          1.4         0.2 48 5.006 50\n49     setosa          5.3         3.7          1.5         0.2 49 5.006 50\n50     setosa          5.0         3.3          1.4         0.2 50 5.006 50\n51 versicolor          7.0         3.2          4.7         1.4 51 5.936 50\n52 versicolor          6.4         3.2          4.5         1.5 52 5.936 50\n53 versicolor          6.9         3.1          4.9         1.5 53 5.936 50\n54 versicolor          5.5         2.3          4.0         1.3 54 5.936 50\n55 versicolor          6.5         2.8          4.6         1.5 55 5.936 50\n   minid maxid\n45     1    50\n46     1    50\n47     1    50\n48     1    50\n49     1    50\n50     1    50\n51    51   100\n52    51   100\n53    51   100\n54    51   100\n55    51   100\n\ntail(dat)\n\n      Species Sepal.Length Sepal.Width Petal.Length Petal.Width  id    sl  n\n145 virginica          6.7         3.3          5.7         2.5 145 6.588 50\n146 virginica          6.7         3.0          5.2         2.3 146 6.588 50\n147 virginica          6.3         2.5          5.0         1.9 147 6.588 50\n148 virginica          6.5         3.0          5.2         2.0 148 6.588 50\n149 virginica          6.2         3.4          5.4         2.3 149 6.588 50\n150 virginica          5.9         3.0          5.1         1.8 150 6.588 50\n    minid maxid\n145   101   150\n146   101   150\n147   101   150\n148   101   150\n149   101   150\n150   101   150\n\n\nNow that we have our dataframe with all of the necessary information, we can plot.\nNote that there are two calls to geom_segment(). For the first, we are plotting the species means, so we use the smmeans dataset. For the second, we are plotting each pointʻs deviation from the species means so we use the full dataset. The rest is telling the function where the start and end points of each segment are:\n\nr <- p + geom_point( size=2) + \n  geom_segment( data=spmeans, aes(x=minid, y = sl, xend=maxid, yend=sl, group=Species )) + \n  geom_segment( data=dat, aes( x = id, y = Sepal.Length, xend = id, yend = sl, color=Species), lty = 3) \n\nr  \n\n\n\n\nBack to our question - is the error sum of squares minimized by accouting for separate species or considering all irises as one group? Another way to state ANOVA is - is at least one of these groups different than the others?\n\nrequire(cowplot)\n\nLoading required package: cowplot\n\nplot_grid(\n  q, \n  r + theme(legend.position=\"none\"), \n  labels=\"AUTO\")\n\n\n\n\nIf we want to know whether species are different in sepal length, then we need to have lm fit the model by species. We do this like so:\n\nlm.fit <- with(iris, lm(Sepal.Length ~ Species))\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sepal.Length ~ Species)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6880 -0.3285 -0.0060  0.3120  1.3120 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         5.0060     0.0728  68.762  < 2e-16 ***\nSpeciesversicolor   0.9300     0.1030   9.033 8.77e-16 ***\nSpeciesvirginica    1.5820     0.1030  15.366  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5148 on 147 degrees of freedom\nMultiple R-squared:  0.6187,    Adjusted R-squared:  0.6135 \nF-statistic: 119.3 on 2 and 147 DF,  p-value: < 2.2e-16\n\n\nInterpretation: One-way ANOVA is like fitting a regression of the individual points against the grand mean of the points vs. separate regressions for each group. The summary shows that the the intercept (the mean of setosa) is about 5 (significantly different than zero), whereas the other species are contrasts against setosa, the first species. Versicolor is 0.93 higher than setosa, and virginica is 1.58 higher than setosa. Both of these contrasts are signficant. So they are actually all significantly different than each other\nNotice that now have more parameters estimated. You can specify which parameter values and contrasts you want displayed. Often we just want an ANOVA table, which tests the hypothesis that at least one group is different than the others:\n\nanova(lm.fit)\n\nAnalysis of Variance Table\n\nResponse: Sepal.Length\n           Df Sum Sq Mean Sq F value    Pr(>F)    \nSpecies     2 63.212  31.606  119.26 < 2.2e-16 ***\nResiduals 147 38.956   0.265                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe can see that species are significantly different in sepal length. Can you make a plot that shows this and add the statistics to it?"
  },
  {
    "objectID": "posts/2023-03-07-univariate/index.html#ancova",
    "href": "posts/2023-03-07-univariate/index.html#ancova",
    "title": "A small review of univariate parametric statistics",
    "section": "ANCOVA",
    "text": "ANCOVA\nThere are many forms of regession and ANOVA. For example, if you want to see if the relationship between Sepal.Length and Sepal.Width differs by species, you woul do an ANCOVA (analysis of covariance):\n\nlm.fit <- with(iris, lm(Sepal.Width ~ Sepal.Length + Species))\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sepal.Width ~ Sepal.Length + Species)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.95096 -0.16522  0.00171  0.18416  0.72918 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        1.67650    0.23536   7.123 4.46e-11 ***\nSepal.Length       0.34988    0.04630   7.557 4.19e-12 ***\nSpeciesversicolor -0.98339    0.07207 -13.644  < 2e-16 ***\nSpeciesvirginica  -1.00751    0.09331 -10.798  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.289 on 146 degrees of freedom\nMultiple R-squared:  0.5693,    Adjusted R-squared:  0.5604 \nF-statistic: 64.32 on 3 and 146 DF,  p-value: < 2.2e-16\n\n\nWhich would fit separate Y-intercepts for each species."
  }
]