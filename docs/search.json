[
  {
    "objectID": "posts/2023-03-09-joining-data/index.html",
    "href": "posts/2023-03-09-joining-data/index.html",
    "title": "Joining data with dplyr",
    "section": "",
    "text": "Read ahead\n\n\n\nBefore class, you can prepare by reading the following materials:\n\nhttps://r4ds.had.co.nz/relational-data\nhttps://rafalab.github.io/dsbook/joining-tables\n\n\n\n\n\n\nMaterial for this lecture was borrowed and adopted from\n\nhttps://www.stephaniehicks.com/jhustatcomputing2022/posts/2022-09-08-joining-data-in-r/\nhttps://rdpeng.github.io/Biostat776/lecture-joining-data-in-r-basics\nhttps://r4ds.had.co.nz/relational-data\nhttps://rafalab.github.io/dsbook/joining-tables"
  },
  {
    "objectID": "posts/2023-03-09-joining-data/index.html#new-packages",
    "href": "posts/2023-03-09-joining-data/index.html#new-packages",
    "title": "Joining data with dplyr",
    "section": "New Packages",
    "text": "New Packages\nYou will have to install if you donʻt already have them:\n\ninstall.packages(\"gapminder\")  # a dataset package"
  },
  {
    "objectID": "posts/2023-03-09-joining-data/index.html#relational-data",
    "href": "posts/2023-03-09-joining-data/index.html#relational-data",
    "title": "Joining data with dplyr",
    "section": "Relational data",
    "text": "Relational data\nData analyses rarely involve only a single table of data.\nTypically you have many tables of data, and you must combine the datasets to answer the questions that you are interested in. Some examples include morphology and ecology data on the same species, or sequence data and metadata.\nCollectively, multiple tables of data are called relational data because it is the relations, not just the individual datasets, that are important.\nRelations are always defined between a pair of tables. All other relations are built up from this simple idea: the relations of three or more tables are always a property of the relations between each pair.\nSometimes both elements of a pair can be in the same table! This is needed if, for example, you have a table of people, and each person has a reference to their parents, or if you have nodes in a phylogeny and each is linked to an ancestral node.\nRelational data are combined with merges or joins."
  },
  {
    "objectID": "posts/2023-03-09-joining-data/index.html#example-with-merge",
    "href": "posts/2023-03-09-joining-data/index.html#example-with-merge",
    "title": "Joining data with dplyr",
    "section": "Example with merge()",
    "text": "Example with merge()\nLetʻs use the geospiza data from the geiger package to practice merging with the base R merge() function.\n\nrequire(geiger)\ndata(geospiza)   # load the dataset into the workspace\nls()               # list the objects in the workspace\n\n[1] \"geospiza\"\n\ngeospiza\n\n$geospiza.tree\n\nPhylogenetic tree with 14 tips and 13 internal nodes.\n\nTip labels:\n  fuliginosa, fortis, magnirostris, conirostris, scandens, difficilis, ...\n\nRooted; includes branch lengths.\n\n$geospiza.data\n                wingL  tarsusL  culmenL    beakD   gonysW\nmagnirostris 4.404200 3.038950 2.724667 2.823767 2.675983\nconirostris  4.349867 2.984200 2.654400 2.513800 2.360167\ndifficilis   4.224067 2.898917 2.277183 2.011100 1.929983\nscandens     4.261222 2.929033 2.621789 2.144700 2.036944\nfortis       4.244008 2.894717 2.407025 2.362658 2.221867\nfuliginosa   4.132957 2.806514 2.094971 1.941157 1.845379\npallida      4.265425 3.089450 2.430250 2.016350 1.949125\nfusca        3.975393 2.936536 2.051843 1.191264 1.401186\nparvulus     4.131600 2.973060 1.974420 1.873540 1.813340\npauper       4.232500 3.035900 2.187000 2.073400 1.962100\nPinaroloxias 4.188600 2.980200 2.311100 1.547500 1.630100\nPlatyspiza   4.419686 3.270543 2.331471 2.347471 2.282443\npsittacula   4.235020 3.049120 2.259640 2.230040 2.073940\n\n$phy\n\nPhylogenetic tree with 14 tips and 13 internal nodes.\n\nTip labels:\n  fuliginosa, fortis, magnirostris, conirostris, scandens, difficilis, ...\n\nRooted; includes branch lengths.\n\n$dat\n                wingL  tarsusL  culmenL    beakD   gonysW\nmagnirostris 4.404200 3.038950 2.724667 2.823767 2.675983\nconirostris  4.349867 2.984200 2.654400 2.513800 2.360167\ndifficilis   4.224067 2.898917 2.277183 2.011100 1.929983\nscandens     4.261222 2.929033 2.621789 2.144700 2.036944\nfortis       4.244008 2.894717 2.407025 2.362658 2.221867\nfuliginosa   4.132957 2.806514 2.094971 1.941157 1.845379\npallida      4.265425 3.089450 2.430250 2.016350 1.949125\nfusca        3.975393 2.936536 2.051843 1.191264 1.401186\nparvulus     4.131600 2.973060 1.974420 1.873540 1.813340\npauper       4.232500 3.035900 2.187000 2.073400 1.962100\nPinaroloxias 4.188600 2.980200 2.311100 1.547500 1.630100\nPlatyspiza   4.419686 3.270543 2.331471 2.347471 2.282443\npsittacula   4.235020 3.049120 2.259640 2.230040 2.073940\n\ngeo &lt;- geospiza$dat  # save the morphometric data as geo\n\nThis is a 5 column dataframe. Letʻs take just the tarsusL data to build our example dataset:\n\ntarsusL &lt;- geo[,\"tarsusL\"]  # geo is a matrix, select tarsusL column\ngeot &lt;- data.frame(tarsusL, \"ecology\" = LETTERS[1:length(tarsusL)])\n\nOften we will be merging data that donʻt perfectly match. Some parts of the data will be missing, for example we may only have ecology data for the first five species. The question is what do you want the merge behavior to be?\nThe default is to drop all observations that are not in BOTH datasets. Here we merge the original geo with only the first five rows of geot:\n\n                    # only maches to both datasets are included\nmerge(x=geo[,\"tarsusL\"], y=geot[1:5, ], by= \"row.names\")    \n\n     Row.names        x  tarsusL ecology\n1  conirostris 2.984200 2.984200       B\n2   difficilis 2.898917 2.898917       C\n3       fortis 2.894717 2.894717       E\n4 magnirostris 3.038950 3.038950       A\n5     scandens 2.929033 2.929033       D\n\n\nIf we want to keep everything, use the all=T flag:\n\n                    # all species in both datasets are included\nmerge(x=geo[,\"tarsusL\"], y=geot[1:5,], by= \"row.names\", all=T)    \n\n      Row.names        x  tarsusL ecology\n1   conirostris 2.984200 2.984200       B\n2    difficilis 2.898917 2.898917       C\n3        fortis 2.894717 2.894717       E\n4    fuliginosa 2.806514       NA    &lt;NA&gt;\n5         fusca 2.936536       NA    &lt;NA&gt;\n6  magnirostris 3.038950 3.038950       A\n7       pallida 3.089450       NA    &lt;NA&gt;\n8      parvulus 2.973060       NA    &lt;NA&gt;\n9        pauper 3.035900       NA    &lt;NA&gt;\n10 Pinaroloxias 2.980200       NA    &lt;NA&gt;\n11   Platyspiza 3.270543       NA    &lt;NA&gt;\n12   psittacula 3.049120       NA    &lt;NA&gt;\n13     scandens 2.929033 2.929033       D\n\n\nThere is also all.x which keeps all values of the first data table but drops non-matching rows of the second table, and all.y which keeps all of the second.\nThe results of merge are sorted by default on the sort key. To turn it off:\n\ngeo &lt;- geo[rev(rownames(geo)), ]   # reverse the species order of geo\n                     # merge on geo first, then geot\nmerge(x=geo[,\"tarsusL\"], y=geot[1:5, ], by= \"row.names\", sort=F)   \n\n     Row.names        x  tarsusL ecology\n1       fortis 2.894717 2.894717       E\n2     scandens 2.929033 2.929033       D\n3   difficilis 2.898917 2.898917       C\n4  conirostris 2.984200 2.984200       B\n5 magnirostris 3.038950 3.038950       A\n\n                     # geot first, then geo\nmerge(x=geot[1:5,], y=geo[,\"tarsusL\"], by= \"row.names\", sort=F)   \n\n     Row.names  tarsusL ecology        y\n1 magnirostris 3.038950       A 3.038950\n2  conirostris 2.984200       B 2.984200\n3   difficilis 2.898917       C 2.898917\n4     scandens 2.929033       D 2.929033\n5       fortis 2.894717       E 2.894717\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nIn a merge, the non-key columns are copied over into the new table.\n\n\n\nCheck out the help page for ?merge for more info."
  },
  {
    "objectID": "posts/2023-03-09-joining-data/index.html#example-of-keys",
    "href": "posts/2023-03-09-joining-data/index.html#example-of-keys",
    "title": "Joining data with dplyr",
    "section": "Example of keys",
    "text": "Example of keys\nImagine you are conduct a study and collecting data on subjects and a health outcome.\nOften, subjects will have multiple observations (a longitudinal study). Similarly, we may record other information, such as the type of housing.\n\nThe first table\nThis code creates a simple table with some made up data about some hypothetical subjects’ outcomes.\n\nlibrary(tidyverse)\n\noutcomes &lt;- tibble(\n        id = rep(c(\"a\", \"b\", \"c\"), each = 3),\n        visit = rep(0:2, 3),\n        outcome = rnorm(3 * 3, 3)\n)\n\nprint(outcomes)\n\n# A tibble: 9 × 3\n  id    visit outcome\n  &lt;chr&gt; &lt;int&gt;   &lt;dbl&gt;\n1 a         0    4.41\n2 a         1    1.55\n3 a         2    3.73\n4 b         0    4.40\n5 b         1    3.13\n6 b         2    3.77\n7 c         0    2.43\n8 c         1    2.09\n9 c         2    3.17\n\n\nNote that subjects are labeled by a unique identifer in the id column.\n\n\nA second table\nHere is some code to create a second table containing data about the hypothetical subjects’ housing type.\n\nsubjects &lt;- tibble(\n        id = c(\"a\", \"b\", \"c\"),\n        house = c(\"detached\", \"rowhouse\", \"rowhouse\")\n)\n\nprint(subjects)\n\n# A tibble: 3 × 2\n  id    house   \n  &lt;chr&gt; &lt;chr&gt;   \n1 a     detached\n2 b     rowhouse\n3 c     rowhouse\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is the primary key and foreign key?\n\nThe outcomes$id is a primary key because it uniquely identifies each subject in the outcomes table.\nThe subjects$id is a foreign key because it appears in the subjects table where it matches each subject to a unique id."
  },
  {
    "objectID": "posts/2023-01-31-what-is-the-question/index.html",
    "href": "posts/2023-01-31-what-is-the-question/index.html",
    "title": "What is the question?",
    "section": "",
    "text": "Take a look at the data in this recent paper: Winchell et. al, (2023) Genome-wide parallelism underlies contemporary adaptation in urban lizards. PNAS 120 (3) e2216789120 Check SLACK channel for pdf\n\n\n\n\n\nIn the age of Artificial Intelligence (AI), we have many data-intensive tools available. But can we just throw more data at a problem to get better outcomes? Please watch this thought provoking short talk by Sebastian Wernicke “How to use data to make a hit TV show”… What goes wrong when we look for decisions in the wrong places r emojifont::emoji('palm_tree')"
  },
  {
    "objectID": "posts/2023-01-31-what-is-the-question/index.html#read-ahead",
    "href": "posts/2023-01-31-what-is-the-question/index.html#read-ahead",
    "title": "What is the question?",
    "section": "",
    "text": "Take a look at the data in this recent paper: Winchell et. al, (2023) Genome-wide parallelism underlies contemporary adaptation in urban lizards. PNAS 120 (3) e2216789120 Check SLACK channel for pdf"
  },
  {
    "objectID": "posts/2023-01-31-what-is-the-question/index.html#watch-ahead-a-motivating-example-for-the-whole-endeavor-of-data-analysis",
    "href": "posts/2023-01-31-what-is-the-question/index.html#watch-ahead-a-motivating-example-for-the-whole-endeavor-of-data-analysis",
    "title": "What is the question?",
    "section": "",
    "text": "In the age of Artificial Intelligence (AI), we have many data-intensive tools available. But can we just throw more data at a problem to get better outcomes? Please watch this thought provoking short talk by Sebastian Wernicke “How to use data to make a hit TV show”… What goes wrong when we look for decisions in the wrong places r emojifont::emoji('palm_tree')"
  },
  {
    "objectID": "posts/2023-04-13-iqtree-setup/index.html",
    "href": "posts/2023-04-13-iqtree-setup/index.html",
    "title": "Setting Up for IQTREE2",
    "section": "",
    "text": "References for this Material:\n\nThe Gene Tree species tree tutorial by Minh Bui http://www.iqtree.org/workshop/molevol2022\nMany pages on Stack Overflow"
  },
  {
    "objectID": "posts/2023-04-13-iqtree-setup/index.html#multiple-installations-of-iqtree2",
    "href": "posts/2023-04-13-iqtree-setup/index.html#multiple-installations-of-iqtree2",
    "title": "Setting Up for IQTREE2",
    "section": "Multiple installations of iqtree2",
    "text": "Multiple installations of iqtree2\nIf you want, you can install multiple versions of iqtree. Just be sure to give the binary different names. For example if you install iqtree v2.2.2.3, (latest pre-release version) you could name that binary iqtree2.2.2.3 or iqtree2beta etc. so as to have one unique name per software application."
  },
  {
    "objectID": "posts/2023-04-18-Tree-Formats/index.html",
    "href": "posts/2023-04-18-Tree-Formats/index.html",
    "title": "All about trees",
    "section": "",
    "text": "Acknowledgements\nReferences for this Material:\n\nThis was adapted from the chapter “All about trees” written by Brian OʻMeara from the manual “Getting started in R for Phylogenetics” by Marguerite A. Butler, Brian C. O’Meara, Jason Pienaar, Michael Alfaro, Graham Slater, and Todd Oakley\n\n\n\nLearning objectives\n\n\n\n\n\n\nLearning Objectives\n\n\n\nAt the end of this lesson you will:\n\nUnderstand information content of phylogenetically structured data\nUnderstand particular R tree formats in ape, phylobase, and ouch\nBe able to hand-make trees\nBe able to import trees from nexus, newick, and other major formats in use today\nBe able to convert trees from one format to another\nBe able to perform basic tree manipulations\n\n\n\n\n\n\n\n\n\nR packages you will need\n\n\n\n\nape\nouch\n\n\n\n\n\nTree vocabulary\nIn nature, a tree is a large perennial woody plant. It has roots, a main trunk, branches, and leaves. In graph theory, a tree is a network where there is only one path between any two nodes (in other words, a network with no loops or cycles).\nIn phylogenetics, we use ideas and terminology from both graph theory and nature:\n\nTerminal taxa are also known as leaves, terminals, OTUs (“Operational Taxonomic Units”), tips, or simply taxa.\nBranches are also called edges or internodes.\nInternal nodes (places where two or more branches connect) are also known as vertices and sometimes simply nodes (technically, leaves are also nodes).\nA rooted tree has one node designated as the root, and all other nodes are descended from this root. Traditionally, the root node has at least two descendants; it may also have a subtending branch (an ancestral branch).\nAn unrooted tree has no root designated, making the oldest ancestor ambiguous.\n\nA tree where every internal node has two and only two descendants is known as a binary or bifurcating tree.\nA tree where at least one internal node has more than two descendants is said to be multifurcating; such a node is a polytomy.\nTrees in phylogenetics generally represent either species trees (a history of the splitting of interbreeding populations) or gene trees (a history of the coalescence of gene copies). In both cases, it is generally believed that the true process is bifurcating, so that each split results in two descendants. Thus, polytomies on trees are generally taken as representing uncertainty in the relationships. However in the case of adaptive radiations, and other rapidly speciating processes, the true process may be approaching multifurcation.\nBranches may have lengths; these lengths may correspond to time, amount of change (e.g., mutations) in some set of characters, number of speciation events, or some other measure.\nA tree where all branch lengths from root to tips are equal is known as an ultrametric tree.\nA tree without branch lengths is known as a topology.\nA monophyletic clade is an ancestor and all its descendants.\nAny edge or branch corresponds to a bipartition: a division of the tree into two parts connected by that edge (if a root were inserted on the edge, then each of those parts would be a clade).\n\n\n\nNewick format\nA very basic tree format is Newick (.tree), named after the seafood restaurant in New Hampshire where it was formalized in 1986 by James Archie, William H. E. Day, Joseph Felsenstein, Wayne Maddison, Christopher Meacham, F. James Rohlf, and David Swofford - the second meeting was at the restaurant; it is also called New Hampshire format for that reason). It is widely used in graph theory as well. It is simply a string. Each nesting on the tree corresponds to a parenthetical statement. For example, for this tree (drawn brute-force):\n\n\n\nPhoto from the Newick restaurant home page\n\n\n\nrequire(ape)\nrequire(ouch)\nrequire(phylobase)\n# simpletree &lt;- rcoal(length(LETTERS[1:7]), rooted = TRUE, tip.label = LETTERS[1:7], br = \"coalescent\")\n\nsimpletree&lt;-structure(list(\n    edge = matrix(\n      c(8, 9, 10, 10, 9, 11, 11, 8, 12, 12, 13, 13, \n        9, 10, 1, 2, 11, 3, 4, 12, 5, 13, 6, 7),\n      nrow=12L), \n    tip.label = c(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"), \n    Nnode = 6L\n  ), \n    class = \"phylo\")\nplot(simpletree,no.margin=TRUE)\n\n\n\n\nFigure 1: A simple tree\n\n\n\n\nTaxa G and F form a clade, as do G, F, and E, as do A and B, and so forth. Thus, to create a Newick string, just go down the tree, nesting as you go:\n(G,F)\n((G,F),E)\nother side:\n(A,B)\n(C,D)\n((A,B),(C,D))\nall together:\n(((G,F),E),((A,B),(C,D)))\nAnd that’s it (it will be clearer in the lecture) If a tree has branch lengths, these are entered following the descendant clade. For example, if the branch leading to G has length 1.0, we would write G:1.0 rather than just G. If the tree is ultrametric, and the branch below the common ancestor of G and F is of length 1.1, and the branch below that of length 3.5, we could write (note: a semicolon ; ends the newick string):\n((G:1.0,F:1.0):1.1,E:2.1):3.5;\nOne aspect of the Newick (and most tree) representation is that there are many ways of representing the same tree. At every node, one can rotate the descendant branches (switching the left and right positions) and get the same tree (for example, imagine switching the G and E labels). Thus, the Newick strings\n((G,F),E)\nand\n((F,G),E)\ndescribe the same trees, though it might not be easy to tell at first glance. This is generally an issue for any tree representation. While Newick strings are compact and easy to understand, they also don’t lend themselves to easy tree traversal (moving up or down the tree). In most analysis software that performs computations along trees, some other representation is used.\n\n\nNEXUS\nThe NEXUS format (*.nex or *.nxs) is widely used in bioinformatics. Many phylogenetics programs use nexus format including \\(PAUP^*\\), MrBayes, Mesquite, MacClade, etc., and many more can read nexus.\nOne of the key features is that it is very extensible with information in blocks. We will only be concerned with blocks of DNA sequence data, but any type of information can be added. This flexibilityy, however, can be a curse when you encounter new extensions to the format that your code did not anticipate. However, for the most part we will be using packages that read in nexus so you can reasonable hope that someone has dealt with the headache for you.\nNexus files are often used as inputs to phylogenetic programs. In that case, they typically only have a TAXA block and a DATA block. In our IQTREE2 example we also had a partition file in nexus format. But they can also return the output of the phylogenetic reconstruction, in that case they will have a TREES block as well.\n\nNexus syntax:\nA NEXUS file begins with #NEXUS on the first line.\nBlocks of information follow, tyically on multiple lines, enclosed by a BEGIN blockname; and END; (Key words are case-insensitive).\nComments are enclosed by square brackets [comments]\nTypical blocks:\n\nTAXA: The taxa block contains the names of the taxa.\nDATA: The data block contains a data matrix (we will use DNA sequences).\nTREES: The trees block contains one or more phylogenetic trees in newick format.\n\nFor example From Wikipedia:\n#NEXUS\nBegin TAXA;\n  Dimensions ntax=4;\n  TaxLabels SpaceDog SpaceCat SpaceOrc SpaceElf;\nEnd;\n\nBegin data;\n  Dimensions nchar=15;\n  Format datatype=dna missing=? gap=- matchchar=.;\n  Matrix\n    [ When a position is a \"matchchar\", it means that it is the same as the first entry at the same position. ]\n    SpaceDog   atgctagctagctcg\n    SpaceCat   ......??...-.a.\n    SpaceOrc   ...t.......-.g. [ same as atgttagctag-tgg ]\n    SpaceElf   ...t.......-.a.           \n  ;\nEnd;\n\nBEGIN TREES;\n  Tree tree1 = (((SpaceDog,SpaceCat),SpaceOrc,SpaceElf));\nEND; \n\n\n\nphylo (ape 5.0 or above)\nThe ape package (Paradis and Schliep 2019) uses a different representation of trees. It uses R structures, lists, matrices, and vectors to store a tree. Each node in the tree receives a number. For example, here is the tree from before in ape format.\nHere is simpletree with the node numbers printed. It is printed with the following commands:\n\nplot(simpletree,no.margin=TRUE)\nnodelabels()\ntiplabels()\n\n\n\n\nFor a tree with N tips, the tips have numbers 1…N and the nodes have numbers greater than N (this is in contrast to how this was done in early (&lt;1.9) versions of ape). These numbers are used to store information about the tree’s structure.\nTo do this, a matrix is created, with height corresponding to the number of internal and terminal nodes and width 2. The first column of the matrix has the node at the beginning of the branch, the second has the node at the end of the branch. For example, for our simple tree, this matrix is\n\nsimpletree$edge\n\n      [,1] [,2]\n [1,]    8    9\n [2,]    9   10\n [3,]   10    1\n [4,]   10    2\n [5,]    9   11\n [6,]   11    3\n [7,]   11    4\n [8,]    8   12\n [9,]   12    5\n[10,]   12   13\n[11,]   13    6\n[12,]   13    7\n\n\nThis alone is enough for a basic topology. However, it might be nice to know what the taxa actually are, rather than just numbers. To do this, a character vector with as many entries as the number of tips is used. In the example tree, this is\n\nsimpletree$tip.label\n\n[1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\"\n\n\nIt’s possible that internal nodes have labels, too (for example, the most recent common ancestor of a set of birds might be labeled Aves). If so, an optional node.label is used. If branch lengths are known, they are included as the numeric vector edge.length.\n\nsimpletree$edge.length\n\n [1] 1.5 1.0 0.5 0.5 1.0 0.5 0.5 2.0 1.0 0.5 0.5 0.5\n\n\nFinally, there are a few other elements (Nnode, the number of internal nodes; class=phylo) to set the class. To dump the the internal representation of phylo tree to screen, you can use unclass() which strips the class attribute (the S4 analog is attributes):\n\nunclass(simpletree)\n\n$edge\n      [,1] [,2]\n [1,]    8    9\n [2,]    9   10\n [3,]   10    1\n [4,]   10    2\n [5,]    9   11\n [6,]   11    3\n [7,]   11    4\n [8,]    8   12\n [9,]   12    5\n[10,]   12   13\n[11,]   13    6\n[12,]   13    7\n\n$tip.label\n[1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\"\n\n$Nnode\n[1] 6\n\n$edge.length\n [1] 1.5 1.0 0.5 0.5 1.0 0.5 0.5 2.0 1.0 0.5 0.5 0.5\n\n\nphylo trees are S3 objects. We’ll be learning more about them later, but an important thing to know is that you directly access any element of them by using the $ operator (as was done above). Optional elements, or even elements of your own devising, can be added to them, too, using the same operator.\n\n\nouchtree (ouch version 2.0 or above)\nOUCH King and Butler (2022) uses a different tree structure than does ape. First, OUCH’s is an S4 class, rather than S3. There are several differences between them, which you’ll learn later. There are two main distinctions that will be important now. It helps to have a metaphor: think of a car. The S3 representation of a car is all the parts, neatly disassembled and laid out. The S4 representation of a car is a closed box. With S3, you can look at and manipulate any part of the car directly and manipulate it (using the $ operator). You could check the amount of gas in the tank by directly accessing the gas. With S4, you should use a method, if one exists, to access and manipulate elements. For example, you could check the gas in the tank using the fuel gauge, if the fuel gauge method exists and works properly. S3 objects can be built up piecemeal, and there aren’t built-in checks to make sure that everything is correct: if you forget to add a wheel element to the S3 char, you won’t know there’s a problem until some function tries to access it and fails. S4 objects are instantiated once, when you pass them all the initialization info they need (they often have defaults, and often have internal consistency checks). OUCH uses the ouchtree class as a basic tree class, then derives other classes from this for storing information on analyses. The ouchtree class is:\nsetClass(\n'ouchtree',\nrepresentation=representation(\nnnodes = 'integer',\nnodes = 'character',\nancestors = 'character',\nnodelabels = 'character',\ntimes = 'numeric',\nroot = 'integer',\nnterm = 'integer',\nterm = 'integer',\nanc.numbers = 'integer',\nlineages = 'list',\nepochs = 'list',\nbranch.times = 'matrix',\ndepth = 'numeric'\n)\n)\nAt first glance, it looks like creating a new ouchtree object will be a lot of work: there are 13 different elements, some of them vectors, built in the class. However, with S4 objects, the beauty of constructors comes into play. The constructor function for a new ouchtree is just the function\nouchtree(nodes, ancestors, times, labels = as.character(nodes))\nThe ouchtree function only has four arguments, one of them optional. Using the function and these elements, all the other elements of the class are initialized.\n\nThe first element is nodes, a character vector of node ids (including internal nodes). Unlike ape, the leaves do not need to have smaller ids than internal nodes.\nThe second argument is ancestors, a character vector of node ids of the ancestors for the nodes in the nodes vector. The nodes and ancestors vectors almost correspond to the second and first columns of the ape edge matrix, respectively, with the exception that ouchtree includes the root node with an ancestor of NA.\nThe third element, times, represents the height of each node from the root. Remember that ape’s edge.length vector has the length of the branch subtending each node; instead, ouchtree has the sum of the lengths of all branches connecting a given node to the root. Again, the root node is included in ouchtree (with height 0) but not in ape.\nThe fourth argument, labels, is a vector of labels for both tips and internal nodes. If internal nodes do not have names, they get a label of &lt;NA&gt;.\n\nFor example, our example tree, when converted to ouchtree format, is\n\nattributes(simpletreeouch)$nodes\n\n [1] \"1\"  \"2\"  \"3\"  \"4\"  \"5\"  \"6\"  \"7\"  \"8\"  \"9\"  \"10\" \"11\" \"12\" \"13\"\n\nattributes(simpletreeouch)$ancestors\n\n [1] NA  \"3\" \"1\" \"6\" \"6\" \"1\" \"2\" \"2\" \"3\" \"4\" \"4\" \"5\" \"5\"\n\nattributes(simpletreeouch)$times\n\n [1] 0.0000000 0.8333333 0.6666667 0.8333333 0.8333333 0.5000000 1.0000000\n [8] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000\n\nattributes(simpletreeouch)$nodelabels\n\n [1] \"\"  \"\"  \"\"  \"\"  \"\"  \"\"  \"G\" \"F\" \"E\" \"D\" \"C\" \"B\" \"A\"\n\n\nOne other element of ouchtree, created on initialization, is a matrix showing shared amount of time on a tree between two tips (which may be the same tip). This, multiplied by a rate parameter, becomes a variance-covariance matrix under a Brownian motion model, which we’ll be discussing in the course.\n\nattributes(simpletreeouch)$branch.times\n\n          [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7]\n[1,] 1.0000000 0.8333333 0.6666667 0.0000000 0.0000000 0.0000000 0.0000000\n[2,] 0.8333333 1.0000000 0.6666667 0.0000000 0.0000000 0.0000000 0.0000000\n[3,] 0.6666667 0.6666667 1.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n[4,] 0.0000000 0.0000000 0.0000000 1.0000000 0.8333333 0.5000000 0.5000000\n[5,] 0.0000000 0.0000000 0.0000000 0.8333333 1.0000000 0.5000000 0.5000000\n[6,] 0.0000000 0.0000000 0.0000000 0.5000000 0.5000000 1.0000000 0.8333333\n[7,] 0.0000000 0.0000000 0.0000000 0.5000000 0.5000000 0.8333333 1.0000000\n\n\nThe entire content of the simpletreeouch object can be dumped to screen using the following command (not executed here to save paper):\n\nattributes(simpletreeouch)\n\n\n\nA note about node numbering\n\nphylo (ape) node numbers and order\nIn ape the node numbering is implied by the order of rows in the $egde matrix. Letʻs make a simple example, a tree with three tips:\nRemember in the edge matrix, the first column is the ancestral node (parent), and the second column is the node number.\n\ntree &lt;- rtree(3)\ntree$edge\n\n     [,1] [,2]\n[1,]    4    5\n[2,]    5    1\n[3,]    5    2\n[4,]    4    3\n\nplot(tree)\nnodelabels()\ntiplabels(adj=c(2))\n\n\n\n\nThe nodes are listed in a rightward tree traversal order.\n\n\ntreedata (ggtree) node numbers and order\nIn the treedata object used by ggtree, treeio, and tidytree, the node numbers are the same (referring to the same nodes and ancestors), but the order that they are stored in is different. (Also note that ggtree plots trees ladderized by default).\n\nlibrary(treeio)\nlibrary(ggtree)\ntd &lt;- as.treedata(tree)\ntd %&gt;% as_tibble %&gt;% as.data.frame\n\n  parent node branch.length label\n1      5    1     0.8941764    t3\n2      5    2     0.8153808    t2\n3      4    3     0.7322952    t1\n4      4    4            NA  &lt;NA&gt;\n5      4    5     0.5673610  &lt;NA&gt;\n\nggtree(td) + \n     geom_tiplab(hjust=-1) +\n     geom_label(aes(label=node))\n\n\n\n\nCompare tree$edge vs treedata:\n\ntree$edge\n\n     [,1] [,2]\n[1,]    4    5\n[2,]    5    1\n[3,]    5    2\n[4,]    4    3\n\ntd %&gt;% as_tibble %&gt;% as.data.frame\n\n  parent node branch.length label\n1      5    1     0.8941764    t3\n2      5    2     0.8153808    t2\n3      4    3     0.7322952    t1\n4      4    4            NA  &lt;NA&gt;\n5      4    5     0.5673610  &lt;NA&gt;\n\n\n\n\n\n\n\n\nCombining phylo with data and other tree formats\n\n\n\n\nIt is important when combining phylo and treedata objects to always match by node (or when only tips are involved, by label).\nIf one simply pastes matrices together (e.g., cbind), the information will be connected to the wrong node!\n\nWeʻll cover this in more detail in the next section.\n\n\n\n\n\n\n\n\nReferences\n\nButler, Marguerite A., and Aaron A. King. 2004. “Phylogenetic Comparative Analysis: A Modeling Approach for Adaptive Evolution.” American Naturalist 164: 683–95. https://doi.org/10.1086/426002.\n\n\nKing, Aaron A., and Marguerite A. Butler. 2022. Ouch: Ornstein-Uhlenbeck Models for Phylogenetic Comparative Hypotheses. https://kingaa.github.io/ouch/.\n\n\nParadis, Emmanuel, and Klaus Schliep. 2019. “Ape 5.0: An Environment for Modern Phylogenetics and Evolutionary Analyses in R.” Bioinformatics 35: 526–28. https://doi.org/10.1093/bioinformatics/bty633."
  },
  {
    "objectID": "posts/2023-03-02-multivariate/index.html",
    "href": "posts/2023-03-02-multivariate/index.html",
    "title": "A small tour of multivariate analysis",
    "section": "",
    "text": "Learning objectives\n\n\n\nAt the end of this lesson you will:\n\nBe able to perform basic univariate statistics\nBe able to perform basic multivariate statistics\nBe able to relate questions to graphical representations of data"
  },
  {
    "objectID": "posts/2023-03-02-multivariate/index.html#nature-of-the-relationship-among-variables",
    "href": "posts/2023-03-02-multivariate/index.html#nature-of-the-relationship-among-variables",
    "title": "A small tour of multivariate analysis",
    "section": "Nature of the relationship among variables",
    "text": "Nature of the relationship among variables\nYou may also expect your data to follow a power law, in which case a log-transformation will make the data linear. For example, things that scale with body size tend to have the form:\n\\[\nY = aMass^b  \n\\] \\[\nlog(Y) = log(a) + b\\times log(Mass)\n\\]"
  },
  {
    "objectID": "posts/2023-03-02-multivariate/index.html#fitting-assumptions-of-parametric-statistics",
    "href": "posts/2023-03-02-multivariate/index.html#fitting-assumptions-of-parametric-statistics",
    "title": "A small tour of multivariate analysis",
    "section": "Fitting assumptions of parametric statistics",
    "text": "Fitting assumptions of parametric statistics\nIf you plan to do parametric statistics, for example many forms of regression, ANOVA, etc. one of the major assumptions is that the errors are normally distributed.\nThat is, the relationship follows the form:\n\\[\nY \\sim X + e\n\\]\nWhich is read as Y is proportional to X plus random error. Where e ~ N or the errors or deviations from this relationship follow a normal distribution. Note that this assumes that X is known without error.\n\nChecking for normality\nA convenient tool for checking the normality of continuous data is qqnorm() which plots the QQ quantiles of the data. If it is normally distributed, the points should fall on a straight line:\n\nqqnorm(iris$Sepal.Length)\nqqline(iris$Sepal.Length)\n\n\n\n\nOr the ggplot2 version:\n\nrequire(ggplot2)\n\nLoading required package: ggplot2\n\nrequire(magrittr)  # for piping using %&gt;%\n\nLoading required package: magrittr\n\nggplot(iris, aes(sample=Sepal.Length)) + \n  stat_qq() +\n  stat_qq_line()\n\n\n\n\nThis data looks pretty good, except for some deviations along the edges. Most data will not ever be perfectly normal, you will get a sense of what is acceptable with more experience.\nHowever, we do know that this data contains three species – what happens if you were to look at the data by species?\n\niris %&gt;% \n    ggplot( aes(sample=Sepal.Length)) + \n   stat_qq( aes(col=Species)) +\n   stat_qq_line( aes(col=Species) )\n\n\n\n\n\n\nSkew\nDeviations from normality are not the end of the world, and often a little is tolerated. What can be more problematic is strong skew. For that you will really want to transform the data:\n Source\nRight skewed data is data with a long tail to the right (the positive side). Left skeweed data has along tail to the left. Here are a few methods. There are more\n\n\n\nSkew\nTransform\nCode\n\n\n\n\nstrong right\ncube root\nz = x^(1/3)\n\n\n\nsquare root\nz = x6(1/2)\n\n\n\nlog\nz = log(x)\n\n\n\n\nz = log10(x)\n\n\n\n\nz = log2(x)\n\n\nstrong left\nsquare\nz = x^2"
  },
  {
    "objectID": "posts/2023-03-02-multivariate/index.html#separating-size-and-shape",
    "href": "posts/2023-03-02-multivariate/index.html#separating-size-and-shape",
    "title": "A small tour of multivariate analysis",
    "section": "Separating Size and Shape",
    "text": "Separating Size and Shape\nWill you want to do an analysis of the data along with a size-corrected dataset? If shape variation is interesting for your data (i.e., do they differ in shape when we control for differences in size, or are they relatively larger or smaller?), then you may want to find some sort of size-adjustment. Popular methods include\n\nRegressing against a size variable, and using residuals\n\nPCA analysis excluding PC1 (PC1 is considered size),\n\nShear or Procrustes methods, and\n\nRatios with size.\nThere is a huge wealth of literature on size and how to analyze shape."
  },
  {
    "objectID": "posts/2023-03-02-multivariate/index.html#pca-loadings",
    "href": "posts/2023-03-02-multivariate/index.html#pca-loadings",
    "title": "A small tour of multivariate analysis",
    "section": "PCA Loadings",
    "text": "PCA Loadings\nSome things to look for in PC analysis: The loadings of the variables on the PC axes show how much each variable is correlated with that PC axis. The magnitude of the loading indicates how strong the correlation is, and the sign indicates the direction. The sign of the loading is only informative if variables load with different signs on the same PC axis. For example if variable A and B load positively with PC 2, and variable C loads negatively, this is often interpreted as varying along PC2 in an increasing direction indicating larger A and B but smaller C. In a morphological analysis, the first PC axis often loads positively and nearly equally on all variables, and is therefore considered to indicate size. PC1 also typically explains a large fraction of the variation.\nThe amount of variation each PC axis explains is called the proportion of variance explained. It is usually expressed as a percent or a fraction. It is not uncommon in morphological analysis for PC1 to explain 90% of the variation in the data.\nIt is important to note, however, that the amount of variation does not necessarily indicate it’s importance. Many ecological associations or functionally significant variation is reflected in shape variation, which as we said may be only 10% of the variation. However, this might be very functionally relevant. Size may vary a lot, but it might be whether or not you have very long legs relative to your size that tells us if you are a good runner. Long legs (in an absolute sense) may not make you a great runner if you are actually huge in size, so that relative to your body length, your legs are actually relatively short. So one thing to keep in mind is that you often will use only 3 axes, even though you have 10 or more variables. If you have managed to capture 90 or 95% of the variation with the first three variables (sometimes even more), you’re probably in great shape. It’s a tradeoff between keeping the analysis and interpretation manageable, and keeping all the variation in the data. Usually the minor axes have less than 1% of the variation, and are usually not interesting even if you were to keep them. Anyway, to conclude this paragraph, you may want to do a PC analysis on the data with size included, and then do a second analysis on the size-adjusted data (shape). Another strategy is to do a PC analysis on the data with size, and then leave out PC1 in downstream analyses of “shape”."
  },
  {
    "objectID": "posts/2023-04-13-iqtree-phylogenetics/index.html",
    "href": "posts/2023-04-13-iqtree-phylogenetics/index.html",
    "title": "Phylogenetic Inference with IQTREE2",
    "section": "",
    "text": "References for this Material:\n\nThe Gene Tree species tree tutorial by Bui Minh http://www.iqtree.org/workshop/molevol2022. Please see for more information.\nAnd Rob Lanfearʻs excellent blog post about concordance factors http://www.robertlanfear.com/blog/files/concordance_factors.html with R code to help with analysis.\nSee Methods Paper: Bui Quang Minh, Matthew W Hahn, Robert Lanfear New Methods to Calculate Concordance Factors for Phylogenomic Datasets Molecular Biology and Evolution, Volume 37, Issue 9, September 2020, Pages 2727–2733, https://doi.org/10.1093/molbev/msaa106"
  },
  {
    "objectID": "posts/2023-04-13-iqtree-phylogenetics/index.html#the-theoretical-problem",
    "href": "posts/2023-04-13-iqtree-phylogenetics/index.html#the-theoretical-problem",
    "title": "Phylogenetic Inference with IQTREE2",
    "section": "The theoretical problem",
    "text": "The theoretical problem\nThe position of turtles among vertebrates has been enigmatic for decades (Figure 2). At various times it has been allied more closely with mammals or lizards. It is one of those truly hard biological problems. Even with the development of genomics, the position of turtles with respect to crocodiles and birds remains a hot topic of research as of late.\n\n\n\nFigure 2: The changing position of turtles. Image by Jeremy Brown, from IQTREE tutorial\n\n\nSource: Jeremy Brown\nImportantly, different groups have gotten different answers even with 100% bootstrap support at the nodes. This highlights a problem that can occur when we have gene trees that conflict with the species tree. This can occur through incomplete lineage sorting (you will see ILS a lot in the literature; Figure 3), or via hybridization, or other mechanisms.\n\n\n\nFigure 3: Incomplete Lineage Sorting. The species tree (thick gray history) in relation to a gene tree (thin colored history). New lineages arising from murations in the gene are represented by changes in color. Differences in the history of the gene versus the species can occur when the two alleles in the population prior to the split of Dmel are maintained through to the split of Dere and Dyak, leading to maintenance of an ancestral polymorphism and incomplete lineage sorting. The species tree is now incongruent with the gene tree (tree 2). The greater the diversity in the ancestral population and the shorter the time between speciation events, the more likely we are to see gene-tree species tree conflict.\n\n\nSource: (Pollard et al. 2006)"
  },
  {
    "objectID": "posts/2023-04-13-iqtree-phylogenetics/index.html#the-data",
    "href": "posts/2023-04-13-iqtree-phylogenetics/index.html#the-data",
    "title": "Phylogenetic Inference with IQTREE2",
    "section": "The data",
    "text": "The data\nThe original genomic data comes from (Chiari et al. 2012). We are analyzing a subset of 29 genes from the original 248 genes, as well as a subset of taxa (to allow faster runs), presented in Bui Minhʻs tutorial. We will use IQTREE2 to generate the phylogenetic data, and then use R to analyze some of the output. We are just going to do the basics, for more indepth coverage see Minh Buiʻs tutorial."
  },
  {
    "objectID": "posts/2023-04-13-iqtree-phylogenetics/index.html#species-trees",
    "href": "posts/2023-04-13-iqtree-phylogenetics/index.html#species-trees",
    "title": "Phylogenetic Inference with IQTREE2",
    "section": "Species Trees",
    "text": "Species Trees\nA shell script for thes IQTREE commands are in iqtreerun.sh\n\nSimplest phylogeny\nIQTREE2 is the latest version of the IQTREE software. It runs on the command line, and each run generates a number of output files. To keep organized, it is a good idea to save your iqtree commands as a shell script, and to make input and output directories.\nMac users open your Terminal, Windows users open your Git-Bash, navigate to your TurtleTutorial folder.\nThe simple command below will find the best-fit model using ModelFinder, reconstruct the ML tree, and calculate branch supports using the ultrafast boostrap.\n\n\nTerminal\n\niqtree2 -s input/turtle.fa -B 1000 -T AUTO\n\nRun options explained:\n\n-s input/turtle.fa the path to the input alignment in turtle.fa.\n\n-B 1000 1000 replicates for the ultrafast bootstrap (Minh et al., 2013).\n\n-T AUTO auto select the best number of CPU cores to speed up the analysis.\n\nOutput files (in out):\n\nturtle.fa.iqtree the main report file that is self-readable. You should look at this file to see the computational results. It also contains a textual representation of the final tree.\n\nturtle.fa.treefile the ML tree in NEWICK format, which can be visualized in FigTree or any other tree viewer program.\n\nturtle.fa.log log file of the entire run (also printed on the screen).\n\nturtle.fa.ckp.gz checkpoint file used to resume an interrupted analysis.\n… and a few other files.\n\n\n\n\n\n\n\nQuestions\n\n\n\n\nLook at the report file turtle.fa.iqtree, and the log file turtle.fa.log.\nWhat is the best-fit model name? What do you know about this model? (see substitution models available in IQ-TREE)\nWhat are the AIC/AICc/BIC scores of this model and tree?\nVisualise the tree turtle.fa.treefile in a tree viewer software like FigTree (or read into R with ape and plot()).\n\nWhat relationship among three trees does this tree support?\nWhat is the ultrafast bootstrap support (%) for the relevant clade?\nDoes this tree agree with the published tree (Chiari et al. 2012)?\n\n\n\n\n\nSpecies Tree partitioned by locus\nIt is well known that some genes evolve faster than others (for example nuclear protein-coding genes vs. mitochondrial genes vs. non-coding genes). Partitioning by locus allows the rate of substitution for each locus to be scaled to different evolutionary rates for an improved fit of the evolutionary model to the observed DNA sequence data.\nWe add a partition file indicating where the loci are in the alighment:\n\n\nTerminal\n\n# infer the species tree with 1000 ultrafast bootstraps \n# and an edge-linked fully-partitioned model \niqtree2 -s input/turtle.fa -p input/turtle.nex --prefix out/species -B 1000 -nt AUTO\n\nRun options explained:\n\n-p input/turtle.nex the partition file that delimits the partitions in the alighment to specify an edge-linked proportional partition model (Chernomor et al., 2016). There is one set of branch lengths for the phylogeny, but each partition can have proportionally shorter or longer tree length (= same as slower or faster evolutionary rates, respectively).\n\n\n\n\n\n\n\nQuestions\n\n\n\n\nLook at the report file turtle.fa.iqtree, and the log file turtle.fa.log.\nWhat are the AIC/AICc/BIC scores of this model and tree? How does it compare?\nWhat relationship among three trees does this tree support?\n\n\n\n\n\nGene Trees\nWe will now infer a separate phylogeny for each locus (= gene). It is really easy to compute gene trees in IQTREE. The same inputs are used as for the paritioned model, but we just tell IQTREE to estimate each gene tree separately with a different (-S) flag for the partition file:\n\n\nTerminal\n\n# infer single-locus trees  \niqtree2 -s input/turtle.fa -S input/turtle.nex -m TEST --prefix out/loci -B 1000 \n\nRun options explained:\n\n-S input/turtle.nex to infer separate trees for every partition in turtle.nex. All output files are similar to a partition analysis, except that the tree turtle.loci.treefile now contains multiple trees - one for each gene."
  },
  {
    "objectID": "posts/2023-04-13-iqtree-phylogenetics/index.html#concordance-factors",
    "href": "posts/2023-04-13-iqtree-phylogenetics/index.html#concordance-factors",
    "title": "Phylogenetic Inference with IQTREE2",
    "section": "Concordance factors",
    "text": "Concordance factors\nTypcial phylogenetic inference methods essentially imply that gene trees and species tree are one and the same. However, it is well known that gene trees might be discordant (i.e., individual genes may have differet histories). We can actually check with large multilocus datasets (with many loci of sufficient length). Therefore, we now want to quantify the agreement between gene trees and species tree in a so-called concordance factor (Minh, Hahn, and Lanfear 2020).\nYou can now compute gene concordance factor (gCF) and site concordance factor (sCF) for the tree inferred under the partition model:\n\n\nTerminal\n\n# locus modelfinder species tree vs. single-locus trees \niqtree2 -t out/species.treefile --gcf out/loci.treefile -s input/turtle.fa --scf 100 --prefix out/concord \n\nRun options explained:\n\n-t out/species.treefile to specify the species tree. We use tree under the partitioned model here, but you can of course use the other tree.\n--gcf out/loci.treefile to specify a gene-trees file.\n--scf 100 to draw 100 random quartets when computing sCF.\n\nOutput files (in out):\n\nconcord.cf.tree: tree file where branches are annotated with bootstrap/gCF/sCF values.\nconcord.cf.stat: a table file with various statistics for every branch of the tree.\n\nSimilarly, you can compute gCF and sCF for the tree under different species trees, for example if you generate an unpartitioned model:\n\n\nTerminal\n\niqtree2 -t out/unpartitioned.treefile --gcf out/loci.treefile -s input/turtle.fa --scf 100 --prefix out/concord.unpartitioned \n\n\n\n\n\n\n\nTip\n\n\n\nIf you need to redo any of these analyses, add the -redo flag at the end of the command.\n\n\n\n\n\n\n\n\nDefinitions\n\n\n\n\nGene concordance factor (gCF) is the percentage of decisive gene trees concordant with a particular branch of the species tree (0% &lt;= gCF(b) &lt;= 100%). gCF=0% means that branch b does not occur in any gene trees, whereas gCF=100% means that branch b occurs in every gene tree.\n\nSite concordance factor (sCF) is the percentage of decisive (parsimony informative) alignment sites supporting a particular branch of the species tree (~33% &lt;= sCF(b) &lt;= 100%). sCF&lt;33% means that another discordant branch b’ is more supported, whereas sCF=100% means that branch b is supported by all sites.\n\nCAUTION when gCF ~ 0% or sCF &lt; 33%, even if boostrap supports are ~100%!\nGREAT when gCF and sCF &gt; 50% (i.e., branch is supported by a majority of genes and sites).\n\n\n\n\n\n\n\n\n\nQuestions\n\n\n\n\nVisualise concord.cf.tree.nex in FigTree.\nExplore gene concordance factor (gCF), gene discordance factors (gDF1, gDF2, gDFP), site concordance factor (sCF) and site discordance factors (sDF1, sDF2).\nHow do gCF and sCF values look compared with bootstrap supports?"
  },
  {
    "objectID": "posts/2023-04-13-iqtree-phylogenetics/index.html#using-concordance-factors-to-understand-your-data",
    "href": "posts/2023-04-13-iqtree-phylogenetics/index.html#using-concordance-factors-to-understand-your-data",
    "title": "Phylogenetic Inference with IQTREE2",
    "section": "Using concordance factors to understand your data",
    "text": "Using concordance factors to understand your data\nThe first step is just to look at the concordance factors on the tree. To do this, load the output tree (concord.cf.tree) in any tree viewer such as Figtree or you can use R. In this tree, each branch label shows bootstrap / gCF / sCF.\nWe can also plot the trees using ggtree (but for a first look around I usually use FigTree).\n\nrequire(viridis)\nrequire(GGally)\nrequire(entropy)\nrequire(ggtree)\nrequire(treeio)\nrequire(tidytree)\nrequire(ggplot2)\nrequire(dplyr)\nrequire(ggrepel)\n\n\n# read the data\nd = read.delim(\"out/concord.cf.stat\", header = T, comment.char='#')\n               \n# rename the bootstrap and branchlength columns\nnames(d)[names(d)==\"Label\"] = \"bootstrap\"\nnames(d)[names(d)==\"Length\"] = \"branchlength\"\n\n# plot the tree\ntree &lt;- read.iqtree(\"out/concord.cf.tree\")  # read in tree file\ntib &lt;- as_tibble(tree)\nd$node &lt;- d$ID + 1  # need to +1 to make IQTREE node numbers = ggtree \n\n# merge the tree and data \ntd &lt;- full_join(tib, d, by = \"node\")  # combine the tree (tib) and data (d)\ntd &lt;- as.treedata(td)       # coerce to treedata format (td)\n\nggtree(td) +          # plot tree\n  theme(legend.position=\"right\") +\n  geom_tiplab() +                         # add tip labels\n  geom_label2(aes(label=label, subset=!isTip), color=\"black\")  # add node labels \n\n\n\n\nThe turtle clade contains Chelonoidis to Phrynops. Which clade do turtles group with? This part of the tree immediately illustrates the most important point: bootstraps and concordance factors are giving you very different information about each branch in the tree. Just take a look at how different the three numbers can be!"
  },
  {
    "objectID": "posts/2023-01-17-the-big-picture/index.html#data-science",
    "href": "posts/2023-01-17-the-big-picture/index.html#data-science",
    "title": "Introduction and The Big Idea",
    "section": "Data Science",
    "text": "Data Science\nFrom R for Data Science 2e by Hadley Wickam, Garrett Grolemund, and Mine Çetinkaya-Rundel\n\n\n\nQuestion -&gt; Gather Data -&gt; Design\nObserve -&gt; Record Data -&gt; Data Table\nDocument -&gt; Comment (annotate)\nProject -&gt; Version Control -&gt; Share"
  },
  {
    "objectID": "posts/2023-01-17-the-big-picture/index.html#learning-r---a-first-session",
    "href": "posts/2023-01-17-the-big-picture/index.html#learning-r---a-first-session",
    "title": "Introduction and The Big Idea",
    "section": "Learning R - A first session",
    "text": "Learning R - A first session\n\n\nThink about how R works as you try out commands\nhttps://www.r-project.org Go to manuals, click on An Introduction to R\nFollow Section 2.1\ninput -&gt; R -&gt; output\nWhat came out? What does it tell you about the rules R follows?\nComputers only do Exactly what you tell them to do\nJump to Appendix A - letʻs try to understand some rules of R together"
  },
  {
    "objectID": "posts/2023-01-17-the-big-picture/index.html#goals",
    "href": "posts/2023-01-17-the-big-picture/index.html#goals",
    "title": "Introduction and The Big Idea",
    "section": "Goals",
    "text": "Goals\n\nCreativity\nAccuracy\nAuthority\nRepeatability\nCommunication\n\nEase of dissemination to other researchers\nShow me, donʻt tell me"
  },
  {
    "objectID": "posts/2023-01-17-the-big-picture/index.html#tools",
    "href": "posts/2023-01-17-the-big-picture/index.html#tools",
    "title": "Introduction and The Big Idea",
    "section": "Tools",
    "text": "Tools\n\n\nOur senses\nNotebooks and Pens\nExcel\nCommand Line (UNIX/ Windows/ File Commands in your operating system)\nR\nGit/GitHub\nQuarto/Rmarkdown\nand many more…"
  },
  {
    "objectID": "posts/2023-01-17-the-big-picture/index.html#data-science-1",
    "href": "posts/2023-01-17-the-big-picture/index.html#data-science-1",
    "title": "Introduction and The Big Idea",
    "section": "Data Science",
    "text": "Data Science\n\n\n\n\n\nNeed\nTools\n\n\n\n\nObserve -&gt; Record Data -&gt; Data Table\nNotebooks\n\n\nDocument -&gt; Comment (annotate)\nR\n\n\nProject -&gt; Version Control -&gt; Share\nGit/GitHub\n\n\nCommunicate\nR/ Quarto"
  },
  {
    "objectID": "posts/2023-01-17-the-big-picture/index.html#software",
    "href": "posts/2023-01-17-the-big-picture/index.html#software",
    "title": "Introduction and The Big Idea",
    "section": "Software",
    "text": "Software\n\nDo you have R installed and working? (R studio is optional)\nFor a longer walk-through here is another resource: Introduction to R/Rstudio\nGit/GitHub: Please Install. Letʻs follow Introduction to Git/GitHub"
  },
  {
    "objectID": "posts/2023-01-31-reading-data/index.html",
    "href": "posts/2023-01-31-reading-data/index.html",
    "title": "Data IO",
    "section": "",
    "text": "Material for this lecture was borrowed and adopted from\n\nhttp://rafalab.dfci.harvard.edu/dsbook/importing-data.html\nhttps://www.stephaniehicks.com/jhustatcomputing2022"
  },
  {
    "objectID": "posts/2023-01-31-reading-data/index.html#get-the-files-onto-your-computer",
    "href": "posts/2023-01-31-reading-data/index.html#get-the-files-onto-your-computer",
    "title": "Data IO",
    "section": "Get the files onto your computer",
    "text": "Get the files onto your computer\nFirst, get the data files from the rcassdatadata GitHub repo. Navigate to within the rclassdata folder on your computer and execute:\n\n\nTerminal\n\ngit pull origin main\n\nYou should probably make a R working directory for your class work. Maybe call it Rclass? Then move or drag rclassdata within it.\nNext, from within R, check which working directory you are in. You should be in your Rclass folder. If you are not, use setwd() to get there.\n\ngetwd()\nsetwd(\"~/Rclass\")  # my folder is at the top level of my user directory\n\nCopy the 2023-01-31-DataIO folder into Rclass to work for today."
  },
  {
    "objectID": "posts/2023-01-31-reading-data/index.html#read.csv",
    "href": "posts/2023-01-31-reading-data/index.html#read.csv",
    "title": "Data IO",
    "section": "read.csv",
    "text": "read.csv\nGetting the file in is easy. If it is in csv format, you just use:\n\nread.csv(\"anolisSSD.csv\")  # look for the file in the Data directory\n\nThis is an Anolis lizard sexual size dimorphism dataset. It has values of dimorphism by species for different ecomorphs, or microhabitat specialists.\nTo save the data, give it a name and save it:\n\nanolis &lt;- read.csv(\"anolisSSD.csv\")\n\nIt is a good practice to always check that the data were read in properly. If it is a large file, you’ll want to at least check the beginning and end were read in properly:\n\nhead(anolis)\n\n  species   logSSD    ecomorph\n1      oc -0.00512        twig\n2      eq  0.08454 crown-giant\n3      co  0.24703 trunk-crown\n4     aln  0.24837 trunk-crown\n5      ol  0.09844  grass-bush\n6      in  0.06137        twig\n\ntail(anolis)\n\n   species  logSSD     ecomorph\n18      cr 0.39796 trunk-ground\n19      st 0.15737  trunk-crown\n20      cy 0.26024 trunk-ground\n21     alu 0.08216   grass-bush\n22      lo 0.13108        trunk\n23      an 0.13547         twig\n\n\nVoila! Now you can plot, take the mean, etc. Which prints out the first six and last six lines of the file.\nNow try reading in anolisSSDsemicolon.csv what did you get? Try reading it in with read.table() - check out the help page.\nR can read in many other formats as well, including database formats, excel native format (although it is easier in practice to save as .csv), fixed width formats, and scanning lines. For more information see the R manual “R Data Import/Export” which you can get from help.start() or at http://www.r-project.org."
  },
  {
    "objectID": "posts/2023-01-31-reading-data/index.html#input-files-generated-by-data-loggers",
    "href": "posts/2023-01-31-reading-data/index.html#input-files-generated-by-data-loggers",
    "title": "Data IO",
    "section": "Input files generated by data loggers",
    "text": "Input files generated by data loggers\nFiles that are generated by computer, even if they are not separated by commas (.csv) are not too bad to deal with. Take, for example, the file format generated from our hand-held Ocean Optics specroradiometer. It is very regular in structure, and we have tons of data files, so it is well worth the programming effort to code a script for automatic file input.\nFirst, you can open the file below in a text editor. If you’d rather open it in R, you can use:\n\nreadLines(\"20070725_01forirr.txt\")\n\nNotice that there is a very large header, in fact the first 17 lines. Notice also that the last line will cause a problem. Also, the delimiter in this file is tab (backslash t).\n\ntemp &lt;- readLines(\"20070725_01forirr.txt\")\nhead(temp)\ntail(temp)\n\nWe can solve these issues using the skip and the comment.char arguments of read.table to ignore both types of lines, reading in only the “good stuff”. Also, the default delimiter in this function is the tab:\n\ndat &lt;- read.table(file=\"20070725_01forirr.txt\", skip=17, comment.char=\"&gt;\")\nnames(dat) &lt;- c(\"lambda\", \"intensity\")\nhead(dat)\n\n  lambda intensity\n1 177.33         0\n2 177.55         0\n3 177.77         0\n4 177.99         0\n5 178.21         0\n6 178.43         0\n\ntail(dat)\n\n     lambda intensity\n3643 888.21   0.29491\n3644 888.38   0.31306\n3645 888.54   0.28153\n3646 888.71   0.28245\n3647 888.87   0.18988\n3648 889.04   0.18988\n\n\nThe file produces (useless) rows of data outside of the range of accuracy of the spectraradiometer. We can get rid of these by subsetting the data, selecting only the range 300-750nm:\n\ndat &lt;- dat[dat$lambda &gt;= 300, ]  # cut off rows below 300nm\ndat &lt;- dat[dat$lambda &lt;= 750, ]  #cut off rows above 750nm\n\nOr do both at once:\n\ndat &lt;- dat[dat$lambda &gt;= 300 & dat$lambda &lt;= 750,]\n\nIf we are going to be doing this subsetting over and over, we might want to save this as an index vector which tells us the position of the rows of data we want to keep in the dataframe (don’t worry, we’ll cover this again in the workhorse functions chapter).\n\noo &lt;- dat$lambda &gt;= 300 & dat$lambda &lt;= 750\ndat &lt;- dat[oo, ]   # same as longer version above\n\nWe can now save the cleaned up version of the irradiance data:\n\nwrite.csv(dat, \"20070725_01forirr.csv\")"
  },
  {
    "objectID": "posts/2023-01-31-reading-data/index.html#human-error",
    "href": "posts/2023-01-31-reading-data/index.html#human-error",
    "title": "Data IO",
    "section": "Human Error",
    "text": "Human Error\nIt is very easy to make a typo, and humans are really bad at catching our own typos in real time. Right? They creep in despite our best efforts. This is why even though it is possible to measure specimens and enter the numbers directly into the computer, itʻs not something I would do.\nI always write my measurements down into a notebook or a datasheet using pencil and paper. This has saved me many errors in three ways (1) sometimes my brain is still processing what I just wrote and I will catch a typo. (2) I write my data in rows for specimens and columns for the different measurements. As the dataset builds, it is easy to notice errors if some number is really off. (3) If you have any doubts, you can quickly look over your page and re-measure anything suspicous. If you have the person-power, you can also have one person taking the measurements and calling them out, and another writing them down and repeating them back. It is a very effective way to check on the spot.\nI had my first post-college job at an insurance rating board. This is a business that deals with reams and reams of data. We did work a lot with Fortran code and spreadsheets, but surprisingly, some of it does actually have to get manually checked. The protocol was simple, a paper printout of the old version was put next to the new version and a person went along with a ruler, literally putting a check mark after verifying the number. A second check was done as well, and then I finally understood the meaning of “double checking” 😝. When I got to grad school, it was eye-opening to find that checking any personʻs data entry was a rare practice 😲. Please, whenever possible check your data entry. So here are some tips:\n\nRecord data in a notebook (paper and pencil). It serves as a permanent record.\nWrite it down in a data table format in an order that minimizes error. For example, if it is convenient to take five measurements in a particular order, then organize your table that way and always take the measurements in the same order.\nOrganize your spreadsheet to mirror the hand-written data. This will minimize data entry errors.\nHave another person check your data entry against the notebook."
  },
  {
    "objectID": "posts/2023-01-31-reading-data/index.html#organizing-your-spreadhseet",
    "href": "posts/2023-01-31-reading-data/index.html#organizing-your-spreadhseet",
    "title": "Data IO",
    "section": "Organizing your spreadhseet",
    "text": "Organizing your spreadhseet\nWhen it comes time to enter your data in a spreadsheet, there are many things you can do to improve organization. Below is a summary of the recommendations made in paper by Karl Broman and Kara Woo (Broman and Woo 2018).\n\nBe Consistent - Have a plan before you start entering data. Be consistent and stick to it.\nChoose Good Names for Things - You want the names you pick for objects, files, and directories to be memorable, easy to spell, descriptive, but concise. This is actually a hard balance to achieve and it does require time and thought.\n\nOne important rule to follow is do not use spaces, use underscores _ or dashes instead -.\nAlso, avoid symbols; stick to letters and numbers.\n\nWrite Dates as YYYY-MM-DD - To avoid confusion, we strongly recommend using this global ISO 8601 standard.\nNo Empty Cells - Fill in all cells and use some common code for missing data.\nPut Just One Thing in a Cell - It is better to add columns to store the extra information rather than having more than one piece of information in one cell.\nMake It a Rectangle - The spreadsheet should be a rectangle.\nCreate a Data Dictionary - If you need to explain things, such as what the columns are or what the labels used for categorical variables are, do this in a separate file. This is an excellent use for a README.md file\nNo Calculations in the Raw Data Files - Excel permits you to perform calculations. Do not make this part of your spreadsheet. Code for calculations should be in a script.\nDo Not Use Font Color or Highlighting as Data - Most import functions are not able to import this information. Encode this information as a variable (a “comment” column) instead.\nMake Backups - Make regular backups of your data.\nUse Data Validation to Avoid Errors - Leverage the tools in your spreadsheet software so that the process is as error-free and repetitive-stress-injury-free as possible. Think of checks you can do for “reality checks”.\nSave the Data as Text Files - Save files for sharing in comma or tab delimited format. An unambiguous text format is the best for archiving your data."
  },
  {
    "objectID": "posts/2023-01-31-reading-data/index.html#text-versus-binary-files",
    "href": "posts/2023-01-31-reading-data/index.html#text-versus-binary-files",
    "title": "Data IO",
    "section": "Text versus binary files",
    "text": "Text versus binary files\nFor data science purposes, files can generally be classified into two categories: text files (also known as ASCII files) and binary files. You have already worked with text files. All your R scripts are text files and so are the Quarto files used to create this website. The .csv tables you have read are also text files. One big advantage of these files is that we can easily “look” at them using a plain text editor, without having to purchase any kind of special software.\nAny text editor can be used to examine a text file, including freely available editors such as R, RStudio, Atom, Notepad, TextEdit, vi, emacs, nano, and pico. However, if you try to open, say, an Excel xls file, jpg or png file, you will not be able to see anything immediately useful. These are binary files. Excel files are actually compressed folders with several text files inside. But the main distinction here is that text files can be easily examined.\nAlthough R includes tools for reading widely used binary files, such as xls files, in general you will want to find data sets stored in text files. If necessary, use the proprietary software to export the data into text files and go from there.\n\n\n\n\n\n\nWarning\n\n\n\nThe problem with using propietary formats for data management is that htey are poor formats for achival purposes. These formats may change or they donʻt copy well, so that eventually you can no longer open them.\n\n\nSimilarly, when sharing data you want to make it available as text files as long as storage is not an issue (binary files are much more efficient at saving space on your disk). In general, plain-text formats make it easier to share data since commercial software is not required for working with the data, and they are more reliable.\nTechnically, html and xml files are text files too, but they have complicated tags around the information. In the Data Wrangling part of the book we learn to extract data from more complex text files such as html files."
  },
  {
    "objectID": "posts/2023-01-31-reading-data/index.html#unicode-versus-ascii",
    "href": "posts/2023-01-31-reading-data/index.html#unicode-versus-ascii",
    "title": "Data IO",
    "section": "Unicode versus ASCII",
    "text": "Unicode versus ASCII\nA pitfall in data science is assuming a file is an ASCII text file when, in fact, it is something else that can look a lot like an ASCII text file, for example, a Unicode text file.\nTo understand the difference between these, remember that everything on a computer needs to eventually be converted to 0s and 1s (binary format). ASCII is an encoding that maps bits to characters that are easier for humans to read.\n\n\n\n\n\n\nBinary data can take on only two values - 0 or 1\n\n\n\n\na bit is the smallest unit, a single binary character (0 or 1).\na byte is eight bits.\na megabyte or MB is one million bytes.\na gigabyte or GB is one billion bytes.\nYou can roughly calculate the size of your data by the numbers of bytes per each observation.\n\n\n\nASCII uses 7 bits – seven variables that can be either 0 or 1 – which results in 27 = 128 unique items, enough to encode all the characters on an English language keyboard (all characters, numbers, and symbols, Figure 1, Figure 2). However, we need to expand the possibilities if we want to include support for other languages or additional characters.\n\n\n\nFigure 1: Binary encodings in ACII, with some of the most used escape sequences like \\t for tab and \\n for new line.\n\n\n\n\n\nFigure 2: Some examples of ASCII encodings for A-E and a-e.\n\n\nFor this reason, a new encoding, using more than 7 bits, was defined: Unicode. When using Unicode, one can chose between 8, 16, and 32 bits abbreviated UTF-8, UTF-16, and UTF-32 respectively. RStudio actually defaults to UTF-8 encoding.\nAlthough we do not go into the details of how to deal with the different encodings here, it is important that you know these different encodings exist so that you can better diagnose a problem if you encounter it. One way problems manifest themselves is when you see “weird looking” characters you were not expecting.\nMany plain text editors (Atom, Sublime, TextWrangler, Notepad++) will detect encodings and tell you what they are and may also convert between them. Also from the command line the file command will reveal the encoding (should also work on Windows if you have git installed:\n\n\nTerminal\n\nfile filename\n\nThis StackOverflow discussion is an example: https://stackoverflow.com/questions/18789330/r-on-windows-character-encoding-hell."
  },
  {
    "objectID": "posts/2023-01-31-reading-data/index.html#line-endings",
    "href": "posts/2023-01-31-reading-data/index.html#line-endings",
    "title": "Data IO",
    "section": "Line endings",
    "text": "Line endings\nOne last potential headache is the character used for line endings. The line ending is the invisible (to us) character that is added to your file when you press the return key.\nIt is all one stream of informatin to the computer, but the computer will interpret the information as a new line when it detects one of these characters. When software is provided a file with an unexpected line ending, it is not able to properly detect the lines of information (maybe it sees only one huge line). See?\nThere are two types of line endings in use today:\n\nOn UNIX and MacOS, text file line-endings are terminated with a newline character (ASCII 0x0a, represented by the \\n escape sequence in most languages), also referred to as a linefeed (LF).\nOn Windows, line-endings are terminated with a combination of a carriage return (ASCII 0x0d or \\r) and a newline(\\n), also referred to as CR/LF.\n\nIf your computer complains about line endings, the easiest thing to do is to open it in one of the good plain text editors and save it with the line endings it is expecting (usually as LF instead of CR/LF)."
  },
  {
    "objectID": "posts/2023-04-11-geometric-morphometrics-analyses/index.html",
    "href": "posts/2023-04-11-geometric-morphometrics-analyses/index.html",
    "title": "Procrustes Superimposition and Analyses",
    "section": "",
    "text": "Acknowledgements\nReferences for this Material:\n\nThe authors of the geomorph package for developing many useful vignettes (Antigoni Kaliontzopoulou) and sample datasets which is drawn upon below\nBardua et al (2019) A Practical Guide to Sliding and Surface Semilandmarks in Morphometric Analyseshttps://academic.oup.com/iob/article/1/1/obz016/5526881\nEmma Sherratʻs quick guide to Geomorph\n\n\n\nLearning objectives\n\n\n\n\n\n\nLearning objectives\n\n\n\nAt the end of this lesson you will:\n\nBe able to create shape data from geometric morphometric data\nUnderstand the general concepts for Procrustes superimposition\nBe able to perform basic analyses on geometric morphometric data\n\n\n\n\n\nThe goals of Geometric Morphometrics\nFrom (MacLeod 2010):\n\nThe ultimate goals for any geometric morphometric analysis - to define a mathematical space in which we can compare sets of landmark configurations that (1) ordinates shapes on the basis of their similarity, (2) treats these configurations as a whole entity rather than an accumulation of independent parts, (3) respects the conventions of the Kendall shape space, (4) supports shape modelling, and (5) is stable in the face of minor changes to the sample and/or reference shape.\n\nWhat is a Kendall shape space, you ask? A shape space or coordinate system is used to describes shapes, and importantly, be used to understand differences between shapes. Kendallʻs shape space describes curves and local approxmation to curves using tangents Klingenberg (2020). For this lesson just remember that shapes are connected by geometries, which we can describe mathematically. This makes it much easier to compare the major differences between multidimensional shapes.\n\n\nSize Adjustment with Procrustes Superimposition\nProcrustes superimposition is the most widely used method to create size and shape variables from landmark data. The basic idea is that larger objects should have landmarks that are farther apart than the same landmarks in smaller objects.\n\nCentroid\nA key concept is the centroid, which the center point of all of the landmarks. It is easily calculated for each specimen by taking the average of all of the landmark coordinates along each axis. For p landmarks in two dimensions the coordinates of the centroid would be:\n\\[\nC_{x} = \\frac{X_1+X_2+X_3+ ... + X_p}{p}\n\\] \\[\nC_{y} = \\frac{Y_1+Y_2+Y_3 ... + Y_p}{p}\n\\]\n\n\nCentroid size\nThe concept of centroid size describes geometric size. It is calculated from the distances of each landmark from the centroid, but it is a geometric mean because it is caculated by taking the square-root of the sums of squared distances of each landmark from the centroid.\n\n\n\nThe centroid of a triangle (with landmarks at the vertices) is the point in the center.\n\n\n[Source: Wikipedia]\n\n\nShape Variables\nShape variables calculated from Procrustes superimposition mathematically separate scale (size) from shape by applying shape-preserving transformations to make the landmarks as similar as possible.\nThe concept: In order to compare differences in shape, we can remove size (in other words, normalize by size or bring all of the specimens to the same size) with the following transformations:\n\nScale: Bring each set of landmarks to the same size by magnifing or reducing the distances of the landmarks around their centroid.\n\nTranslate: Shift the landmarks in the coordinate space to place the centroids at a common point.\n\nRotate: Rotate the landmarks with an angular transformation to bring them into a common orientation.\n\nAt the end of Procrustes superimposition, the landmarks for the different specimens will be scaled to the same size and their coordinates will lie as close together as possible while preserving the differences in shape. The figure below illustrates these steps.\nThe mathematics of these transofrmations come from basic linear algebra operations on matrices, which are beyond the scope of the class, but to understand what is happening we can analogize it to univariate arithmetic.\nScaling is like dividing each variable by a size variable to put all of the individuals on the same baseline with regard to size (magnitude).\nTranslation brings the centers of all of the landmarks into alignment. Think of a situation where we measured the length of specimens according to wherever they fell within the picture frame, for example if one specimen started at 2 and extended to 10, and another started at 6 and extended to 12. We could translate (or slide) each of these to place their midpoint at zero by subtracting the mean value from each of their endpoints. For the first specimen, we would subtract 6, translating the specimen from (2,10) to (-4,4), and for the second we would subtract 9, tanslating the specimen from (6,12) to (-3,3).\nRotation is an angular rotation to bring a 2 or 3D object into a common orientation. For a more thorough discussion please see (Zelditch, Swiderski, and Sheets 2012).\n\n\n\nFigure: Procrustes superimposition. The figure shows the three transformation steps of an ordinary Procrustes fit for two configurations of landmarks. (a) Scaling of both configurations to the same size; (b) Translation to the same position of the center of gravity; (c) Rotation to the orientation that provides the minimum sum of squared distances between corresponding landmarks.\n\n\nSource: Christian Klingenberg\n\n\n\n\n\n\nFrom the documentation for gpagen() in geomorph:\n\n\n\nGPA translates all specimens to the origin, scales them to unit-centroid size, and optimally rotates them (using a least-squares criterion) until the coordinates of corresponding points align as closely as possible. The resulting aligned Procrustes coordinates represent the shape of each specimen, and are found in a curved space related to Kendall’s shape space (Kendall 1984). Typically, these are projected into a linear tangent space yielding Kendall’s tangent space coordinates (i.e., Procrustes shape variables), which are used for subsequent multivariate analyses (Dryden and Mardia 1993, Rohlf 1999).\n\n\n\n\n\nGeomorph Package\n\nData Structures\nThe package geomorph is written to work with standard geomorphic morphometrics data file types such as TPS and NTS, as well as others Baken et al. (2021).\nreadland.tps() reads morphometric data in TPS format. It returns an array for storing landmark data: p landmarks X k dimensions X N specimens\nThe third dimension of this array contains names for each specimen, which are obtained from the image names in the .tps file.\n\n\nBuilt-in Example\nLetʻs look again at the built-in dataset plethodon that comes with geomorph. It has skull landmark data for 40 specimens of two species of plethdon salamanders (Dean C. Adams 2004).\n\n\n\nPlethodon jordani, Photo by Kevin Stohlgren\n\n\nSource: Ambibians and Reptiles of North Carolina website\nThe dataset plethodon is a list.\n\nrequire(geomorph)\n\nLoading required package: geomorph\n\n\nLoading required package: RRPP\n\n\nLoading required package: rgl\n\n\nLoading required package: Matrix\n\ndata(plethodon) \nclass(plethodon)\n\n[1] \"list\"\n\nattributes(plethodon)\n\n$names\n[1] \"land\"    \"links\"   \"species\" \"site\"    \"outline\"\n\ntable(plethodon$species)\n\n\n Jord Teyah \n   20    20 \n\ndim(plethodon$land)\n\n[1] 12  2 40\n\n\nThe landmarks are in plethodon$land. There are twenty of each species, and from the dimensions of plethodon$land we see that each of the 40 specimens has 12 landmarks in two dimensions.\nplotAllSpecimens plots the coordinates of all samples, so we can start to look at variation.\n\nShape: General Procrustes Analysis\n\n#|warning: false\nplotAllSpecimens(plethodon$land)\n\n\n\nY.gpa &lt;- gpagen(plethodon$land, PrinAxes = FALSE)\n\n\nPerforming GPA\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |======================================================================| 100%\n\nMaking projections... Finished!\n\nsummary(Y.gpa)\n\n\nCall:\ngpagen(A = plethodon$land, PrinAxes = FALSE) \n\n\n\nGeneralized Procrustes Analysis\nwith Partial Procrustes Superimposition\n\n12 fixed landmarks\n0 semilandmarks (sliders)\n2-dimensional landmarks\n2 GPA iterations to converge\n\n\nConsensus (mean) Configuration\n\n             X            Y\n1   0.15263287 -0.023350400\n2   0.19445064 -0.092643121\n3  -0.03361223 -0.007346212\n4  -0.28069721 -0.092850147\n5  -0.30998751 -0.061672264\n6  -0.32557841 -0.036112255\n7  -0.31804390  0.036125318\n8  -0.18947114  0.098010902\n9   0.02036829  0.099112877\n10  0.18852641  0.077278061\n11  0.35134314  0.065866346\n12  0.55006905 -0.062419105\n\n\nThe summary provides the mean for each of the coordinates after GPA. We can see the mean of the coordinates among the individual coordinates by plotting:\n\nplot(Y.gpa)\n\n\n\n\nLetʻs compare the plot of all specimens before and after Generalized Procrustes Analysis. The below plot is before GPA, on the raw landmarks plethodon$land:\n\nplotAllSpecimens(plethodon$land)\n\n\n\n\nPlotting again after alignment with Generalized Procrustes Analysis gives a good representation of shape variation. Plotting links between the landmarks helps with visualization.\n\nplotAllSpecimens(Y.gpa$coords,links=plethodon$links)\n\n\n\n\n\n\nPrincipal Components Analysis\nPCA on the procrustes coordinates (size-adjusted or shape variables) can be done with:\n\npleth.pca &lt;- gm.prcomp(Y.gpa$coords)\npleth.pca \n\n\nOrdination type: Principal Component Analysis \nCentering by OLS mean\nOrthogonal projection of OLS residuals\nNumber of observations: 40 \nNumber of vectors 20 \n\nImportance of Components:\n                             Comp1       Comp2        Comp3        Comp4\nEigenvalues            0.001855441 0.001566597 0.0004141056 0.0002278444\nProportion of Variance 0.367433295 0.310233333 0.0820053761 0.0451200584\nCumulative Proportion  0.367433295 0.677666628 0.7596720044 0.8047920628\n                              Comp5        Comp6        Comp7        Comp8\nEigenvalues            0.0001725997 0.0001672575 0.0001549008 0.0001251132\nProportion of Variance 0.0341799312 0.0331220169 0.0306750315 0.0247761773\nCumulative Proportion  0.8389719940 0.8720940109 0.9027690424 0.9275452197\n                              Comp9       Comp10       Comp11       Comp12\nEigenvalues            8.468503e-05 6.869926e-05 5.430853e-05 4.423148e-05\nProportion of Variance 1.677019e-02 1.360452e-02 1.075473e-02 8.759165e-03\nCumulative Proportion  9.443154e-01 9.579199e-01 9.686747e-01 9.774338e-01\n                             Comp13       Comp14       Comp15       Comp16\nEigenvalues            0.0000261742 0.0000192644 1.741769e-05 1.715141e-05\nProportion of Variance 0.0051832795 0.0038149309 3.449226e-03 3.396495e-03\nCumulative Proportion  0.9826170994 0.9864320303 9.898813e-01 9.932778e-01\n                             Comp17       Comp18       Comp19       Comp20\nEigenvalues            1.555244e-05 9.253679e-06 5.336746e-06 3.802714e-06\nProportion of Variance 3.079853e-03 1.832507e-03 1.056837e-03 7.530520e-04\nCumulative Proportion  9.963576e-01 9.981901e-01 9.992469e-01 1.000000e+00\n\n\nThere are 20 PC axes, with 67% of the total shape variation in the first two axes. A plot on PC1 vs PC2 reveals several distinct clusters, suggesting shape differences. The scores are in pleth.pca$x. There is a plot method for geomorph pca results:\n\npleth.pca.plot &lt;- plot(pleth.pca)\n\n\n\n\nThe specimens are from two species in two distinct environments, and there seems to be some separation between the points along PC1 vs. PC2. These points correspond to specimens of two different species in the original TPS data that information is in plethodon$species. Because the species names are a factor, we can use this to do a quick and dirty plot colored by species. We can plot additional PC axes by specifying the axes to plot:\n\nclass(plethodon$species)\n\n[1] \"factor\"\n\npar(mfrow=c(2,2)) # set up 2x2 plots\nplot(pleth.pca, main = \"PCA\", \n      col=plethodon$species # color points by species\n    )\nplot(pleth.pca, main = \"PCA\", \n      axis1 = 1, axis2 = 3, # plot PC axes 1 vs 3\n      col=plethodon$species \n    )\nplot(pleth.pca, main = \"PCA\", \n      axis1 = 2, axis2 = 3, # PC 2 vs PC3\n      col=plethodon$species\n    ) \nplot(pleth.pca, main = \"PCA\", \n      axis1 = 3, axis2 = 4, # PC3 vs PC4\n      col=plethodon$species\n    ) \n\n\n\n\nAll of the separation between species comes from PC1 vs. 2, with no additional separation by adding PC3 or PC4.\n\n\nStatistical test for differences among species and sites\nWe can test the shape variables (GPA coordinates) as explanatory variables for phenotypic varation. Because there are multiple pheontypic (Y) variables, we use MANOVA (multivariate ANOVA).\ngeomorph has custom linear model functions that use randomized residuals (permutation test) to assess linear model fits on the high-dimensional geomorphic morphometric data Collyer and Adams (2021) (rather than assume the data come from a multivariate normal distribution).\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nprocD.lm\nProcrustes ANOVA/regression for Procrustes shape variables\n\n\nprocD.pgls\nPhylogenetic version of procD.lm\n\n\n\ngeomorph has a helper function geomorph.data.frame that creates an object of class geomorph.data.frame from your procrustes shape coordinates that includes the centroid size (Csize), and allows you to associate your metadata such as species and site together with the shape data. The idea is keep it organized in a data-frame like manner, even though it is a list (it makes sure that all the elements have the same length).\n\ngdf &lt;-geomorph.data.frame(Y.gpa$coords)\nattributes(gdf) \n\n$class\n[1] \"geomorph.data.frame\"\n\ngdf &lt;-geomorph.data.frame(Y.gpa, \n              species = plethodon$species, \n              site = plethodon$site\n              )\nattributes(gdf) \n\n$names\n[1] \"coords\"  \"Csize\"   \"species\" \"site\"   \n\n$class\n[1] \"geomorph.data.frame\"\n\n\n\nlm.fit &lt;- procD.lm(coords~species * site, data=gdf)\nsummary(lm.fit)\n\n\nAnalysis of Variance, using Residual Randomization\nPermutation procedure: Randomization of null model residuals \nNumber of permutations: 1000 \nEstimation method: Ordinary Least Squares \nSums of Squares and Cross-products: Type I \nEffect sizes (Z) based on F distributions\n\n             Df       SS       MS     Rsq      F      Z Pr(&gt;F)   \nspecies       1 0.029258 0.029258 0.14856 14.544 4.2241  0.001 **\nsite          1 0.064375 0.064375 0.32688 32.000 5.2101  0.001 **\nspecies:site  1 0.030885 0.030885 0.15682 15.352 5.4075  0.001 **\nResiduals    36 0.072422 0.002012 0.36774                        \nTotal        39 0.196940                                         \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCall: procD.lm(f1 = coords ~ species * site, data = gdf)\n\n\nThe MANOVA reveals significant shape differences between species, between sites, and in the interaction between the two factors (species and site).\nWe can also look at variation with centroid size:\n\nanova(procD.lm(coords ~ Csize + species * site, data = gdf))\n\n\nAnalysis of Variance, using Residual Randomization\nPermutation procedure: Randomization of null model residuals \nNumber of permutations: 1000 \nEstimation method: Ordinary Least Squares \nSums of Squares and Cross-products: Type I \nEffect sizes (Z) based on F distributions\n\n             Df       SS       MS     Rsq       F      Z Pr(&gt;F)   \nCsize         1 0.010409 0.010409 0.05286  5.2986 2.8820  0.001 **\nspecies       1 0.025661 0.025661 0.13030 13.0625 4.0712  0.001 **\nsite          1 0.063018 0.063018 0.31999 32.0780 5.3722  0.001 **\nspecies:site  1 0.029093 0.029093 0.14773 14.8093 5.2418  0.001 **\nResiduals    35 0.068758 0.001965 0.34913                         \nTotal        39 0.196940                                          \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCall: procD.lm(f1 = coords ~ Csize + species * site, data = gdf)\n\n\nThere is signficant multivariate difference among the shape data by size as well as by species and site. This is a good place to note that while size and shape are mathematically uncorrelated by GPA, there could still be some biological variation that produces some pattern of shape variation with size.\n\n\nRelative Warps\nWe can look at the relative shape change between the specimens. This is a common technique you will see in the literature using relative warps analysis. It uses a shape analysis technique called thin plate splines which is a smoothing method that is forced to go through the landmarks, but will fit a surface between them (in as many diemsions as there are in the data).\nRelative warps analysis will show the shape deformation needed to move from a reference shape to a another shape.\nFirst we calcuate the mean shape in all of the data as the reference shape.\n\nref&lt;-mshape(gdf$coords)\n\nCalculate the mean shape of Plethodon jordanii, and we can compare this species to the mean of both species:\n\ngp1.mn&lt;-mshape(gdf$coords[,,gdf$species==\"Jord\"])\nplotRefToTarget(ref,gp1.mn,mag=2, links=plethodon$links)\n\n\n\nplotAllSpecimens(gdf$coords,links=plethodon$links)\n\n\n\n\nWe can see that most of the shape change between P. jordanii and the mean shape is around the jaw hinge (points 1,2) and top of the skull (post cranial, points 11,12). Compare to the sketch of the landmark coordinates.\n\n\n\nSkull landmarks from Dean C. Adams (2004)\n\n\nCan you do the same but comparing the mean of the two species against each other?\n\n\n\n\nMore geomorphic morphometrics software\nhttps://academic.oup.com/view-large/223239151\n\n\nMore reading\n\n(Bookstein 1992) Morphometric Tools for Landmark Data: Geometry and Biology\n(Webster and Sheets 2010) A Practical Introduction to Landmark-Based Geometric Morphometrics\n(Bardua et al. 2019) A Practical Guide to Sliding and Surface Semilandmarks in Morphometric Analyses\n(Savriama 2018) A Step-by-Step Guide for Geometric Morphometrics of Floral Symmetry\n(Zelditch, Swiderski, and Sheets 2012) Geometric morphometrics for biologists a primer (the green book)\n\n\n\n\n\n\nReferences\n\nAdams, D. C., M. L. Collyer, A. Kaliontzopoulou, and E. K. Baken. 2022. “Geomorph: Software for Geometric Morphometric Analyses. R Package Version 4.0.4.” https://cran.r-project.org/package=geomorph.\n\n\nAdams, Dean C. 2004. “Character Displacement via Aggressive Interference in Appalachian Salamanders.” Ecology 85 (10): 2664–70. http://www.jstor.org/stable/3450422.\n\n\nBaken, E. K., M. L. Collyer, A. Kaliontzopoulou, and D. C. Adams. 2021. “Geomorphic V4.0 and gmShiny: Enhanced Analytics and a New Graphical Interface for a Comprehensive Morphometric Experience.” Methods in Ecology and Evolution.\n\n\nBardua, C, R N Felice, A Watanabe, A -C Fabre, and A Goswami. 2019. “A Practical Guide to Sliding and Surface Semilandmarks in Morphometric Analyses.” Integrative Organismal Biology 1 (1). https://doi.org/10.1093/iob/obz016.\n\n\nBookstein, Fred L. 1992. Morphometric Tools for Landmark Data: Geometry and Biology. Cambridge: Cambridge University Press. https://doi.org/DOI: 10.1017/CBO9780511573064.\n\n\nCollyer, M. L., and D. C. Adams. 2018. “RRPP: An r Package for Fitting Linear Models to High‐dimensional Data Using Residual Randomization.” Methods in Ecology and Evolution.\n\n\n———. 2021. “RRPP: Linear Model Evaluation with Randomized Residuals in a Permutation Procedure, r Package Version 1.1.2.” https://cran.r-project.org/package=RRPP.\n\n\nKendall, David G. 1984. “Shape Manifolds, Procrustean Metrics, and Complex Projective Spaces.” Bulletin of the London Mathematical Society 16 (2): 81–121. https://doi.org/10.1112/blms/16.2.81.\n\n\nKlingenberg, Christian Peter. 2020. “Walking on Kendall’s Shape Space: Understanding Shape Spaces and Their Coordinate Systems.” Evolutionary Biology 47 (4): 334–52. https://doi.org/10.1007/s11692-020-09513-x.\n\n\nMacLeod, Norm. 2010. “PalaeoMath: Part 21 - Principal Warps, Relative Warps, and Procrustes PCA.” Newsletter 75. PaleoMath 101. The Paleontological Association.\n\n\nSavriama, Yoland. 2018. “A Step-by-Step Guide for Geometric Morphometrics of Floral Symmetry.” Frontiers in Plant Science 9. https://doi.org/10.3389/fpls.2018.01433.\n\n\nWebster, Mark, and H. David Sheets. 2010. “A Practical Introduction to Landmark-Based Geometric Morphometrics.” The Paleontological Society Papers 16: 163–88. https://doi.org/DOI: 10.1017/S1089332600001868.\n\n\nZelditch, Miriam Leah, Donald L. Swiderski, and H. David Sheets. 2012. Geometric Morphometrics for Biologists a Primer. Amsterdam ; Elsevier Academic Press."
  },
  {
    "objectID": "posts/2023-01-26-reproducible-research/index.html",
    "href": "posts/2023-01-26-reproducible-research/index.html",
    "title": "Reproducible Research",
    "section": "",
    "text": "The shocking assertion will be that most statistics in most scientific papers has errors. —Charles Geyer\n\n\nPre-lecture materials\n\nRead ahead\n\n\n\n\n\n\nRead ahead\n\n\n\nBefore class, you can prepare by reading the following materials:\n\nStatistical programming, Small mistakes, big impacts by Simon Schwab and Leonhard Held\nReproducible Research: A Retrospective by Roger Peng and Stephanie Hicks\n\n\n\n\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\n\nhttp://users.stat.umn.edu/~geyer/Sweave/\nhttp://users.stat.umn.edu/~geyer/repro.pdf\nhttps://rdpeng.github.io/Biostat776\nReproducible Research: A Retrospective by Roger Peng and Stephanie Hicks\n\n\n\n\nLearning objectives\n\n\n\n\n\n\nLearning objectives\n\n\n\nAt the end of this lesson you will:\n\nKnow the difference between replication and reproducibility\nIdentify valid reasons why replication and/or reproducibility is not always possible\nIdentify the type of reproducibility\nIdentify key components to enable reproducible data analyses\n\n\n\n\n\nIntroduction\nFrom a young age, we have learned that scientific conclusions should be reproducible. After all, isnʻt that what the methods section is for? We are taught to write methods sections so that any scientist could, in theory, repeat the experiment with the idea that if the phenomenon is true, they should obtain comparable results and more often than not should come to the same conclusions.\nBut how repeatable is modern science? Many experiments are now so complex and so expensive that repeating them is not practical. However, it is even worse than that. As datasets get larger and analyses become ever more complex, there is a growing concern that even given the data, we still cannot necessarily repeat the analysis. This is called “the reproducbility crisis”.\nRecently, there has been a lot of discussion of reproducibility in the media and in the scientific literature. The journal Science had a special issue on reproducibility and data replication.\n\nhttps://www.science.org/toc/science/334/6060\n\nTake for example a recent study by the Crowdsourced Replication Initiative (2022). It was a massive effort by 166 coauthors published in PNAS to test repeatability:\n\n73 research teams from around the world analyzed the same social science data.\nThey investigated the same hypothesis: that more immigration will reduce public support for government provision of social policies.\nTogether they fit 1261 statistical models and came to widely varying concluisons.\nA meta-analysis of the results by the PIs could not explain the variation in results. Even after accounting for the choices made by the research teams in designing their statistical tests, 95% of the total variation remained unexplained.\nThe authors claim that “a hidden universe of uncertainty remains.”\n\n\n\n\n\n\nSource: Breznau et al., 2022\nThis should be very disturbing. It was very disturbing to me! Greyer notes that the meta-analysis did not investigate how much of the variability of results was due to outright error. He furthermore notes that while the meta-analysis was done in a reproducibly, the original 73 analyses were not. What does he mean?\n\n\nSome of the issues from a statisticianʻs perspective\nGreyer provides nine ideas worth considering:\n\nMost scientific papers that need statistics have conclusions that are not actually supported by the statistical calculations done, because of\n\nmathematical or computational error,\nstatistical procedures inappropriate for the data, or\nstatistical procedures that do not lead to the inferences claimed.\n\nGood computing practices — version control, well thought out testing, code reviews, literate programming — are essential to correct computing.\nFailure to do all calculations from raw data to conclusions (every number or figure shown in a paper) in a way that is fully reproducible and available in a permanent public repository is, by itself, a questionable research practice.\nFailure to do statistics as if it could have been pre-registered is a questionable research practice.\nJournals that use P &lt; 0.05 as a criterion of publication are not scientific journals (publishing only one side of a story is as unscientific as it is possible to be).\nStatistics should be adequately described, at least in the supplementary material.\nScientific papers whose conclusions depend on nontrivial statistics should have statistical referees, and those referees should be heeded.\nNot all errors are describable by statistics. There is also what physicists call systematic error that is the same in every replication of an experiment. Physicists regularly attempt to quantify this. Others should too.\n\nA reasonable ideal for reproducible research today - Research should be reproducible. Anything in a scientific paper should be reproducible by the reader. - Whatever may have been the case in low tech days, this ideal has long gone. Much scientific research in recent years is too complicated and the published details to scanty for anyone to reproduce it. - The lack of detail is not entirely the author’s fault. Journals have severe page pressure and no room for full explanations. - For many years, the only hope of reproducibility is old-fashioned person-to-person contact. Write the authors, ask for data, code, whatever. Some authors help, some don’t. If the authors are not cooperative, tough. - Even cooperative authors may be unable to help. If too much time has gone by and their archiving was not systematic enough and if their software was unportable, there may be no way to recreate the analysis. - Fortunately, the internet comes to the rescue. No page pressure there! - Nowadays, many scientific papers also point to supplementary materials on the internet. Data, computer programs, whatever should be there, permanently. Ideally with a permanent Document Identifier or DOI. There are complaints that many Supplmentary Materials are incomprehensible, but that can be improved with practices of reproducible reserach.\nTherefore, at the very least scientists should use in their statistical programming - version control, - software testing, - code reviews, - literate programming, and - all data and code available in a permanent public repository.\nSome journals have specific policies to promote reproducibility in manuscripts that are published in their journals. For example, the Journal of American Statistical Association (JASA) requires authors to submit their code and data to reproduce their analyses and a set of Associate Editors of Reproducibility review those materials as part of the review process:\n\nhttps://jasa-acs.github.io/repro-guide\n\n\n\nRecommendations\nDiscuss Table 1\n\nAuthors and Readers\nIt is important to realize that there are multiple players when you talk about reproducibility–there are different types of parties that have different types of interests. There are authors who produce research and they want to make their research reproducible. There are also readers of research and they want to reproduce that work. Everyone needs tools to make their lives easier.\nOne current challenge is that authors of research have to undergo considerable effort to make their results available to a wide audience.\n\nPublishing data and code today is not necessarily a trivial task. Although there are a number of resources available now, that were not available even five years ago, it is still a bit of a challenge to get things out on the web (or at least distributed widely).\nResources like GitHub, kipoi, and RPubs and various data repositories have made a big difference, but there is still a ways to go with respect to building up the public reproducibility infrastructure.\n\nFurthermore, even when data and code are available, readers often have to download the data, download the code, and then they have to piece everything together, usually by hand. It’s not always an easy task to put the data and code together.\n\nReaders may not have the same computational resources that the original authors did.\nIf the original authors used an enormous computing cluster, for example, to do their analysis, the readers may not have that same enormous computing cluster at their disposal. It may be difficult for readers to reproduce the same results.\n\nGenerally, the toolbox for doing reproducible research is small, although it’s definitely growing.\n\nIn practice, authors often just throw things up on the web. There are journals and supplementary materials, but they are famously disorganized.\nThere are only a few central databases that authors can take advantage of to post their data and make it available. So if you are working in a field that has a central database that everyone uses, that is great. If you are not, then you have to assemble your own resources.\n\n\n\n\n\n\n\nSummary\n\n\n\n\nThe process of conducting and disseminating research can be depicted as a “data science pipeline”\nReaders and consumers of data science research are typically not privy to the details of the data science pipeline\nOne view of reproducibility is that it gives research consumers partial access to the raw pipeline elements.\n\n\n\n\n\n\nPost-lecture materials\n\nFinal Questions\nHere are some post-lecture questions to help you think about the material discussed.\n\n\n\n\n\n\nQuestions\n\n\n\n\nWhy can replication be difficult to achieve? Why is reproducibility a reasonable minimum standard when replication is not possible?\nWhat is needed to reproduce the results of a data analysis?\n\n\n\n\n\nAdditional Resources\n\n\n\n\n\n\nTip\n\n\n\n\nReproducibility and Error by Charles J. Geyer"
  },
  {
    "objectID": "posts/2023-03-21-functions/index.html",
    "href": "posts/2023-03-21-functions/index.html",
    "title": "Writing your own functions",
    "section": "",
    "text": "Learning objectives\n\n\n\n\n\n\nLearning objectives\n\n\n\nAt the end of this lesson you will:\n\nUnderstand the components of a function\nBe able to write your own functions\nUnderstand the scope of variables within functions\nUnderstand methods dispatch\nHave a better understanding of how packages work\n\n\n\n\n\nOverview\nWe’ve learned how to write good scripts and debug at the console. You may have noticed that you sometimes have to do the same things over and over again. And you find yourself cutting and pasting bits of code and making minor changes to it. This is a situation where writing your own function is a big help.\nFunctions help in several ways. Once you perfect a bit of code, they help achieve these goals of good programming by writing code that is:\n\nReusable and Generic\n\nModular\n\nEasy to Maintain\n\nWhen do you want to write a function? Any time you find yourself cutting and pasting bits of your code (more than once or twice). Think ahead to making your function reusable.\n\n\nFunctions are wrappers for code that you want to reuse\nFunctions are just bits of code that you want to reuse. You can even build up your own function library in a script like myfunctions.R which you can source with every script you write. So in this way, you can save yourself a lot of trouble by designing and maintaining a tight function library.\nA function is very easy to define. You need a name for your function, some arguments (the input), a valid R statement (i.e., some code to run), and output to return. You then put it together in this following format:\n\nmy_function_name &lt;- function( argument )  statement\n\nThe only things that are actually required are the name of your function, and the word function followed by parentheses. Arguments are optional (well so are the statements, but what would be the point of that?).\nHere is a very simple function to calculate the square of a value:\n\nmysq &lt;- function( x ) {   # function name is mysq\n  x*x      # the function will return the square of x\n}\n\nOnce you run the function definition through the console, your function will be stored in RAM. Then you can then use your function in the normal way that you use functions:\n\nmysq(2)\n\n[1] 4\n\n\nYou can have multiple lines of R code to run, and you can even have functions within functions.The output from the function (the return value) is the last value computed. It is often best practice to explicitly use the return function as in the code below.\n\nmysq &lt;- function( x ) {\n  plot( x, x*x, ylab=\"Square of x\")   # plot x and x*x on the y axis\n  return (x*x)    # return the square of x\n  }\n  \nmysq(1:10)  \n\n\n\n\n [1]   1   4   9  16  25  36  49  64  81 100\n\n\n\n\nArguments\nGenerally speaking, arguments are included in functions because you might want to change them. Things that stay the same are usually hard-coded into the function. But what if you want to change it sometimes but not others?In our little example, what if you wanted to be able to change the label on the y-axis sometimes, but most of the time you wanted it to just say “Square of x”?\n\nmysq &lt;- function( x, yylab=\"Square of x\" ) {   # default argument for the y-label\n\n  plot( x, x*x, ylab=yylab) \n  return (x*x)    \n\n  }\n  \nmysq(1:10, yylab=\"X times X\") \n\n\n\n\n [1]   1   4   9  16  25  36  49  64  81 100\n\n\nWhat happens if you just run the following:\n\nmysq(1:10)\n\n\n\n\n [1]   1   4   9  16  25  36  49  64  81 100\n\n\nThese arguments with default values are therefore optional. Because they will run just fine even if you don’t put anything for them. Whenever I write functions, I try to make as many default arguments as I can so that I can run them with minimal brain power. When you look at it 6 months later, you don’t want to have to reconstruct why you wrote it in the first place just to make it go.\nAnother cute trick is that if you don’t want to have anything as your default value, but you still want to have the option to change it, set the default to NULL.\n\nmysq &lt;- function( x, yylab=NULL) {   # default arg is no value for the y-label, \n\n  plot( x, x*x, ylab=yylab) # but you can specify it if you want to.\n  return (x*x)    \n\n  }\n  \nmysq(1:10, yylab=\"X times X\") \n\n\n\n\n [1]   1   4   9  16  25  36  49  64  81 100\n\n\n\n\nOrder of arguments\nYou may have noticed that you can run a function with or without naming the arguments. For example (here I have supressed printing the figure in quarto):\n\nmysq(2)\n\n[1] 4\n\nmysq(x=2)\n\n[1] 4\n\n\nwork just the same.\nThe reason is that R will assume that if you don’t name the arguments, they are in the same order as in the function definition. Therefore,\n\nmysq( c(1, 3, 5, 7), \"Squares of prime numbers\")\n\n[1]  1  9 25 49\n\nmysq( yylab = \"Squares of prime numbers\", x=c(1, 3, 5, 7))\n\n[1]  1  9 25 49\n\n\nAre the same. Another way to put this, if you don’t want to worry about the order that the arguments are defined in, always use the names=.\n\n\nArbitrary numbers of arguments\nR is very flexible with its arguments. You can also have an arbitrary number of arguments by adding ... This is often used to pass additional arguments to plot(), such as below:\n\nmyfun &lt;- function(x, y,  ...) {\n  plot(x, y, ...)\n  }\n\n# optional args color and line plot are passed to plot()\nmyfun( 1:10, sqrt(1:10), col=\"red\", type=\"l\")   \n\n\n\n\nNote: We did not create arguments for color or type in myfun, but these are arguments for plot, so the ... in the function definition is like the expandable suitcase that allows us to pass anything through. This is for the plot() function, which has many optional arguments, but it can be used for anything.\n\nmyfun( 1:10, sqrt(1:10), cex=3)   # optional arg for point size passed to plot()\n\n\n\n\nAnother common place where variable numbers of arguments comes up is in database queries, where you may want to run a search on a number of terms.\n\nquery &lt;- function( ... ) {\n  paste( ... )\n  }\n\nquery( \"cat\", \"dog\", \"rabbit\")  \n\n[1] \"cat dog rabbit\"\n\n\nOr any situation where you are just not sure how many inputs you will have. For example, you could have a list builder (this is a real function! no joke):\n\naddlist &lt;- function( ... ) {\n  list( ... )\n  }\n\nmetadat &lt;- addlist ( dataset = \"myeco\", date=\"Jan 20, 2023\")\nmetadat\n\n$dataset\n[1] \"myeco\"\n\n$date\n[1] \"Jan 20, 2023\"\n\ndat &lt;- addlist (ind=1:10, names=letters[1:10], eco=rnorm(10) )  \ndat\n\n$ind\n [1]  1  2  3  4  5  6  7  8  9 10\n\n$names\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\"\n\n$eco\n [1] -0.9572213 -0.3388834 -0.9815275  1.5341439 -0.3236124 -2.0562527\n [7] -0.5141652 -0.8511749 -0.3397510 -0.5002090\n\n\nThis may seem like a silly example (and it is), but it is kept simple so you can see what’s going on. These default arguments are very useful for making your functions flexible so that they can be more generic and reusable for many purposes.\n\n\nReturn value\nAs you have seen in the examples, R will return the last value computed (by default) or whatever you specify in the return() function. One thing that is a little peculiar to R is that you can only return one and only one object. So what do you do if you have several pieces of information you want returned? Use a list:\n\nmysq &lt;- function( x, yylab=\"Square of x\" ) {   # default argument for the y-label\n\n  plot( x, x*x, ylab=yylab)\n  output &lt;- list( input=x, output=x*x )   \n  return (output)     \n\n  }\n\nThis is in fact what many model-fitting packages do. They return a list with the inputs, any fitted parameters, and model fit statistics, as well as characteristics of the objects such as names and factor levels, etc.\n\n\nLooking inside R: functions that are inside packages\nIf you want to look at how any particular function in R is written – you can! R is open-source. You just type the name of the function with no parentheses:\n\nsummary\n\nfunction (object, ...) \nUseMethod(\"summary\")\n&lt;bytecode: 0x7fefc0814670&gt;\n&lt;environment: namespace:base&gt;\n\n\nYou can see that it is a generic function, and that it uses different methods depending on the class of the object.\nHere are all the methods that are defined for summary:\n\nmethods('summary')\n\n [1] summary.aov                         summary.aovlist*                   \n [3] summary.aspell*                     summary.check_packages_in_dir*     \n [5] summary.connection                  summary.data.frame                 \n [7] summary.Date                        summary.default                    \n [9] summary.ecdf*                       summary.factor                     \n[11] summary.glm                         summary.infl*                      \n[13] summary.lm                          summary.loess*                     \n[15] summary.manova                      summary.matrix                     \n[17] summary.mlm*                        summary.nls*                       \n[19] summary.packageStatus*              summary.POSIXct                    \n[21] summary.POSIXlt                     summary.ppr*                       \n[23] summary.prcomp*                     summary.princomp*                  \n[25] summary.proc_time                   summary.rlang_error*               \n[27] summary.rlang_message*              summary.rlang_trace*               \n[29] summary.rlang_warning*              summary.rlang:::list_of_conditions*\n[31] summary.srcfile                     summary.srcref                     \n[33] summary.stepfun                     summary.stl*                       \n[35] summary.table                       summary.tukeysmooth*               \n[37] summary.vctrs_sclr*                 summary.vctrs_vctr*                \n[39] summary.warnings                   \nsee '?methods' for accessing help and source code\n\n\nHere’s how we find out what’s inside summary.factor for example:\n\nsummary.factor\n\nfunction (object, maxsum = 100L, ...) \n{\n    nas &lt;- is.na(object)\n    ll &lt;- levels(object)\n    if (ana &lt;- any(nas)) \n        maxsum &lt;- maxsum - 1L\n    tbl &lt;- table(object)\n    tt &lt;- c(tbl)\n    names(tt) &lt;- dimnames(tbl)[[1L]]\n    if (length(ll) &gt; maxsum) {\n        drop &lt;- maxsum:length(ll)\n        o &lt;- sort.list(tt, decreasing = TRUE)\n        tt &lt;- c(tt[o[-drop]], `(Other)` = sum(tt[o[drop]]))\n    }\n    if (ana) \n        c(tt, `NA's` = sum(nas))\n    else tt\n}\n&lt;bytecode: 0x7fefba4bf1d0&gt;\n&lt;environment: namespace:base&gt;\n\n\nNote: Internal functions are hidden inside the namespace of a package – the programmer has chosen to not make it available to the global environment. To find these, use getAnywhere('functionname') ha!\n\n\nScope\nIt is important to know that when you write a function, everything that happens inside the function is local in scope. It’s like a big family secret –\n\neverything that is said in the family stays in the family.\n\nIf you try to go talking about it to the outside world, no one will know what you are talking about. For example, suppose you wrote a function with some internal variables like this:\n\nmyfunc &lt;- function( fattony, littlejimmy) {\n\n  canolis &lt;- fattony*2 + littlejimmy\n  return(canolis)\n}\n\nmyfunc( 5, 4 )\n\n[1] 14\n\n\nIf we try type the following on the command line, we will get an error ... object 'canolis' not found.\n\ncanolis\n\nEven though you ran the function, you can’t ask R how many canolis you need because what’s created in the function stays in the function. When the function is over, poof! It’s gone. That’s because the objects used within the function are local in scope and not available to the global environment.\nOf course, global variables are available to use inside of functions, just as family members are aware of what’s going on in the outside world. So for example, it is perfectly valid to use pi or anything you’ve defined previously in the global environment inside a function:\n\nmyfunc &lt;- function( fattony, littlejimmy) {\n\n  canolis &lt;- fattony*pi + littlejimmy+littlebit\n  return( round(canolis) )\n}\n\nlittlebit &lt;- 1\nmyfunc( 5, 4 )\n\n[1] 21\n\n\nThe code above worked because littlebit was defined prior to running our function. But you can see that it’s often a good idea to actually pass into a function anything that is needed to make it go.\nSo you may be wondering why it works this way? Well in general, in most advanced programming languages,\n\nthe objects within functions are local in scope. This is to make it easier to program.\n\nIf there is a clean separation between what goes on inside a function and what is outside of it, then you can write functions without worrying about every possibility regarding what could happen. You only have to worry about what is happening inside your little function. That’s what helps to make it modular and extensible – so your functions can play nice with other codes.\n\n\nSearch Paths and Environment\nRemember what we were saying about functions in R are objects? So if we look at our workspace, our functions should be there:\n\nls()\n\n[1] \"addlist\"   \"dat\"       \"littlebit\" \"metadat\"   \"myfun\"     \"myfunc\"   \n[7] \"mysq\"      \"query\"    \n\n\nAnd sure enough they are! As well as all of our data frames, lists, and other objects that we created. Now I should note that it is possible to write a function in R with the same name as a built-in function in R. For example, if for some crazy reason, we wanted to redefine the mean function, we can!\n\nmean &lt;- function(...) {\n  return (\"dirty harry\")\n  }\n  \nmean( 1:10 )\n\n[1] \"dirty harry\"\n\n\nWhat happened? Well we wrote our own function for mean. Why is R only returning our new function, an not the built-in one?\n\nWell, any object that we create (including our own functions) are in the Global Environment.\n\nWhereas functions in packages are in their further down the search path. R knows where things are by the order that they are attached. The global environment is first (containing any user-created objects), followed by attached packages:\n\nsearch()\n\n [1] \".GlobalEnv\"        \"tools:quarto\"      \"tools:quarto\"     \n [4] \"package:stats\"     \"package:graphics\"  \"package:grDevices\"\n [7] \"package:utils\"     \"package:datasets\"  \"package:methods\"  \n[10] \"Autoloads\"         \"package:base\"     \n\n\nThe function mean() is in the base package, which is all the way at the end. So when we type mean() R will first look to see if there is any function by that name in our global environment, then in any of the other attached packages before finally finding it in base. Needless to say, it’s very confusing (and potentially dangerous!) to name objects by the same name as R key words or built-in functions. Don’t do it!\nIf you need to get rid of the custom build mean function, just type rm(mean) at the console.\n\nrm(mean)\nmean(1:10)\n\n[1] 5.5\n\n\nWhew! Or just shut down and restart R. It’s a clean slate after that! (Don’t worry, you can’t break R ;).\n\n\nExercises\n\nWrite your own function for calculating a mean of a vector, using only the sum() and the length() functions. The input should be a vector, and the output is the mean.\nWrite your own function for calculating the standard error. You can use the sd(), sqrt(), and the length() functions. The input should be a vector of values.\nGo back to some of the class data Iʻve given you. Write a function that will read in the irradiance data, trim it to wavelengths between 300 and 750 nm, and plot the data. Then use that function to read in files for the different directions: up, for (forward), left, and right: 20070725\\_01upirr.txt, 20070725\\_01forirr.txt, 20070725\\_01leftirr.txt, 20070725\\_01rightirr.txt. Your function should take as input just the file name. Write a script that defines the function and then calls the function four times, once for each file.\nNow take the function you just made, and add optional arguments for the cut off values 300 and 750. You may want to trim the data to different values. Try trimming it to different values and see what happens using your new function."
  },
  {
    "objectID": "posts/2023-02-16-plotting-systems/index.html",
    "href": "posts/2023-02-16-plotting-systems/index.html",
    "title": "Plotting Systems",
    "section": "",
    "text": "The data may not contain the answer. And, if you torture the data long enough, it will tell you anything. —John W. Tukey\n#✏️"
  },
  {
    "objectID": "posts/2023-02-16-plotting-systems/index.html#the-base-plotting-system",
    "href": "posts/2023-02-16-plotting-systems/index.html#the-base-plotting-system",
    "title": "Plotting Systems",
    "section": "The Base Plotting System",
    "text": "The Base Plotting System\nThe base plotting system is the original plotting system for R. The basic model is sometimes referred to as the “artist’s palette” model, because you start with a blank canvas and add to it, element by element.\nWe begin with an R plotting function that creates a new plot window typically the plot() function, and we can add annotations to the plot with functions such as (text, lines, points, axis).\nIf you get an error\nError in plot.xy(xy.coords(x, y), type = type, ...) : \n  plot.new has not been called yet\nIt is probably because you tried to add an annotation before you created a plot first.\nThe base plotting system is often the most convenient plotting system to because it mirrors the cartesian coordinate (X,Y) syntax that we sometimes think of when building plots and analyzing data.\nIt is also the most customizablbe because there are many options that users can specify. See the help page for ?plot.default and ?par for graphical parameters that once can set. It does take some learning, but it is possible to make multi-figure plots while precisely controlling their placement, margins, and any annotations including points, lines, text etc.\nThe plot system is also very useful at the very beginning of a data analysis When we donʻt precisely know what we want to plot. We can start by “throwing some data on the page” and then slowly modifying or adding more information to it as our thought process evolves.\n\n\n\n\n\n\nExample\n\n\n\nWe might look at a simple scatterplot and then decide to add a linear regression line or a smoother to it to highlight the trends.\n\ndata(airquality)\nwith(airquality, {\n        plot(Temp, Ozone)\n        lines(loess.smooth(Temp, Ozone))\n})\n\n\n\n\nScatterplot with loess curve\n\n\n\n\n\n\nIn the code above:\n\nThe plot() function creates the initial plot and draws the points on the canvas.\nThe lines function is used to annotate or add to the plot (in this case it adds a loess smoother to the scatterplot).\nR has many other types of smoothing functions as well.\n\nNext, we use the plot() function to draw the points on the scatterplot and then use the main argument to add a main title to the plot.\n\ndata(airquality)\nwith(airquality, {\n        plot(Temp, Ozone, main = \"my plot\")\n        lines(loess.smooth(Temp, Ozone))\n})\n\n\n\n\nScatterplot with loess curve\n\n\n\n\nOne downside with constructing base plots is that annotations can only be added. If your annotations are crashing, or you run out of room, etc., you will have to rerun the code starting from plot().\nAnd while the base plotting system is nice in that it gives you the flexibility to specify these kinds of details to painstaking accuracy, sometimes it would be nice if the system could just figure it out for you. Thatʻs where lattice and ggplot2 have contributed.\nAnother downside of the customizability of the base plotting system is that it is difficult to describe or translate a plot to others because there is no clear graphical language or grammar. In other words, you cannot paint the format of one onto another plot, unless you design the code (maybe by using the same variable names, etc.) to do it yourself.\nThe only real way to describe what you have done in a base plot is to just list the series of commands/functions that you have executed, which is not a particularly compact way of communicating things. The ggplot2 package has developed a grammar of graphics that make transferring formats easy.\n\n\n\n\n\n\nExample\n\n\n\nAnother typical base plot for a linear regression is constructed with the following code. Here we are using formula representation. Instead of plot(x,y) we use plot(y~x) which is read “plot y as a function of x”. These styles are equivalent, but formula is in the style of regression models.\n\ndata(cars)\n\n## Create the plot / draw canvas\nwith(cars, plot(dist ~ speed))\n\n\n\n\nBase plot with title\n\n\n\n## Fit a linear model and save it\nlm.fit &lt;- with(cars, lm( dist ~ speed))\n\nWe can add annotations as before. This time, to add the regression line to the plot, we use the abline() function which adds straight lines, specified by slope and intercept.\n\nwith(cars, plot(dist ~ speed))\n\n## Add annotations\ntitle(\"Linear model of speed vs. stopping distance\")\nabline(lm.fit, col=\"red\")\ntext(7, 120, paste(\"slope =\", round(coef(lm.fit)[2], digits=2)))\ntext(7, 110, paste(\"intercept =\", round(coef(lm.fit)[1], digits=2)))\n\n\n\n\nUsing text() we added the slope and intercept from the linear model fit. That information is returned by the coef() function. We used round() to make it prettier (8 decimal places is not necessary!)"
  },
  {
    "objectID": "posts/2023-02-16-plotting-systems/index.html#the-lattice-system",
    "href": "posts/2023-02-16-plotting-systems/index.html#the-lattice-system",
    "title": "Plotting Systems",
    "section": "The Lattice System",
    "text": "The Lattice System\nThe lattice plotting system is ideal for visualization of multivariate data, and is implemented in the lattice R package which comes with every installation of R (although it is not loaded by default).\nTo use the lattice plotting functions, you must first load the lattice package.\n\nrequire(lattice)\n\nLoading required package: lattice\n\n\nWith the lattice system, plots are created with a single function call, such as xyplot() or bwplot(), and the plot is delivered in a predefined format.\nThere is no real distinction between functions that create or initiate plots and functions that annotate plots because it all happens at once.\nLattice plots tend to be most useful for conditioning types of plots, i.e. looking at how y changes with x across levels of z.\n\ne.g. these types of plots are useful for looking at multi-dimensional data and often allow you to squeeze a lot of information into a single window or page.\n\nAnother aspect of lattice that makes it different from base plotting is that things like margins and spacing are set automatically. The downside is that it is not very customizable.\n\n\n\n\n\n\nExample\n\n\n\nHere is a lattice plot that looks at the relationship between life expectancy and income and how that relationship varies by region in the United States.\n\nstate &lt;- data.frame(state.x77, region = state.region)\nhead(state)\n\n           Population Income Illiteracy Life.Exp Murder HS.Grad Frost   Area\nAlabama          3615   3624        2.1    69.05   15.1    41.3    20  50708\nAlaska            365   6315        1.5    69.31   11.3    66.7   152 566432\nArizona          2212   4530        1.8    70.55    7.8    58.1    15 113417\nArkansas         2110   3378        1.9    70.66   10.1    39.9    65  51945\nCalifornia      21198   5114        1.1    71.71   10.3    62.6    20 156361\nColorado         2541   4884        0.7    72.06    6.8    63.9   166 103766\n           region\nAlabama     South\nAlaska       West\nArizona      West\nArkansas    South\nCalifornia   West\nColorado     West\n\nxyplot(Life.Exp ~ Income | region, data = state, layout = c(4, 1))\n\n\n\n\nLattice plot\n\n\n\n\n\n\nYou can see that the entire plot was generated by the call to xyplot() and all of the data for the plot were specified to come from the state data frame.\nThe layout specifies four panels —one for each region— within each panel is a scatterplot of life expectancy and income. Note that in this case, layout is columns, rows. Try changing the numbers or leaving out the layout argument and see what happens.\nThe notion of panels comes up a lot with lattice plots because you typically have many panels in a lattice plot (each panel typically represents a factor level, like “region”).\n\n\n\n\n\n\nNote\n\n\n\nDownsides with the lattice system\n\nIt can sometimes be very awkward to specify an entire plot in a single function call (you end up with functions with many many arguments).\nAnnotation in panels in plots is not especially intuitive and can be difficult to explain. In particular, the use of custom panel functions and subscripts can be difficult to wield and requires a lot of trial and error."
  },
  {
    "objectID": "posts/2023-02-16-plotting-systems/index.html#the-ggplot2-system",
    "href": "posts/2023-02-16-plotting-systems/index.html#the-ggplot2-system",
    "title": "Plotting Systems",
    "section": "The ggplot2 System",
    "text": "The ggplot2 System\nThe ggplot2 plotting system attempts to split the difference between base and lattice in a number of ways.\n\n\n\n\n\n\nNote\n\n\n\nTaking cues from lattice, the ggplot2 system automatically deals with spacings, text, titles but also allows you to annotate by “adding” to a plot, with annotations added in layers.\n\n\nThe ggplot2 system is implemented in the ggplot2 package (part of the tidyverse package), which is available from CRAN (it does not come with R).\nYou can install it from CRAN via\n\ninstall.packages(\"ggplot2\")\n\nand then load it into R.\n\nrequire(ggplot2)\n\nLoading required package: ggplot2\n\n\nSuperficially, the ggplot2 functions are similar to lattice, but the system is generally easier and more intuitive to use once you learn the syntax.\nThe defaults used in ggplot2 make many choices for you, but you can still customize plots.\n\n\n\n\n\n\nExample\n\n\n\nA typical plot with the ggplot2 package looks like the code below.\nNote the use of the pipe operator %&gt;% from the magrittr package in the tidyverse, which sends the dataframe mpg as input into the function ggplot. People love the pipe operator because you can just pass output along from one function to the next (as long as the function is written for piping). All of the tidyverse allows piping, and some base R functions do as well.\n\nlibrary(tidyverse)\ndata(mpg)\nmpg %&gt;%\n  ggplot(aes(displ, hwy)) + \n  geom_point()\n\n\n\n\nggplot2 plot\n\n\n\n\n\n\nIn ggplot, elements of the plot are specified as aesthetics, and layers can be added onto the plot with + anotherfunction(). Try running the first part, and then running the whole thing together. Then try adding + geom_smooth()\nThere are additional functions in ggplot2 that allow you to make arbitrarily sophisticated plots.\nWe will discuss more about this in the next lecture."
  },
  {
    "objectID": "posts/2023-02-14-tidying-exploring-data/index.html",
    "href": "posts/2023-02-14-tidying-exploring-data/index.html",
    "title": "Tidying and Exploring Data",
    "section": "",
    "text": "✏️"
  },
  {
    "objectID": "posts/2023-02-14-tidying-exploring-data/index.html#acknowledgements",
    "href": "posts/2023-02-14-tidying-exploring-data/index.html#acknowledgements",
    "title": "Tidying and Exploring Data",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nMaterial for this lecture was borrowed and adopted from"
  },
  {
    "objectID": "posts/2023-02-14-tidying-exploring-data/index.html#vectors",
    "href": "posts/2023-02-14-tidying-exploring-data/index.html#vectors",
    "title": "Tidying and Exploring Data",
    "section": "Vectors",
    "text": "Vectors\nThe index of a vector is it’s number in the array. Each and every element in any data object has at least one index (if vector, it is one dimensional so it is its position along the vector, if a matrix or data frame, which are two-dimensional, it’s the row and column number, etc.)\nLet’s create a vector:\n\nxx &lt;- c(1, 5, 2, 3, 5)\nxx\n\n[1] 1 5 2 3 5\n\n\nAccess specific values of xx by number:\n\nxx[1]\n\n[1] 1\n\nxx[3]\n\n[1] 2\n\n\nYou can use a function to generate an index. Get the last element (without knowing how many there are) by:\n\nxx[length(xx)]\n\n[1] 5\n\n\nRetrieve multiple elements of xx by using a vector as an argument:\n\nxx[c(1, 3, 4)]\n\n[1] 1 2 3\n\nxx[1:3]\n\n[1] 1 5 2\n\nxx[c(1, length(xx))]  # first and last\n\n[1] 1 5\n\n\nExclude elements by using a negative index:\n\nxx\n\n[1] 1 5 2 3 5\n\nxx[-1]  # exclude first\n\n[1] 5 2 3 5\n\nxx[-2] # exclude second\n\n[1] 1 2 3 5\n\nxx[-(1:3)] # exclude first through third\n\n[1] 3 5\n\nxx[-c(2, 4)] # exclude second and fourth, etc. \n\n[1] 1 2 5\n\n\nUse a logical vector:\n\nxx[ c( T, F, T, F, T) ]  # T is the same as TRUE\n\n[1] 1 2 5\n\n\n\nxx &gt; 2\n\n[1] FALSE  TRUE FALSE  TRUE  TRUE\n\nxx[ xx &gt; 2 ]\n\n[1] 5 3 5\n\nxx &gt; 2 & xx &lt; 5\n\n[1] FALSE FALSE FALSE  TRUE FALSE\n\nxx[ xx&gt;2 & xx&lt;5]\n\n[1] 3\n\n\nSubsetting (picking particular observations out of an R object) is something that you will have to do all the time. It’s worth the time to understand it clearly.\n\nsubset_xx &lt;-  xx[ xx &gt; 2 ]\nsubset_xx2 &lt;- subset(xx, xx&gt;2)  # using subset function\nsubset_xx == subset_xx2   # check if the same\n\n[1] TRUE TRUE TRUE\n\n\nThe subset function is just another way of subsetting by index, just in function form with arguments. It can be more clear to use for dataframes, but it is really a matter of personal preference as you develop your style. Whichever way you go, it is important to be aware of the different ways to achieve the same goals."
  },
  {
    "objectID": "posts/2023-02-14-tidying-exploring-data/index.html#matrices-and-dataframes",
    "href": "posts/2023-02-14-tidying-exploring-data/index.html#matrices-and-dataframes",
    "title": "Tidying and Exploring Data",
    "section": "Matrices and Dataframes",
    "text": "Matrices and Dataframes\nMatrices and dataframes are both rectangular having two dimensions, and are handled very similarly for indexing and subsetting.\nLet’s work with a dataframe that is provided with the geiger package called geospiza. It is a list with a tree and a dataframe. The dataframe contains five morphological measurements for 13 species. First, let’s clear the workspace (or clear and start a new R session):\n\ninstall.packages(\"geiger\")  # if you need to install geiger\n\nGet the built-in dataset this way:\n\nrm(list=ls())\nrequire(geiger)\n\nLoading required package: geiger\n\n\nLoading required package: ape\n\ndata(geospiza)   # load the dataset into the workspace\nls()               # list the objects in the workspace\n\n[1] \"geospiza\"\n\n\nLet’s find out some basic information about this object:\n\nclass(geospiza)\n\n[1] \"list\"\n\nattributes(geospiza)\n\n$names\n[1] \"geospiza.tree\" \"geospiza.data\" \"phy\"           \"dat\"          \n\nstr(geospiza)\n\nList of 4\n $ geospiza.tree:List of 4\n  ..$ edge       : num [1:26, 1:2] 15 16 17 18 19 20 21 22 23 24 ...\n  ..$ edge.length: num [1:26] 0.2974 0.0492 0.0686 0.134 0.1035 ...\n  ..$ Nnode      : int 13\n  ..$ tip.label  : chr [1:14] \"fuliginosa\" \"fortis\" \"magnirostris\" \"conirostris\" ...\n  ..- attr(*, \"class\")= chr \"phylo\"\n $ geospiza.data: num [1:13, 1:5] 4.4 4.35 4.22 4.26 4.24 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:13] \"magnirostris\" \"conirostris\" \"difficilis\" \"scandens\" ...\n  .. ..$ : chr [1:5] \"wingL\" \"tarsusL\" \"culmenL\" \"beakD\" ...\n $ phy          :List of 4\n  ..$ edge       : num [1:26, 1:2] 15 16 17 18 19 20 21 22 23 24 ...\n  ..$ edge.length: num [1:26] 0.2974 0.0492 0.0686 0.134 0.1035 ...\n  ..$ Nnode      : int 13\n  ..$ tip.label  : chr [1:14] \"fuliginosa\" \"fortis\" \"magnirostris\" \"conirostris\" ...\n  ..- attr(*, \"class\")= chr \"phylo\"\n $ dat          : num [1:13, 1:5] 4.4 4.35 4.22 4.26 4.24 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:13] \"magnirostris\" \"conirostris\" \"difficilis\" \"scandens\" ...\n  .. ..$ : chr [1:5] \"wingL\" \"tarsusL\" \"culmenL\" \"beakD\" ...\n\n\nIt is a list with four elements. Here we want the data\n\ngeo &lt;- as.data.frame(geospiza$geospiza.data)\ndim(geo)\n\n[1] 13  5\n\n\nIt is a dataframe with 13 rows and 5 columns. If we want to know all the attributes of geo:\n\nattributes(geo)\n\n$names\n[1] \"wingL\"   \"tarsusL\" \"culmenL\" \"beakD\"   \"gonysW\" \n\n$class\n[1] \"data.frame\"\n\n$row.names\n [1] \"magnirostris\" \"conirostris\"  \"difficilis\"   \"scandens\"     \"fortis\"      \n [6] \"fuliginosa\"   \"pallida\"      \"fusca\"        \"parvulus\"     \"pauper\"      \n[11] \"Pinaroloxias\" \"Platyspiza\"   \"psittacula\"  \n\n\nWe see that it has a “names” attribute, which refers to column names in a dataframe. Typically, the columns of a dataframe are the variables in the dataset. It also has “rownames” which contains the species names (so it does not have a separate column for species names).\nDataframes have two dimensions which we can use to index with: dataframe[row, column].\n\ngeo     # the entire object, same as geo[] or geo[,]\ngeo[c(1, 3), ]   # select the first and third rows, all columns\ngeo[, 3:5]   # all rows, third through fifth columns\ngeo[1, 5]  # first row, fifth column (a single number)\ngeo[1:2, c(3, 1)]  # first and second row, third and first column (2x2 matrix)\ngeo[-c(1:3, 10:13), ]  # everything but the first three and last three rows\ngeo[ 1:3, 5:1]  # first three species, but variables in reverse order\n\nTo prove to ourselves that we can access matrices in the same way, let’s coerce geo to be a matrix:\n\ngeom &lt;- as.matrix( geo ) \nclass(geom)\n\n[1] \"matrix\" \"array\" \n\nclass(geo)\n\n[1] \"data.frame\"\n\ngeo[1,5]  # try a few more from the choices above to test\n\n[1] 2.675983\n\n\nSince geo and geom have row and column names, we can access by name (show that this works for geom too):\n\ngeo[\"pauper\", \"wingL\"]  # row pauper, column wingL\n\n[1] 4.2325\n\ngeo[\"pauper\", ]  # row pauper, all columns \n\n        wingL tarsusL culmenL  beakD gonysW\npauper 4.2325  3.0359   2.187 2.0734 1.9621\n\n\nWe can also use the names (or rownames) attribute if we are lazy. Suppose we wanted all the species which began with “pa”. we could find which position they hold in the dataframe by looking at the rownames, saving them to a vector, and then indexing by them:\n\nsp &lt;- rownames(geo)\nsp                            # a vector of the species names\n\n [1] \"magnirostris\" \"conirostris\"  \"difficilis\"   \"scandens\"     \"fortis\"      \n [6] \"fuliginosa\"   \"pallida\"      \"fusca\"        \"parvulus\"     \"pauper\"      \n[11] \"Pinaroloxias\" \"Platyspiza\"   \"psittacula\"  \n\nsp[c(7,8,10)]     # the ones we want are #7,8, and 10\n\n[1] \"pallida\" \"fusca\"   \"pauper\" \n\ngeo[ sp[c(7,8,10)], ]  # rows 7,8 and 10, same as geo[c(7, 8, 10)]\n\n           wingL  tarsusL  culmenL    beakD   gonysW\npallida 4.265425 3.089450 2.430250 2.016350 1.949125\nfusca   3.975393 2.936536 2.051843 1.191264 1.401186\npauper  4.232500 3.035900 2.187000 2.073400 1.962100\n\n\nOne difference between dataframes and matrices is that Indexing a data frame by a single vector (meaning, no comma separating) selects an entire column. This can be done by name or by number:\n\ngeo[3]   # third column\ngeo[\"culmenL\"]  # same\ngeo[c(3,5)]  # third and fifth column\ngeo[c(\"culmenL\", \"gonysW\")]  # same\n\nProve to yourself that selecting by a single index has a different behavior for matrices (and sometimes produces an error.\n\n\n\n\n\n\nWhy?\n\n\n\n\nBecause internally, a dataframe is actually a list of vectors. Thus a single name or number refers to the column, rather than a coordinate in a cartesian-coordinate-like system.\nHowever, a matrix is actually a vector with breaks in it. So a single number refers to a position along the single vector.\nA single name could work, but only if the individual elements of the matrix have names (like naming the individual elements of a vector).\n\n\n\nAnother difference is that dataframes (and lists below) can be accessed by the $ operator. It means indicates a column within a dataframe, so dataframe$column. This is another way to select by column:\n\ngeo$culmenL\n\n [1] 2.724667 2.654400 2.277183 2.621789 2.407025 2.094971 2.430250 2.051843\n [9] 1.974420 2.187000 2.311100 2.331471 2.259640\n\n\nAn equivalent way to index is by using the subset function. Some people prefer it because you have explicit parameters for what to select and which variables to include. See help page ?subset."
  },
  {
    "objectID": "posts/2023-02-14-tidying-exploring-data/index.html#lists",
    "href": "posts/2023-02-14-tidying-exploring-data/index.html#lists",
    "title": "Tidying and Exploring Data",
    "section": "Lists",
    "text": "Lists\nA list is a vector, except that whereas an ordinary vector has the same type of data (numeric, character, factor) in each slot, a list can have different types in different slots. They are sort of like expandable containers, flexibly accommodating any group of objects that the user wants to keep together.\nThey are accessed by numeric index or by name (if they are named), but they are accessed by double square brackets. Also, you can’t access multiple elements of lists by using vectors of indices:\n\nmylist &lt;- list( vec = 2*1:10, mat = matrix(1:10, nrow=2), cvec = c(\"frogs\", \"birds\"))\nmylist\n\n$vec\n [1]  2  4  6  8 10 12 14 16 18 20\n\n$mat\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    3    5    7    9\n[2,]    2    4    6    8   10\n\n$cvec\n[1] \"frogs\" \"birds\"\n\nmylist[[2]]\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    3    5    7    9\n[2,]    2    4    6    8   10\n\nmylist[[\"vec\"]]\n\n [1]  2  4  6  8 10 12 14 16 18 20\n\n# mylist[[1:3]]  # gives an error if you uncomment it\nmylist$cvec\n\n[1] \"frogs\" \"birds\""
  },
  {
    "objectID": "posts/2023-02-14-tidying-exploring-data/index.html#missing-values",
    "href": "posts/2023-02-14-tidying-exploring-data/index.html#missing-values",
    "title": "Tidying and Exploring Data",
    "section": "Missing Values",
    "text": "Missing Values\nMissing values compared to anything else will return a missing value (so NA == NA returns NA, which is usually not what you want). You must test it with is.na function. You can also test multiple conditions with and (&) and or (|)\n\n!is.na(geo$gonysW) \n\n [1]  TRUE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE  TRUE\n[13]  TRUE\n\ngeo[!is.na(geo$gonysW) & geo$wingL &gt; 4, ]  # element by element \"and\"\n\n                wingL  tarsusL  culmenL    beakD   gonysW\nconirostris  4.349867 2.984200 2.654400 2.513800 2.360167\nfortis       4.244008 2.894717 2.407025 2.362658 2.221867\nmagnirostris 4.404200 3.038950 2.724667 2.823767 2.675983\npsittacula   4.235020 3.049120 2.259640 2.230040 2.073940\nplatyspiza   4.419686 3.270543 2.331471 2.347471 2.282443\nscandens     4.261222 2.929033 2.621789 2.144700 2.036944\n\ngeo[!is.na(geo$gonysW) | geo$wingL &gt; 4, ]   # element by element \"or\"\n\n                wingL  tarsusL  culmenL    beakD   gonysW\nconirostris  4.349867 2.984200 2.654400 2.513800 2.360167\ndifficilis   4.224067 2.898917 2.277183 2.011100       NA\nfuliginosa   4.132957 2.806514 2.094971       NA       NA\nfortis       4.244008 2.894717 2.407025 2.362658 2.221867\nmagnirostris 4.404200 3.038950 2.724667 2.823767 2.675983\nparvulus     4.131600 2.973060       NA       NA       NA\npinaroloxias 4.188600 2.980200 2.311100       NA       NA\npauper       4.232500 3.035900 2.187000 2.073400       NA\npsittacula   4.235020 3.049120 2.259640 2.230040 2.073940\npallida      4.265425 3.089450 2.430250 2.016350       NA\nplatyspiza   4.419686 3.270543 2.331471 2.347471 2.282443\nscandens     4.261222 2.929033 2.621789 2.144700 2.036944\n\n!is.na(geo$gonysW) && geo$wingL &gt; 4   # vectorwise \"and\"\n\nWarning in !is.na(geo$gonysW) && geo$wingL &gt; 4: 'length(x) = 13 &gt; 1' in coercion\nto 'logical(1)'\n\nWarning in !is.na(geo$gonysW) && geo$wingL &gt; 4: 'length(x) = 13 &gt; 1' in coercion\nto 'logical(1)'\n\n\n[1] TRUE\n\n\nMatching works on strings also:\n\ngeo[rownames(geo) == \"pauper\",]   # same as   geo[\"pauper\", ]\ngeo[rownames(geo) &lt; \"pauper\",]\n\nThere are even better functions for strings, though. In the expression A %in% B, the %in% operator compares two vectors of strings, and tells us which elements of A are present in B.\n\nnewsp &lt;- c(\"clarkii\", \"pauper\", \"garmani\")\nnewsp[newsp  %in% rownames(geo)]     # which new species are in geo?\n\nWe can define the “without” operator:\n\n\"%w/o%\" &lt;- function(x, y) x[!x %in% y]\nnewsp  %w/o% rownames(geo)   # which new species are not in geo?"
  },
  {
    "objectID": "posts/2023-02-14-tidying-exploring-data/index.html#plot",
    "href": "posts/2023-02-14-tidying-exploring-data/index.html#plot",
    "title": "Tidying and Exploring Data",
    "section": "Plot()",
    "text": "Plot()\nFor a generic x-y plot use plot(). It will also start a graphical device.\nHere we are using the with() function to specify which dataframe to look into for our named variables. We could instead do penguins$body_mass_g, etc.\n\nwith(penguins, plot( body_mass_g, bill_length_mm))\n\n\n\n\nTo add points to an existing plot, use points()\n\nwith(penguins, plot( body_mass_g, bill_length_mm))\nwith(penguins[penguins$species==\"Adelie\",], points( body_mass_g, bill_length_mm, col=\"red\"))\n\n\n\n\nOne could fit linear models, for example, and use lines() to overlay the line on the plot."
  },
  {
    "objectID": "posts/2023-02-14-tidying-exploring-data/index.html#distribution-using-hist-and-density",
    "href": "posts/2023-02-14-tidying-exploring-data/index.html#distribution-using-hist-and-density",
    "title": "Tidying and Exploring Data",
    "section": "Distribution using hist() and density()",
    "text": "Distribution using hist() and density()\nTo see a histogram, use hist(). You can change the breaks and many other features by checking out the help page ?hist\n\nwith(penguins, hist( body_mass_g ))\n\n\n\n\nTo see a density plot use density() to create the density, then plot it.\n\ndens &lt;- with(penguins, density( body_mass_g, na.rm=T ))\nplot(dens)"
  },
  {
    "objectID": "posts/2023-02-21-ggplot2/index.html",
    "href": "posts/2023-02-21-ggplot2/index.html",
    "title": "The ggplot2 package",
    "section": "",
    "text": "For more details see\n\n\n\n\nWonderful Window shopping in the R graph gallery (with code): https://r-graph-gallery.com\nThe “grammar of graphics” explained in Hadley Wickamʻs article: http://vita.had.co.nz/papers/layered-grammar.pdf\nVery gentle intro for beginners: https://posit.cloud/learn/primers/3\nHadley Wickamʻs overview: https://r4ds.had.co.nz/data-visualisation\nCedric Schererʻs Step-by-step tutorial: https://www.cedricscherer.com/2019/08/05/a-ggplot2-tutorial-for-beautiful-plotting-in-r/\nFor in-depth reading, Hadley Wickamʻs ggplot2 book: https://ggplot2-book.org"
  },
  {
    "objectID": "posts/2023-02-21-ggplot2/index.html#basic-components-of-a-ggplot2-plot",
    "href": "posts/2023-02-21-ggplot2/index.html#basic-components-of-a-ggplot2-plot",
    "title": "The ggplot2 package",
    "section": "Basic components of a ggplot2 plot",
    "text": "Basic components of a ggplot2 plot\n\n\n\n\n\n\nKey components\n\n\n\nA ggplot2 plot consists of a number of key components.\n\nData: In the form of a dataframe or tibble, containing all of the data that will be displayed on the plot.\nGeometry: The geometry or geoms define the style of the plot such as scatterplot, barplot, histogram, violin plots, smooth densities, qqplot, boxplot, and others.\nAesthetic mapping: Aesthetic mappings describe how data are mapped to color, size, shape, location, or to legend elements. How we define the mapping depends on what geometry we are using.\n\nNearly all plots drawn with ggplot2 will have the above compoents. In addition you may want to have specify additional elements:\n\nFacets: When used, facets describe how panel plots based on partions of the data should be drawn.\nStatistical Transformations: Or stats are transformations of the data such as log-transformation, binning, quantiles, smoothing.\nScales: Scales are used to indicate which factors are associated with the levels of the aesthetic mapping. Use manual scales to specify each level.\nCoordinate System: ggplot2 will use a default coordinate system drawn from the data, but you can customize the coordinate system in which the locations of the geoms will be drawn\n\n\n\nPlots are built up in layers, with the typical ordering being\n\nPlot the data\nOverlay a summary that reveals the relationship\nAdd metadata and annotation"
  },
  {
    "objectID": "posts/2023-02-21-ggplot2/index.html#legend",
    "href": "posts/2023-02-21-ggplot2/index.html#legend",
    "title": "The ggplot2 package",
    "section": "Legend",
    "text": "Legend\nFor example, we can make changes to the legend title via the scale_color_discrete function():\n\np + geom_point(aes(col=Species), size = 3) + \n  scale_color_discrete(name = \"Iris Varieties\")"
  },
  {
    "objectID": "posts/2023-02-21-ggplot2/index.html#no-legend",
    "href": "posts/2023-02-21-ggplot2/index.html#no-legend",
    "title": "The ggplot2 package",
    "section": "No legend",
    "text": "No legend\nggplot2 automatically adds a legend that maps color to species. To remove the legend we set the geom_point() argument show.legend = FALSE.\n\np + geom_point(aes(col=Species), size = 3, show.legend=FALSE)"
  },
  {
    "objectID": "posts/2023-02-21-ggplot2/index.html#themes",
    "href": "posts/2023-02-21-ggplot2/index.html#themes",
    "title": "The ggplot2 package",
    "section": "Themes",
    "text": "Themes\nThe default theme for ggplot2 uses the gray background with white grid lines.\nIf you don’t like this, you can use the black and white theme by using the theme_bw() function.\nThe theme_bw() function also allows you to set the typeface for the plot, in case you don’t want the default Helvetica. Here we change the typeface to Times.\n\n\n\n\n\n\nNote\n\n\n\nFor things that only make sense globally, use theme(), i.e. theme(legend.position = \"none\"). Two standard appearance themes are included\n\ntheme_gray(): The default theme (gray background)\ntheme_bw(): More stark/plain\n\n\n\n\np + \n  geom_point(aes(color = Species)) + \n  theme_bw(base_family = \"Times\")\n\n\n\n\nModifying the theme for a plot"
  },
  {
    "objectID": "posts/2023-02-21-ggplot2/index.html#ggthemes",
    "href": "posts/2023-02-21-ggplot2/index.html#ggthemes",
    "title": "The ggplot2 package",
    "section": "ggthemes",
    "text": "ggthemes\nThe style of a ggplot2 graph can be changed using the theme functions. Several themes are included as part of the ggplot2 package.\nMany other themes are added by the package ggthemes. Among those are the theme_economist theme that we used. After installing the package, you can change the style by adding a layer like this:\n\nrequire(ggthemes)\n\nLoading required package: ggthemes\n\n\n\nAttaching package: 'ggthemes'\n\n\nThe following object is masked from 'package:cowplot':\n\n    theme_map\n\np + geom_point(aes(col=Species, size = 3, alpha = 1/2)) +\n  scale_color_manual(values=cols) + \n  theme_economist()\n\n\n\n\nYou can see how some of the other themes look by simply changing the function. For instance, you might try the theme_fivethirtyeight() theme instead.\nggrepel\nThe final difference has to do with the position of the labels. In our plot, some of the labels fall on top of each other. The package ggrepel includes a geometry that adds labels while ensuring that they don’t fall on top of each other. We simply change geom_text with geom_text_repel.\n\nrequire(ggrepel)\n\nLoading required package: ggrepel\n\np + geom_point(aes(col=Species, size = 3, alpha = 1/2)) +\n  scale_color_manual(values=cols) + \n  theme_economist() +\n  geom_text_repel(aes(Petal.Length, Petal.Width, label = id))\n\nWarning: ggrepel: 90 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps"
  },
  {
    "objectID": "posts/2023-03-09-reshaping-data/index.html",
    "href": "posts/2023-03-09-reshaping-data/index.html",
    "title": "Reshaping data with dplyr",
    "section": "",
    "text": "Pre-lecture materials\n\nRead ahead\n\n\n\n\n\n\nRead ahead\n\n\n\nBefore class, you can prepare by reading the following materials:\n\nhttps://r4ds.had.co.nz/tidy-data\ntidyr cheat sheet from RStudio\n\n\n\n\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\n\nhttps://www.stephaniehicks.com/jhustatcomputing2022/posts/2022-09-08-tidy-data-and-the-tidyverse/\n\n\n\n\nLearning objectives\n\n\n\n\n\n\nLearning objectives\n\n\n\nAt the end of this lesson you will:\n\nBe able to transform wide data into long data\nBe able to separate character columns into multiple columns\nBe able to unite/separate multiple character columns into one column\n\n\n\n\n\nOverview\nCommon data table reshaping tasks include reshaping your matrices or dataframes manually, or converting between short-wide to tall-thin formats.\n\n\nManual reshaping\nRecall from our discussion on data objects that internally, R objects are stored as one huge vector. The various shapes of objects are simply created by R knowing where to break the vector into rows and columns. So it is very easy to reshape matrices:\n\nvv &lt;- 1:10  # a vector\nmm &lt;- matrix( vv, nrow=2)  # a matrix\nmm\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    3    5    7    9\n[2,]    2    4    6    8   10\n\ndim(mm) &lt;- NULL\nmm &lt;- matrix( vv, nrow=2, byrow=T)  # a matrix, but cells are now filled by row\nmm\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    2    3    4    5\n[2,]    6    7    8    9   10\n\ndim(mm) &lt;- NULL\nmm  # vector is now n a different order because the collapse occurred by column\n\n [1]  1  6  2  7  3  8  4  9  5 10\n\n\nLoad the geospiza data:\n\nrequire(geiger)\n\nLoading required package: geiger\n\n\nLoading required package: ape\n\ndata(geospiza)   # load the dataset into the workspace\ngeo &lt;- geospiza$dat  # save the morphometric data as geo\n\nOther means of “collapsing” dataframes are:\n\nunlist(geo)   # produces a vector from the dataframe\n\n                wingL  tarsusL  culmenL    beakD   gonysW\nmagnirostris 4.404200 3.038950 2.724667 2.823767 2.675983\nconirostris  4.349867 2.984200 2.654400 2.513800 2.360167\ndifficilis   4.224067 2.898917 2.277183 2.011100 1.929983\nscandens     4.261222 2.929033 2.621789 2.144700 2.036944\nfortis       4.244008 2.894717 2.407025 2.362658 2.221867\nfuliginosa   4.132957 2.806514 2.094971 1.941157 1.845379\npallida      4.265425 3.089450 2.430250 2.016350 1.949125\nfusca        3.975393 2.936536 2.051843 1.191264 1.401186\nparvulus     4.131600 2.973060 1.974420 1.873540 1.813340\npauper       4.232500 3.035900 2.187000 2.073400 1.962100\nPinaroloxias 4.188600 2.980200 2.311100 1.547500 1.630100\nPlatyspiza   4.419686 3.270543 2.331471 2.347471 2.282443\npsittacula   4.235020 3.049120 2.259640 2.230040 2.073940\n\n            # the atomic type of a dataframe is a list\nunclass(geo)  # removes the class attribute, turning the dataframe into a \n\n                wingL  tarsusL  culmenL    beakD   gonysW\nmagnirostris 4.404200 3.038950 2.724667 2.823767 2.675983\nconirostris  4.349867 2.984200 2.654400 2.513800 2.360167\ndifficilis   4.224067 2.898917 2.277183 2.011100 1.929983\nscandens     4.261222 2.929033 2.621789 2.144700 2.036944\nfortis       4.244008 2.894717 2.407025 2.362658 2.221867\nfuliginosa   4.132957 2.806514 2.094971 1.941157 1.845379\npallida      4.265425 3.089450 2.430250 2.016350 1.949125\nfusca        3.975393 2.936536 2.051843 1.191264 1.401186\nparvulus     4.131600 2.973060 1.974420 1.873540 1.813340\npauper       4.232500 3.035900 2.187000 2.073400 1.962100\nPinaroloxias 4.188600 2.980200 2.311100 1.547500 1.630100\nPlatyspiza   4.419686 3.270543 2.331471 2.347471 2.282443\npsittacula   4.235020 3.049120 2.259640 2.230040 2.073940\n\n            # series of vectors  plus any names attributes, same as setting \n            # class(geo) &lt;- NULL\nc(geo)  # similar to unclass but without the attributes            \n\n [1] 4.404200 4.349867 4.224067 4.261222 4.244008 4.132957 4.265425 3.975393\n [9] 4.131600 4.232500 4.188600 4.419686 4.235020 3.038950 2.984200 2.898917\n[17] 2.929033 2.894717 2.806514 3.089450 2.936536 2.973060 3.035900 2.980200\n[25] 3.270543 3.049120 2.724667 2.654400 2.277183 2.621789 2.407025 2.094971\n[33] 2.430250 2.051843 1.974420 2.187000 2.311100 2.331471 2.259640 2.823767\n[41] 2.513800 2.011100 2.144700 2.362658 1.941157 2.016350 1.191264 1.873540\n[49] 2.073400 1.547500 2.347471 2.230040 2.675983 2.360167 1.929983 2.036944\n[57] 2.221867 1.845379 1.949125 1.401186 1.813340 1.962100 1.630100 2.282443\n[65] 2.073940\n\n\n\n\nAn example of “untidy” data\nPeople often make tables in short-wide format that end up not being tidy data. When people use column names to store data, it is no longer tidy. For example take a look at this built-in dataset that comes with tidyr on religion and income survey data with the number of respondents with income range in column name.\n\nlibrary(tidyr)\nrelig_income\n\n# A tibble: 18 × 11\n   religion      `&lt;$10k` $10-2…¹ $20-3…² $30-4…³ $40-5…⁴ $50-7…⁵ $75-1…⁶ $100-…⁷\n   &lt;chr&gt;           &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 Agnostic           27      34      60      81      76     137     122     109\n 2 Atheist            12      27      37      52      35      70      73      59\n 3 Buddhist           27      21      30      34      33      58      62      39\n 4 Catholic          418     617     732     670     638    1116     949     792\n 5 Don’t know/r…      15      14      15      11      10      35      21      17\n 6 Evangelical …     575     869    1064     982     881    1486     949     723\n 7 Hindu               1       9       7       9      11      34      47      48\n 8 Historically…     228     244     236     238     197     223     131      81\n 9 Jehovah's Wi…      20      27      24      24      21      30      15      11\n10 Jewish             19      19      25      25      30      95      69      87\n11 Mainline Prot     289     495     619     655     651    1107     939     753\n12 Mormon             29      40      48      51      56     112      85      49\n13 Muslim              6       7       9      10       9      23      16       8\n14 Orthodox           13      17      23      32      32      47      38      42\n15 Other Christ…       9       7      11      13      13      14      18      14\n16 Other Faiths       20      33      40      46      49      63      46      40\n17 Other World …       5       2       3       4       2       7       3       4\n18 Unaffiliated      217     299     374     365     341     528     407     321\n# … with 2 more variables: `&gt;150k` &lt;dbl&gt;, `Don't know/refused` &lt;dbl&gt;, and\n#   abbreviated variable names ¹​`$10-20k`, ²​`$20-30k`, ³​`$30-40k`, ⁴​`$40-50k`,\n#   ⁵​`$50-75k`, ⁶​`$75-100k`, ⁷​`$100-150k`\n\n\nTake a second to look at this data. In this case the variables are religion (a proper vector), income bracket (in the column names), and the number of respts, which is the third variable, is presented inside the table. onden Converting this data to tidy format would give us\n\nlibrary(tidyverse)\n\nrelig_income %&gt;%\n  pivot_longer(-religion, names_to = \"income\", values_to = \"respondents\") \n\n# A tibble: 180 × 3\n   religion income             respondents\n   &lt;chr&gt;    &lt;chr&gt;                    &lt;dbl&gt;\n 1 Agnostic &lt;$10k                       27\n 2 Agnostic $10-20k                     34\n 3 Agnostic $20-30k                     60\n 4 Agnostic $30-40k                     81\n 5 Agnostic $40-50k                     76\n 6 Agnostic $50-75k                    137\n 7 Agnostic $75-100k                   122\n 8 Agnostic $100-150k                  109\n 9 Agnostic &gt;150k                       84\n10 Agnostic Don't know/refused          96\n# … with 170 more rows\n\n\nNow we have each variable along the columns and each row corresponds to one observation (or category, here a combination of religion and income bracket).\n\n\nReshaping data with dplyr\n\npivot_longer()\nThe tidyr package includes functions to transfer a data frame between long and wide.\n\nWide format data has different attributes or variables describing an observation placed in separate columns.\nLong format data tends to have different attributes encoded as levels of a single variable, followed by another column that contains the values of the observation at those different levels.\n\nThe key problem with the tidyness of the original data is that the income variables are not in their own columns, but rather are embedded in the structure of the columns, making it hard to manipuate the income variables.\nTo fix this, you can use the pivot_longer() function to gather values spread across several columns into a single column, here with the column names gathered into an income column.\n\n\nMutate to convert character to factor\n\n# Gather everything EXCEPT religion to tidy data\nrelig_income %&gt;%\n  pivot_longer(-religion, names_to = \"income\", values_to = \"respondents\") %&gt;%\n  mutate(religion = factor(religion), income = factor(income))\n\n# A tibble: 180 × 3\n   religion income             respondents\n   &lt;fct&gt;    &lt;fct&gt;                    &lt;dbl&gt;\n 1 Agnostic &lt;$10k                       27\n 2 Agnostic $10-20k                     34\n 3 Agnostic $20-30k                     60\n 4 Agnostic $30-40k                     81\n 5 Agnostic $40-50k                     76\n 6 Agnostic $50-75k                    137\n 7 Agnostic $75-100k                   122\n 8 Agnostic $100-150k                  109\n 9 Agnostic &gt;150k                       84\n10 Agnostic Don't know/refused          96\n# … with 170 more rows\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nWhen gathering, exclude any columns that you do not want “gathered” (religion in this case) by including the column names with a the minus sign in the pivot_longer() function.\nEven if your data is in a tidy format, pivot_longer() is occasionally useful for pulling data together to take advantage of faceting, or plotting separate plots based on a grouping variable.\n\n\n\n\n\npivot_wider()\nThe pivot_wider() function is the opposite function. It is useful for creating summary tables for reports, but generally less commonly needed to tidy data.\nYou use the summarize() function in dplyr to summarize the total number of respondents per income category.\n\nrelig_income %&gt;%\n  pivot_longer(-religion, names_to = \"income\", values_to = \"respondents\") %&gt;%\n  mutate(religion = factor(religion), income = factor(income)) %&gt;% \n  group_by(income) %&gt;% \n  summarize(total_respondents = sum(respondents)) \n\n# A tibble: 10 × 2\n   income             total_respondents\n   &lt;fct&gt;                          &lt;dbl&gt;\n 1 &lt;$10k                           1930\n 2 &gt;150k                           2608\n 3 $10-20k                         2781\n 4 $100-150k                       3197\n 5 $20-30k                         3357\n 6 $30-40k                         3302\n 7 $40-50k                         3085\n 8 $50-75k                         5185\n 9 $75-100k                        3990\n10 Don't know/refused              6121\n\n\npivot_wider() can be flexibly used in combination with pivot_longer to make a nicer table to print.\nNotice in this example how pivot_wider() has been used at the very end of the code sequence to convert the summarized data into a shape that offers a better tabular presentation for a report.\n\nrelig_income %&gt;%\n  pivot_longer(-religion, names_to = \"income\", values_to = \"respondents\") %&gt;%\n  mutate(religion = factor(religion), income = factor(income)) %&gt;% \n  group_by(income) %&gt;% \n  summarize(total_respondents = sum(respondents)) %&gt;%\n\n  pivot_wider(names_from = \"income\", \n              values_from = \"total_respondents\") %&gt;%\n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;$10k\n&gt;150k\n$10-20k\n$100-150k\n$20-30k\n$30-40k\n$40-50k\n$50-75k\n$75-100k\nDon’t know/refused\n\n\n\n\n1930\n2608\n2781\n3197\n3357\n3302\n3085\n5185\n3990\n6121\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nIn the pivot_wider() call, you first specify the name of the column to use for the new column names (income in this example) and then specify the column to use for the cell values (total_respondents here).\nLong format is often (but not always) the shape we need for tidy data. The important thing, however, is to think carefully about the shape you need for your analysis.\n\n\n\n\n\n\n\n\n\nExample of pivot_longer()\n\n\n\nLet’s try another dataset. This data contain an excerpt of the Gapminder data on life expectancy, GDP per capita, and population by country.\n\nlibrary(gapminder)\ngapminder\n\n# A tibble: 1,704 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# … with 1,694 more rows\n\n\nIf we wanted to make lifeExp, pop and gdpPercap (all measurements that we observe) go from a wide table into a long table, what would we do?\n\n# try it yourself\n\n\n\n\n\n\n\n\n\nExample\n\n\n\nOne more! Try using pivot_longer() to convert the the following data that contains simulated revenues for three companies by quarter for years 2006 to 2009.\nAfterward, use group_by() and summarize() to calculate the average revenue for each company across all years and all quarters.\nBonus: Calculate a mean revenue for each company AND each year (averaged across all 4 quarters).\n\ndf &lt;- tibble(\n  \"company\" = rep(1:3, each=4), \n  \"year\"  = rep(2006:2009, 3),\n  \"Q1\"    = sample(x = 0:100, size = 12),\n  \"Q2\"    = sample(x = 0:100, size = 12),\n  \"Q3\"    = sample(x = 0:100, size = 12),\n  \"Q4\"    = sample(x = 0:100, size = 12),\n)\ndf\n\n# A tibble: 12 × 6\n   company  year    Q1    Q2    Q3    Q4\n     &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1       1  2006    77     4    95    88\n 2       1  2007     0    22    70    45\n 3       1  2008    53     7    87    97\n 4       1  2009    64    80    33     8\n 5       2  2006     3    33    50    14\n 6       2  2007    98    39    11    37\n 7       2  2008   100    35    88    22\n 8       2  2009    38    96    78    89\n 9       3  2006    61     6    14    87\n10       3  2007    11    52    28    58\n11       3  2008    46    37    29    18\n12       3  2009    23    82    56    59\n\n\n\n# try it yourself \n\n\n\n\n\nseparate() and unite() cells within columns of data\nStill in thetidyr package:\n\nunite(): paste contents of two or more columns into a single column\nseparate(): split contents of a column into two or more columns\n\nFirst, we combine the first three columns into one new column using unite(). This function is similar to newvar &lt;- paste(A,B,C, sep=\"_\")\n\nnames(gapminder)\n\n[1] \"country\"   \"continent\" \"year\"      \"lifeExp\"   \"pop\"       \"gdpPercap\"\n\ngapminder %&gt;% \n  unite(col=\"country_continent_year\", \n        country:year, \n        sep=\"_\")\n\n# A tibble: 1,704 × 4\n   country_continent_year lifeExp      pop gdpPercap\n   &lt;chr&gt;                    &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan_Asia_1952     28.8  8425333      779.\n 2 Afghanistan_Asia_1957     30.3  9240934      821.\n 3 Afghanistan_Asia_1962     32.0 10267083      853.\n 4 Afghanistan_Asia_1967     34.0 11537966      836.\n 5 Afghanistan_Asia_1972     36.1 13079460      740.\n 6 Afghanistan_Asia_1977     38.4 14880372      786.\n 7 Afghanistan_Asia_1982     39.9 12881816      978.\n 8 Afghanistan_Asia_1987     40.8 13867957      852.\n 9 Afghanistan_Asia_1992     41.7 16317921      649.\n10 Afghanistan_Asia_1997     41.8 22227415      635.\n# … with 1,694 more rows\n\n\nNext, we show how to separate the columns into three separate columns using separate() using the col, into and sep arguments. Note that this works by finding the delimiter, and relies on order of the information.\n\ngapminder %&gt;% \n  unite(col=\"country_continent_year\", \n        country:year, \n        sep=\"_\") %&gt;% \n  separate(col=\"country_continent_year\", \n           into=c(\"country\", \"continent\", \"year\"), \n           sep=\"_\")\n\n# A tibble: 1,704 × 6\n   country     continent year  lifeExp      pop gdpPercap\n   &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia      1952     28.8  8425333      779.\n 2 Afghanistan Asia      1957     30.3  9240934      821.\n 3 Afghanistan Asia      1962     32.0 10267083      853.\n 4 Afghanistan Asia      1967     34.0 11537966      836.\n 5 Afghanistan Asia      1972     36.1 13079460      740.\n 6 Afghanistan Asia      1977     38.4 14880372      786.\n 7 Afghanistan Asia      1982     39.9 12881816      978.\n 8 Afghanistan Asia      1987     40.8 13867957      852.\n 9 Afghanistan Asia      1992     41.7 16317921      649.\n10 Afghanistan Asia      1997     41.8 22227415      635.\n# … with 1,694 more rows\n\n\n\n\n\nPost-lecture materials\n\nFinal Questions\nHere are some post-lecture questions to help you think about the material discussed.\n\n\n\n\n\n\nQuestions\n\n\n\n\nUsing prose, describe how the variables and observations are organised in a tidy dataset versus an non-tidy dataset.\nWhat do the extra and fill arguments do in separate()? Experiment with the various options for the following two toy datasets.\n\n\ntibble(x = c(\"a,b,c\", \"d,e,f,g\", \"h,i,j\")) %&gt;% \n  separate(x, c(\"one\", \"two\", \"three\"))\n\ntibble(x = c(\"a,b,c\", \"d,e\", \"f,g,i\")) %&gt;% \n  separate(x, c(\"one\", \"two\", \"three\"))\n\n\nBoth unite() and separate() have a remove argument. What does it do? Why would you set it to FALSE?\nCompare and contrast separate() and extract(). Why are there three variations of separation (by position, by separator, and with groups), but only one unite()?\n\n\n\n\n\nSolution for gapminder example\n\ngapminder %&gt;% \n  pivot_longer(-c(country, continent, year), names_to = \"metrics\", values_to = \"values\")\n\n# A tibble: 5,112 × 5\n   country     continent  year metrics       values\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt; &lt;chr&gt;          &lt;dbl&gt;\n 1 Afghanistan Asia       1952 lifeExp         28.8\n 2 Afghanistan Asia       1952 pop        8425333  \n 3 Afghanistan Asia       1952 gdpPercap      779. \n 4 Afghanistan Asia       1957 lifeExp         30.3\n 5 Afghanistan Asia       1957 pop        9240934  \n 6 Afghanistan Asia       1957 gdpPercap      821. \n 7 Afghanistan Asia       1962 lifeExp         32.0\n 8 Afghanistan Asia       1962 pop       10267083  \n 9 Afghanistan Asia       1962 gdpPercap      853. \n10 Afghanistan Asia       1967 lifeExp         34.0\n# … with 5,102 more rows\n\n\nWe stacked the three variables lifeExp, pop, and gdpPercap so now the table is a little thinner and three times as long.\nWhy did we have to make the non-gathered variables into a vector?"
  },
  {
    "objectID": "posts/2023-01-26-intro-quarto/index.html",
    "href": "posts/2023-01-26-intro-quarto/index.html",
    "title": "Literate Statistical Programming and Quarto",
    "section": "",
    "text": "Read ahead\n\n\n\nBefore class, you can prepare by reading the following materials:\n\nhttps://rss.onlinelibrary.wiley.com/doi/epdf/10.1111/1740-9713.01522\nCreating a Website in Quarto quickstart up to and including Render https://quarto.org/docs/websites/\nPublishing to GitHub up to and including Render to docs https://quarto.org/docs/publishing/github-pages.html\n\n\n\n\n\n\nMaterial for this lecture was borrowed and adopted from\n\nhttps://www.stephaniehicks.com/jhustatcomputing2022/posts/2022-09-01-literate-programming/\nhttp://users.stat.umn.edu/~geyer/Sweave/\nhttps://rdpeng.github.io/Biostat776/lecture-literate-statistical-programming.html\nhttps://statsandr.com/blog/tips-and-tricks-in-rstudio-and-r-markdown"
  },
  {
    "objectID": "posts/2023-01-26-intro-quarto/index.html#read-ahead",
    "href": "posts/2023-01-26-intro-quarto/index.html#read-ahead",
    "title": "Literate Statistical Programming and Quarto",
    "section": "",
    "text": "Read ahead\n\n\n\nBefore class, you can prepare by reading the following materials:\n\nhttps://rss.onlinelibrary.wiley.com/doi/epdf/10.1111/1740-9713.01522\nCreating a Website in Quarto quickstart up to and including Render https://quarto.org/docs/websites/\nPublishing to GitHub up to and including Render to docs https://quarto.org/docs/publishing/github-pages.html"
  },
  {
    "objectID": "posts/2023-01-26-intro-quarto/index.html#acknowledgements",
    "href": "posts/2023-01-26-intro-quarto/index.html#acknowledgements",
    "title": "Literate Statistical Programming and Quarto",
    "section": "",
    "text": "Material for this lecture was borrowed and adopted from\n\nhttps://www.stephaniehicks.com/jhustatcomputing2022/posts/2022-09-01-literate-programming/\nhttp://users.stat.umn.edu/~geyer/Sweave/\nhttps://rdpeng.github.io/Biostat776/lecture-literate-statistical-programming.html\nhttps://statsandr.com/blog/tips-and-tricks-in-rstudio-and-r-markdown"
  },
  {
    "objectID": "posts/2023-01-26-intro-quarto/index.html#weaving-and-tangling",
    "href": "posts/2023-01-26-intro-quarto/index.html#weaving-and-tangling",
    "title": "Literate Statistical Programming and Quarto",
    "section": "Weaving and Tangling",
    "text": "Weaving and Tangling\nLiterate programs by themselves are a bit difficult to work with, but they can be processed in two important ways.\nLiterate programs can be weaved to produce human readable documents like PDFs or HTML web pages, and they can tangled to produce machine-readable “documents”, or in other words, machine readable code.\nIn order to use a system like this you need a documentational language, that’s human readable, and you need a programming language that’s machine readable (or can be compiled/interpreted into something that’s machine readable)."
  },
  {
    "objectID": "posts/2023-01-26-intro-quarto/index.html#sweave",
    "href": "posts/2023-01-26-intro-quarto/index.html#sweave",
    "title": "Literate Statistical Programming and Quarto",
    "section": "Sweave",
    "text": "Sweave\nOne of the original literate programming systems in R that was designed to do this was called Sweave written by Friedrich Leisch. Sweave enables users to combine R code with a documentation program called LaTeX. Sweave revolutionized coding, and has become part of the R base code. Leisch is on the R Core Development Team and the BioConductor Project.\nSweave files ends a .Rnw and have R code weaved through the document:\n&lt;&lt;plot1, height=4, width=5, eval=FALSE&gt;&gt;=\ndata(airquality)\nplot(airquality$Ozone ~ airquality$Wind)\n@\nOnce you have created your .Rnw file, Sweave will process the file, executing the R chunks and replacing them with output as appropriate before creating the PDF document.\nSweaveʻs main limitation is that it requires knowledge of LaTeX\n\nLaTeX is very powerful for laying out mathematical equations and fine-tuned control of formatting, but is not a documentation language that is widely used outside of mathematics.\nTherefore, there is a steep learning curve.\nSweave also lacks a lot of features that people find useful like caching, and multiple plots per page and mixing programming languages.\n\nInstead, folks have moved towards using something called knitr, which offers everything Sweave does, plus it extends it to much simpler Markdown documents."
  },
  {
    "objectID": "posts/2023-01-26-intro-quarto/index.html#rmarkdown",
    "href": "posts/2023-01-26-intro-quarto/index.html#rmarkdown",
    "title": "Literate Statistical Programming and Quarto",
    "section": "rmarkdown",
    "text": "rmarkdown\nAnother choice for literate programming is to build documents based on Markdown language. A markdown file is a plain text file that is typically given the extension .md. The rmarkdown R package takes a R Markdown file (.Rmd) and weaves together R code chunks Figure 1, producing a large number of user-specified outputs.\n\n\n\nFigure 1: R markdown translates text and code to many different formats\n\n\nR chunks surrounded by text looks like this:\n```{r plot1, height=4, width=5, eval=FALSE, echo=TRUE}\ndata(airquality)\nplot(airquality$Ozone ~ airquality$Wind)\n```\n\n\n\n\n\n\nTip\n\n\n\nThe best resource for learning about R Markdown this by Yihui Xie, J. J. Allaire, and Garrett Grolemund:\n\nhttps://bookdown.org/yihui/rmarkdown\n\nThe R Markdown Cookbook by Yihui Xie, Christophe Dervieux, and Emily Riederer is really good too:\n\nhttps://bookdown.org/yihui/rmarkdown-cookbook\n\nThe authors of the 2nd book describe the motivation for the 2nd book as:\n\n“However, we have received comments from our readers and publisher that it would be beneficial to provide more practical and relatively short examples to show the interesting and useful usage of R Markdown, because it can be daunting to find out how to achieve a certain task from the aforementioned reference book (put another way, that book is too dry to read). As a result, this cookbook was born.”\n\n\n\nBecause this is lecture is built in a .qmd file (which is very similar to a .Rmd file), let’s demonstrate how this work. I am going to change eval=FALSE to eval=TRUE.\n\ndata(airquality)\nplot(airquality$Ozone ~ airquality$Wind)\n\n\n\n\n\n\n\n\n\n\nQuestions\n\n\n\n\nWhy do we not see the back ticks ``` anymore in the code chunk above that made the plot?\nWhat do you think we should do if we want to have the code executed, but we want to hide the code that made it?\n\n\n\nBefore we leave this section, I find that there is quite a bit of terminology to understand the magic behind rmarkdown that can be confusing, so let’s break it down:\n\nPandoc. Pandoc is a command line tool with no GUI that converts documents (e.g. from number of different markup formats to many other formats, such as .doc, .pdf etc). It is completely independent from R (but does come bundled with RStudio). If you donʻt have Rstudio installed, you will have to install pandoc.\nMarkdown (markup language). Markdown is a lightweight markup language with plain text formatting syntax designed so that it can be converted to HTML and many other formats. A markdown file is a plain text file that is typically given the extension .md. It is completely independent from R.\nR Markdown (markup language). R Markdown is an extension of the markdown syntax for weaving together text with R code. R Markdown files are plain text files that typically have the file extension .Rmd.\nrmarkdown (R package). The R package rmarkdown is a library that uses pandoc to process and convert text and R code written in .Rmd files into a number of different formats. This core function is rmarkdown::render(). Note: this package only deals with the markdown language. If the input file is e.g. .Rhtml or .Rnw, then you need to use knitr prior to calling pandoc (see below).\n\n\n\n\n\n\n\nTip\n\n\n\nCheck out the R Markdown Quick Tour for more:\n\nhttps://rmarkdown.rstudio.com/authoring_quick_tour.html\n\n\n\n\n\n\nArtwork by Allison Horst on RMarkdown\n\n\n\nknitr\nOne of the alternative that has come up in recent times is something called knitr.\n\nThe knitr package for R takes a lot of these ideas of literate programming and updates and improves upon them.\nknitr still uses R as its programming language, but it allows you to mix other programming languages in.\nYou can also use a variety of documentation languages now, such as LaTeX, markdown and HTML.\nknitr was developed by Yihui Xie while he was a graduate student at Iowa State and it has become a very popular package for writing literate statistical programs.\n\nKnitr takes a plain text document with embedded code, executes the code and ‘knits’ the results back into the document.\nFor for example, it converts\n\nAn R Markdown (.Rmd) file into a standard markdown file (.md)\nAn .Rnw (Sweave) file into to .tex format.\nAn .Rhtml file into to .html.\n\nThe core function is knitr::knit() and by default this will look at the input document and try and guess what type it is e.g. Rnw, Rmd etc.\nThis core function performs three roles:\n\nA source parser, which looks at the input document and detects which parts are code that the user wants to be evaluated.\nA code evaluator, which evaluates this code\nAn output renderer, which writes the results of evaluation back to the document in a format which is interpretable by the raw output type. For instance, if the input file is an .Rmd, the output render marks up the output of code evaluation in .md format.\n\n\n\n\n\n\nConverting a Rmd file to many outputs using knitr and pandoc\n\n\n\n\n[Source]\nAs seen in the figure above, from there pandoc is used to convert e.g. a .md file into many other types of file formats into a .html, etc.\nSo in summary:\n\n“R Markdown stands on the shoulders of knitr and Pandoc. The former executes the computer code embedded in Markdown, and converts R Markdown to Markdown. The latter renders Markdown to the output format you want (such as PDF, HTML, Word, and so on).”\n\n[Source]"
  },
  {
    "objectID": "posts/2023-01-26-intro-quarto/index.html#create-your-website-locally-with-quarto",
    "href": "posts/2023-01-26-intro-quarto/index.html#create-your-website-locally-with-quarto",
    "title": "Literate Statistical Programming and Quarto",
    "section": "Create your website locally with Quarto",
    "text": "Create your website locally with Quarto\nIn this section, I am adding a bit more explanation to the Quarto quickstart guide up to and including Render. If something is not clear, please consult https://quarto.org/docs/websites/\nThere are three main quarto commands we will use:\n\nquarto create-project: Make a website project template\nquarto preview: Take a look at what the webite will look like\nquarto render: Render your qmd to html\n\n\nMake your website directory and template\nCreate your website (here called mysite) using the following command. It will make a directory of the same name and put the website contents within it.\n\n\nTerminal\n\nquarto create-project mysite --type website\n\nYou should now see the following files in your mysite directory (Figure 2):\n\n\n\nFigure 2: Website files from the Terminal view\n\n\nThis is the bare-bones version of your website. Check that the code is functional by looking at a preview:\n\n\nTerminal\n\nquarto preview\n\nThis should open up a browser window showing a temporary file made by quarto by rendering your website files.\n\n\n\n\n\n\nTip\n\n\n\n\nquarto preview will refresh the preview every time you save your index.qmd (or any) website files. So itʻs a good idea to keep the preview open as you make edits and saves.\nCheck every edit, it is easier to debug in small steps.\nTerminate quarto preview with Control-c\n\n\n\n\n\nRender your website to html\nUse quarto to render your content to html, the format used by browsers. First navigate into your website directory then render:\n\n\nTerminal\n\ncd mysite\nquarto render\n\nTake a look at the mysite contents after rendering, you should see a new directory _site (Figure 3). The html was rendered and put in there (go ahead, open up the files and check it out):\n\n\n\nFigure 3: Website files after rendering\n\n\n\n\nPersonalize your content\nWhat is really nice is that you can personalize your website by simply editing the quarto markdown and yaml files.\n\nWeb content goes in .qmd\nUsing any text editor, edit the index.qmd to personalize your website.\nThe first section of your index.qmd is the header. You can change the title and add additional header information, including any cover images and website templates.\nFor example this is what I have in my course website index.qmd header. Note that my cover image is in a folder called images within at the top level of my website directory. If you want to try this out substitute or remove the image line and change the twitter/github handles.\n\n\nindex.qmd\n\n---\ntitle: \"Welcome to Introduction to Data Science in R for Biologists!\"\nimage: images/mycoolimage.png\nabout:\n  template: jolla\n  links:\n    - icon: twitter\n      text: Twitter\n      href: https://twitter.com/mbutler808\n    - icon: github\n      text: Github\n      href: https://github.com/mbutler808\n---\n\nYou should edit the body of your website as well. You simply edit the text.\nThe quarto markdown page has great examples showing how to format your content. Take a look at how to specify header sizes, lists, figures and tables.\nTry editing the about.qmd file as well. You will notice that this is another tab in your website. YOu can add more tabs by adding .qmd files.\nWith each addition, be sure to quarto preview your changes to make sure it works. When you are satisfied with your website, quarto render to render to html.\n\n\n\n\n\n\nTip\n\n\n\n\nWhen editing markdown, take care to note spaces and indents as they are interpreted for formatting.\nIndentations are really important for formatting lists.\nFor example in a hyperlink, there is no space between the square brackets and parentheses. [This is a cool link](http://mycoollink.com)\n\n\n\n\n\nWebsite-wide settings go in _quarto.yml\nAll Quarto projects include a _quarto.yml configuration file that sets the global options that apply across the entire website.\nYAML started off as “Yet Another Markup Language” 😜. It is clean, clear, and widely used. You can edit your YAML to add options or change the format of your website. Take a look at your _quarto.yml.\nHere is an example for a simple website. title: is the parameter to set the websiteʻs title. navbar: sets the menu, in this case on the left sidebar. By default tabs will be named based on the names of the .qmd files, but you can set them manually. There are many themes you can choose from too, check them out. For something different try cyborg.\n\n\n_quarto.yml\n\nproject:\n  type: website\n\nwebsite:\n  title: \"today\"\n  navbar:\n    left:\n      - href: index.qmd\n        text: Home\n      - about.qmd\n\nformat:\n  html:\n    theme: minty\n    css: styles.css\n    toc: true\n\nAgain, after saving your edits, quarto preview to see the effects. When you are satisfied with your website, quarto render to render to html.\n\n\nTerminal\n\nquarto render"
  },
  {
    "objectID": "posts/2023-01-26-intro-quarto/index.html#publishing-your-website-to-github",
    "href": "posts/2023-01-26-intro-quarto/index.html#publishing-your-website-to-github",
    "title": "Literate Statistical Programming and Quarto",
    "section": "Publishing your website to GitHub",
    "text": "Publishing your website to GitHub\nYou can publish your website for free on GitHub, which is a very cool feature. In his section I am adding a bit more explanation to the Quarto quickstart guide up to and including Render to docs https://quarto.org/docs/publishing/github-pages.html. I describe the most important stpes below:\n\nRender your html to a docs directory\nSupress GitHub jekyll html processing by creating a .nojekyll file\nMake your website directory into a repo, and link it to a GitHub repo\nEdit the GitHub repo settings to publish your website\n\n\nRender your html to docs\nEdit the _quarto.yml file at the top level of your website to send output to docs. This will also create the docs folder.\n\n\n_quarto.yml\n\nproject:\n  type: website\n  output-dir: docs\n\nThe next time you quarto render it will create docs and all of its contents.\n\n\nSupress GitHub jekyll html processing\nGitHub uses a sofware called jekyll to render html from markdown. Since weʻre using quarto, we want to supress that. Create an empty file named .nojekyll at the top level of your website directory to supress default jekyll processing.\n\n\n\n\n\n\n\nMac/Linux\n\n\nTerminal\n\ntouch .nojekyll\n\n\n\nWindows\n\n\nTerminal\n\ncopy NUL .nojekyll\n\n\n\n\n\n\nSetup a GitHub repo for your website\n\nTurn your website directory into a git repo:\n\n\n\nTerminal\n\ngit init\ngit add .\ngit commit -m \"first commit\"\n\n\nCreate a GitHub repo by the same name\n\nFor example, mine might be github.com/mbutler808/mysite.\n\nLink your local repo and GitHub repo together\n\nIf you forgot how to do this, go back here\n\nCheck your GitHub repo. Are your files there?\n\n\n\nGitHub settings to serve your webpage\nAlmost there! A couple more steps.\nFrom your GitHub repo, click on Settings in the top menu, and Pages on the left menu.\nYour website should deploy from branch. Under Select branch choose main and under Select folder choose docs.\nAfter clicking save GitHub will trigger a deployment of your website. After a few minutes, your URL will appear near the top at Your site is live at...:\n\nCongratulations! ⚡️ Your website is now live 🎉🎊😍"
  },
  {
    "objectID": "posts/2023-01-26-intro-quarto/index.html#now-make-more-changes",
    "href": "posts/2023-01-26-intro-quarto/index.html#now-make-more-changes",
    "title": "Literate Statistical Programming and Quarto",
    "section": "Now make more changes!",
    "text": "Now make more changes!\n\n\n\n\n\n\nThe Quarto Workflow is\n\n\n\n\nEdit the content in .qmd\nFrom the Command line:\n\nquarto preview to check that edits are correct\nquarto render to render .qmd to .html\ngit add .\ngit commit -m \"message\"\ngit push origin main\n\nCheck your website (this may take a beat)"
  },
  {
    "objectID": "posts/2023-01-26-intro-quarto/index.html#for-fun",
    "href": "posts/2023-01-26-intro-quarto/index.html#for-fun",
    "title": "Literate Statistical Programming and Quarto",
    "section": "For fun",
    "text": "For fun\nYou can have fun with emoji! Guangchuang Yu wrote the package emojifont (this is the same person who wrote the widely used ggtree package) and now you can bring your emoji out of your phone and into your quarto documents! Install the R package emojifont:\n\ninstall.packages(\"emojifont\")\n\nThen anywhere you want an emoji in the markdown file, you just type:\n\n`r emojifont::emoji('palm_tree')`\n\n🌴\nOr if you want several, just line them up:\n\n`r emojifont::emoji('balloon')``r emojifont::emoji('tada')``r emojifont::emoji('smiley')`\n\n🎈🎉😃\nThere is a handy cheat sheet of emoji names here https://gist.github.com/rxaviers/7360908"
  },
  {
    "objectID": "posts/2023-01-26-intro-quarto/index.html#final-tips",
    "href": "posts/2023-01-26-intro-quarto/index.html#final-tips",
    "title": "Literate Statistical Programming and Quarto",
    "section": "Final tips",
    "text": "Final tips\n\n\n\n\n\n\nTip\n\n\n\n\nAlways always quarto render before you push up your changes to GitHub!\nIf your changes are not appearing, try quarto preview and check that your changes appear in the preview. Then quarto render before you use git to add, commit, and push\nNote: It can take a few minutes to render on GitHub before your changes appear on your website\n\n\n\nPlease see Stephanie Hicksʻ lecture for more literate programming examples and tips."
  },
  {
    "objectID": "posts/2023-03-07-univariate/index.html",
    "href": "posts/2023-03-07-univariate/index.html",
    "title": "A small review of univariate parametric statistics",
    "section": "",
    "text": "Learning objectives\n\n\n\nAt the end of this lesson you will:\n\nBe able to perform basic univariate statistics\nBe able to design plots to display univariate comparisons\nBe able to relate questions to graphical representations of data"
  },
  {
    "objectID": "posts/2023-03-07-univariate/index.html#linear-regression",
    "href": "posts/2023-03-07-univariate/index.html#linear-regression",
    "title": "A small review of univariate parametric statistics",
    "section": "Linear Regression",
    "text": "Linear Regression\nLinear regression asks whether there is a relationship between X and Y, that is if you know X can you predict the value of Y?\n\nwith(iris, plot(Sepal.Width ~ Sepal.Length))\nlm.fit &lt;- with(iris, lm(Sepal.Width ~ Sepal.Length))\nabline(lm.fit, col=\"blue\")\n\n\n\n\nLinear regression results in two parameters, the best-fit slope and intercept:\n\nlm.fit \n\n\nCall:\nlm(formula = Sepal.Width ~ Sepal.Length)\n\nCoefficients:\n (Intercept)  Sepal.Length  \n     3.41895      -0.06188  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sepal.Width ~ Sepal.Length)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1095 -0.2454 -0.0167  0.2763  1.3338 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   3.41895    0.25356   13.48   &lt;2e-16 ***\nSepal.Length -0.06188    0.04297   -1.44    0.152    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4343 on 148 degrees of freedom\nMultiple R-squared:  0.01382,   Adjusted R-squared:  0.007159 \nF-statistic: 2.074 on 1 and 148 DF,  p-value: 0.1519\n\n\nRegression minimizes the sum of squared errors (or deviations) from the line. The “errors” are the difference between where Y is, and where Y should be if it followed a perfect line.\nWe can illustrate what this means:\n\nx &lt;- iris$Sepal.Length\ny &lt;- iris$Sepal.Width\nyhat &lt;- predict(lm.fit)\n\nwith(iris, plot(Sepal.Width ~ Sepal.Length))\nwith(iris, abline(lm.fit, col=\"blue\"))\nfor(i in 1:length(x))  lines(x[c(i,i)],c(y[i], yhat[i]), col=\"red\", lty=2)\n\n\n\n\nThe regression line is the best fit line that minimizes the sums of squared deviations from the regression. It turns out that the least-squares fit of the regression line is also provides the Maximum Likelihood fits of the parameters of the line (the slope and intercept)."
  },
  {
    "objectID": "posts/2023-03-07-univariate/index.html#anova",
    "href": "posts/2023-03-07-univariate/index.html#anova",
    "title": "A small review of univariate parametric statistics",
    "section": "ANOVA",
    "text": "ANOVA\nAnalysis of variance is very closely related to regression. It also works by minimizing sums of squares, but it asks a different question.\nDoes the data better fit a model with one group (one regression line?) or multiple groups (multiple regression lines, one for each group)?\nGraphically, it looks like the plot below with the question Is the data explained better by a single group with a grand mean? or with separate means for each Species?\n\npar(mfrow=c(1,2))\nwith(iris, boxplot(Sepal.Length))\nwith(iris, plot(Sepal.Length ~ Species))\n\n\n\n\n\nIs One of these Groups not like the others?\nStatistically, this is asking whether the sums of squares is minimized by assuming there is only one group (one mean)? Or three groups?\nFor this plot we will add an index column (1 to number of observations), and use ggplot2, dplyr, and the pipe from magrittr\n\nrequire(dplyr)\nrequire(magrittr)\nrequire(ggplot2)\n\ndat &lt;- cbind(iris, id = 1:length(iris$Species))\nyhat &lt;- mean(iris$Sepal.Length)  # grand mean of Sepal Length\n\np &lt;- dat %&gt;% ggplot(aes( x = id, y = Sepal.Length, group=Species)) \n\nq &lt;- p + geom_point( size=2) + \n  geom_hline( aes(yintercept = mean(iris$Sepal.Length)) ) + \n  geom_segment( data=dat, aes( x = id, y = Sepal.Length, xend = id, yend = yhat), color=\"red\", lty = 3)\n\nq  \n\n\n\n\nWe added two new ggplot2 functions:\n\ngeom_hline() which adds a horizontal line used for the grand mean. This is similar to base R abline()\n\ngeom_segment() which plots line segments indicated by x,y start points and xend,yend end points (this is based on the base R segment() function)\n\nTo add in the group structure, we need to compute means by species, and know where one species ends and the other begins in the data vector. We can do this with group_by() and summarize():\n\nspmeans  &lt;- dat %&gt;% group_by(Species) %&gt;% \n        summarise(\n          sl = mean(Sepal.Length),\n          n = length(Sepal.Length),\n          minid = min(id),\n          maxid = max(id)\n        )\n\nspmeans\n\n# A tibble: 3 × 5\n  Species       sl     n minid maxid\n  &lt;fct&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1 setosa      5.01    50     1    50\n2 versicolor  5.94    50    51   100\n3 virginica   6.59    50   101   150\n\n\n(You should always check that minid and maxid is what you intended)\nWe want to include this mean information in the dataframe, so to add it as a vector, we can merge():\n\ndat &lt;- merge(dat, spmeans)\nhead(dat)\n\n  Species Sepal.Length Sepal.Width Petal.Length Petal.Width id    sl  n minid\n1  setosa          5.1         3.5          1.4         0.2  1 5.006 50     1\n2  setosa          4.9         3.0          1.4         0.2  2 5.006 50     1\n3  setosa          4.7         3.2          1.3         0.2  3 5.006 50     1\n4  setosa          4.6         3.1          1.5         0.2  4 5.006 50     1\n5  setosa          5.0         3.6          1.4         0.2  5 5.006 50     1\n6  setosa          5.4         3.9          1.7         0.4  6 5.006 50     1\n  maxid\n1    50\n2    50\n3    50\n4    50\n5    50\n6    50\n\ndat[45:55,]\n\n      Species Sepal.Length Sepal.Width Petal.Length Petal.Width id    sl  n\n45     setosa          5.1         3.8          1.9         0.4 45 5.006 50\n46     setosa          4.8         3.0          1.4         0.3 46 5.006 50\n47     setosa          5.1         3.8          1.6         0.2 47 5.006 50\n48     setosa          4.6         3.2          1.4         0.2 48 5.006 50\n49     setosa          5.3         3.7          1.5         0.2 49 5.006 50\n50     setosa          5.0         3.3          1.4         0.2 50 5.006 50\n51 versicolor          7.0         3.2          4.7         1.4 51 5.936 50\n52 versicolor          6.4         3.2          4.5         1.5 52 5.936 50\n53 versicolor          6.9         3.1          4.9         1.5 53 5.936 50\n54 versicolor          5.5         2.3          4.0         1.3 54 5.936 50\n55 versicolor          6.5         2.8          4.6         1.5 55 5.936 50\n   minid maxid\n45     1    50\n46     1    50\n47     1    50\n48     1    50\n49     1    50\n50     1    50\n51    51   100\n52    51   100\n53    51   100\n54    51   100\n55    51   100\n\ntail(dat)\n\n      Species Sepal.Length Sepal.Width Petal.Length Petal.Width  id    sl  n\n145 virginica          6.7         3.3          5.7         2.5 145 6.588 50\n146 virginica          6.7         3.0          5.2         2.3 146 6.588 50\n147 virginica          6.3         2.5          5.0         1.9 147 6.588 50\n148 virginica          6.5         3.0          5.2         2.0 148 6.588 50\n149 virginica          6.2         3.4          5.4         2.3 149 6.588 50\n150 virginica          5.9         3.0          5.1         1.8 150 6.588 50\n    minid maxid\n145   101   150\n146   101   150\n147   101   150\n148   101   150\n149   101   150\n150   101   150\n\n\nNow that we have our dataframe with all of the necessary information, we can plot.\nNote that there are two calls to geom_segment(). For the first, we are plotting the species means, so we use the smmeans dataset. For the second, we are plotting each pointʻs deviation from the species means so we use the full dataset. The rest is telling the function where the start and end points of each segment are:\n\nr &lt;- p + geom_point( size=2) + \n  geom_segment( data=spmeans, aes(x=minid, y = sl, xend=maxid, yend=sl, group=Species )) + \n  geom_segment( data=dat, aes( x = id, y = Sepal.Length, xend = id, yend = sl, color=Species), lty = 3) \n\nr  \n\n\n\n\nBack to our question - is the error sum of squares minimized by accouting for separate species or considering all irises as one group? Another way to state ANOVA is - is at least one of these groups different than the others?\n\nrequire(cowplot)\n\nLoading required package: cowplot\n\nplot_grid(\n  q, \n  r + theme(legend.position=\"none\"), \n  labels=\"AUTO\")\n\n\n\n\nIf we want to know whether species are different in sepal length, then we need to have lm fit the model by species. We do this like so:\n\nlm.fit &lt;- with(iris, lm(Sepal.Length ~ Species))\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sepal.Length ~ Species)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6880 -0.3285 -0.0060  0.3120  1.3120 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         5.0060     0.0728  68.762  &lt; 2e-16 ***\nSpeciesversicolor   0.9300     0.1030   9.033 8.77e-16 ***\nSpeciesvirginica    1.5820     0.1030  15.366  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5148 on 147 degrees of freedom\nMultiple R-squared:  0.6187,    Adjusted R-squared:  0.6135 \nF-statistic: 119.3 on 2 and 147 DF,  p-value: &lt; 2.2e-16\n\n\nInterpretation: One-way ANOVA is like fitting a regression of the individual points against the grand mean of the points vs. separate regressions for each group. The summary shows that the the intercept (the mean of setosa) is about 5 (significantly different than zero), whereas the other species are contrasts against setosa, the first species. Versicolor is 0.93 higher than setosa, and virginica is 1.58 higher than setosa. Both of these contrasts are signficant. So they are actually all significantly different than each other\nNotice that now have more parameters estimated. You can specify which parameter values and contrasts you want displayed. Often we just want an ANOVA table, which tests the hypothesis that at least one group is different than the others:\n\nanova(lm.fit)\n\nAnalysis of Variance Table\n\nResponse: Sepal.Length\n           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nSpecies     2 63.212  31.606  119.26 &lt; 2.2e-16 ***\nResiduals 147 38.956   0.265                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe can see that species are significantly different in sepal length. Can you make a plot that shows this and add the statistics to it?\nAnd thatʻs how ANOVA is related to regression!"
  },
  {
    "objectID": "posts/2023-03-07-univariate/index.html#ancova",
    "href": "posts/2023-03-07-univariate/index.html#ancova",
    "title": "A small review of univariate parametric statistics",
    "section": "ANCOVA",
    "text": "ANCOVA\nThere are many forms of regession and ANOVA. For example, if you want to see if the relationship between Sepal.Length and Sepal.Width differs by species, you woul do an ANCOVA (analysis of covariance):\n\nlm.fit &lt;- with(iris, lm(Sepal.Width ~ Sepal.Length + Species))\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sepal.Width ~ Sepal.Length + Species)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.95096 -0.16522  0.00171  0.18416  0.72918 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        1.67650    0.23536   7.123 4.46e-11 ***\nSepal.Length       0.34988    0.04630   7.557 4.19e-12 ***\nSpeciesversicolor -0.98339    0.07207 -13.644  &lt; 2e-16 ***\nSpeciesvirginica  -1.00751    0.09331 -10.798  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.289 on 146 degrees of freedom\nMultiple R-squared:  0.5693,    Adjusted R-squared:  0.5604 \nF-statistic: 64.32 on 3 and 146 DF,  p-value: &lt; 2.2e-16\n\n\nWhich would fit separate Y-intercepts for each species."
  },
  {
    "objectID": "posts/2023-04-25-ggtree/index.html",
    "href": "posts/2023-04-25-ggtree/index.html",
    "title": "The ggtree-verse",
    "section": "",
    "text": "References for this Material:\n\nTreedata book by Guangchuang Yu (Yu 2022) https://yulab-smu.top/treedata-book/\nhttps://bioconnector.github.io/workshops/r-ggtree.html"
  },
  {
    "objectID": "posts/2023-04-25-ggtree/index.html#coverting-between-data-types",
    "href": "posts/2023-04-25-ggtree/index.html#coverting-between-data-types",
    "title": "The ggtree-verse",
    "section": "Coverting between data types",
    "text": "Coverting between data types\nThe main functions for coersion are:\n\nas.phylo (tree)  # to phylo\n\n\nPhylogenetic tree with 20 tips and 19 internal nodes.\n\nTip labels:\n  t20, t10, t3, t7, t6, t18, ...\n\nRooted; includes branch lengths.\n\nas.treedata(tree) # to treedata\n\n'treedata' S4 object'.\n\n...@ phylo:\n\nPhylogenetic tree with 20 tips and 19 internal nodes.\n\nTip labels:\n  t20, t10, t3, t7, t6, t18, ...\n\nRooted; includes branch lengths.\n\ntreedata &lt;- as.treedata(tree)  \nas_tibble(treedata)  # to tibble\n\n# A tibble: 39 × 4\n   parent  node branch.length label\n    &lt;int&gt; &lt;int&gt;         &lt;dbl&gt; &lt;chr&gt;\n 1     22     1       0.915   t20  \n 2     26     2       0.532   t10  \n 3     26     3       0.847   t3   \n 4     25     4       0.118   t7   \n 5     28     5       0.690   t6   \n 6     28     6       0.725   t18  \n 7     29     7       0.261   t14  \n 8     31     8       0.00314 t9   \n 9     31     9       0.0128  t4   \n10     30    10       0.289   t12  \n# … with 29 more rows\n# ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "posts/2023-04-25-ggtree/index.html#subsetting-the-tree",
    "href": "posts/2023-04-25-ggtree/index.html#subsetting-the-tree",
    "title": "The ggtree-verse",
    "section": "Subsetting the tree",
    "text": "Subsetting the tree\nFunctions: drop.tip() and keep.tip()\nSuppose we want to drop all of the even tips:\n\ntodrop &lt;- paste(\"t\", 1:10*2, sep=\"\")\ntodrop\n\n [1] \"t2\"  \"t4\"  \"t6\"  \"t8\"  \"t10\" \"t12\" \"t14\" \"t16\" \"t18\" \"t20\"\n\nsmalltree &lt;- drop.tip(ttree, todrop)\nsmalltree\n\n'treedata' S4 object'.\n\n...@ phylo:\n\nPhylogenetic tree with 10 tips and 9 internal nodes.\n\nTip labels:\n  t3, t7, t9, t5, t13, t15, ...\n\nRooted; includes branch lengths.\n\nwith the following features available:\n  'size', 'habitat'.\n\n# The associated data tibble abstraction: 19 × 5\n# The 'node', 'label' and 'isTip' are from the phylo tree.\n    node label isTip  size habitat   \n   &lt;int&gt; &lt;chr&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;chr&gt;     \n 1     1 t3    TRUE  15.3  intertidal\n 2     2 t7    TRUE  21.1  forest    \n 3     3 t9    TRUE  19.5  grassland \n 4     4 t5    TRUE  13.7  grassland \n 5     5 t13   TRUE  11.3  desert    \n 6     6 t15   TRUE  14.2  intertidal\n 7     7 t17   TRUE   7.39 desert    \n 8     8 t11   TRUE  17.8  intertidal\n 9     9 t19   TRUE  14.0  forest    \n10    10 t1    TRUE  17.9  grassland \n# … with 9 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\nggtree(smalltree) + geom_tiplab()\n\n\n\n\ndrop.tip keeps all of the metadata! keep.tip is imported from ape so it has to be converted to phylo and then the data joined again after."
  },
  {
    "objectID": "posts/2023-04-25-ggtree/index.html#plotting-with-node-labels",
    "href": "posts/2023-04-25-ggtree/index.html#plotting-with-node-labels",
    "title": "The ggtree-verse",
    "section": "Plotting with node labels",
    "text": "Plotting with node labels\nThe geometries geom_text() and geom_node() are helpful for labelling all of the nodes. The function geom_tiplab() labels only the tips.\nAdd node labels so you know what the internal node numbers are:\n\nggtree(smalltree) + \n    geom_tiplab() +\n    geom_text(aes(label=node), hjust=-.3)    # node numbers\n\n\n\n\nNote: The tiplabels and the node labels crashed!\nThere are also 2 versions: geom_text2() and geom_node2() that allow subsetting the nodes, when you want the geometry to apply to only some of the nodes.\n\nggtree(smalltree) + \n    geom_tiplab() +\n    geom_text2(aes(label=node, subset=!isTip), hjust=-.3)    # node numbers\n\n\n\n\nisTip is a column of the ggtree object, so it is inherited when we provide the ggtree object."
  },
  {
    "objectID": "posts/2023-04-25-ggtree/index.html#plotting-with-alternative-tip-labels",
    "href": "posts/2023-04-25-ggtree/index.html#plotting-with-alternative-tip-labels",
    "title": "The ggtree-verse",
    "section": "Plotting with alternative tip labels",
    "text": "Plotting with alternative tip labels\nThe dataframe portion of the treedata object can hold any number of columns of metadata. Perhaps you have some real names in a different column (like a display name), it is easy to swap out the tip labels. Here letʻs just use the habitat column\n\nggtree(smalltree) + \n    geom_tiplab(aes(label=habitat)) +\n    geom_text2(aes(label=node, subset=!isTip), hjust=-.3)    # node numbers"
  },
  {
    "objectID": "posts/2023-04-25-ggtree/index.html#when-your-tip-labels-get-cut-off",
    "href": "posts/2023-04-25-ggtree/index.html#when-your-tip-labels-get-cut-off",
    "title": "The ggtree-verse",
    "section": "When your tip labels get cut off",
    "text": "When your tip labels get cut off\nAdd an x scale (usually time):\n\nggtree(smalltree) + \n    geom_tiplab(aes(label=habitat)) +\n    geom_text2(aes(label=node, subset=!isTip), hjust=-.3) +   # node numbers\n    theme_tree2()\n\n\n\n\nYou can increase the size of the plot area to accommodate the longer labels:\n\nggtree(smalltree) + \n    geom_tiplab(aes(label=habitat)) +\n    geom_text2(aes(label=node, subset=!isTip), hjust=-.3) +   # node numbers\n    theme_tree2() +\n    xlim(0,5)"
  },
  {
    "objectID": "posts/2023-04-25-ggtree/index.html#example-plot-smalltree-with-size-in-a-barplot",
    "href": "posts/2023-04-25-ggtree/index.html#example-plot-smalltree-with-size-in-a-barplot",
    "title": "The ggtree-verse",
    "section": "Example: plot smalltree with size in a barplot",
    "text": "Example: plot smalltree with size in a barplot\nFirst make a tibble to attach to the tree. As of this writing, geom_facet will not accept a treedata object. It wants a dataframe or tibble of only the tips. But this is easy to make from the treedata. We just have to filter out the non-tip rows, then rearrange the columns to put the labels first:\n\nrequire(ggplot2)\nrequire(dplyr)\n\nsmdat &lt;- smalltree %&gt;% \n           as_tibble %&gt;% \n           filter(!is.na(label)) %&gt;%\n           relocate(label, .before=1) \n\nWe can then add the barplot as a panel next to the tree plot using geom_facet:\n\nggtree(smalltree) + \n    geom_tiplab() +\n    theme_tree2()  + \n    geom_facet(panel = \"Size\", \n               data=smdat, \n               geom = geom_col, \n               mapping=aes(x = smdat$size), \n               orientation = 'y', \n               width = .6, \n               fill=\"blue\") \n\n\n\n\nThe arguments for geom_facet() are:\n\npanel : The name of the panel, displayed on top\ndata : a tibble or dataframe containing the metadata to plot. Must have as the first column the tip labels that are found in the phylogenetic tree.\ngeom : a geom layer specifying the style of plot\nmapping : the aesthetic mapping. I should not have to supply the smdat$ here but it wonʻt work otherwise.\nany additional parameters for the plot"
  },
  {
    "objectID": "posts/2023-04-25-ggtree/index.html#the-operator-for-ggtree-objects",
    "href": "posts/2023-04-25-ggtree/index.html#the-operator-for-ggtree-objects",
    "title": "The ggtree-verse",
    "section": "The %+>% operator for ggtree objects",
    "text": "The %+&gt;% operator for ggtree objects\nThe %+&gt;% operator is used to add data (dataframe, tibble) to a ggtree object:\nmy_ggtree &lt;- my_ggtree %&lt;+% new_data\nThe result is a combined object that can be used for plotting, but it does not modify the original treedata object (which is a different object from the ggtree object). The full_join() function can be used to combine a tree with data to produce a new treedata object."
  },
  {
    "objectID": "posts/2023-04-25-ggtree/index.html#example-of-the-operator-to-add-data-to-a-ggtree-object.",
    "href": "posts/2023-04-25-ggtree/index.html#example-of-the-operator-to-add-data-to-a-ggtree-object.",
    "title": "The ggtree-verse",
    "section": "Example of the %+>% operator to add data to a ggtree object.",
    "text": "Example of the %+&gt;% operator to add data to a ggtree object.\nThe package TDbook is the data accompanyment to (Yu 2022)ʻs Tree Data book. It is available on CRAN so you can install it with the usual install.packages(\"TDbook\") function call.\n\nrequire(ggtree)\nrequire(ggplot2)\nrequire(dplyr)\nrequire(TDbook)\n\n# load `tree_boots`, `df_tip_data`, and `df_inode_data` from 'TDbook'\n\n\np &lt;- ggtree(tree_boots) %&lt;+% df_tip_data + xlim(-.1, 4)\np2 &lt;- p + geom_tiplab(offset = .6, hjust = .5) +\n    geom_tippoint(aes(shape = trophic_habit, color = trophic_habit, \n                size = mass_in_kg)) + \n    theme(legend.position = \"right\") + \n    scale_size_continuous(range = c(3, 10))\n\np2 %&lt;+% df_inode_data + \n    geom_label(aes(label = vernacularName.y, fill = posterior)) + \n    scale_fill_gradientn(colors = RColorBrewer::brewer.pal(3, \"YlGnBu\"))\n\nWarning: Removed 7 rows containing missing values (`geom_label()`).\n\n\n\n\n\n\nExplore df_info\ndf_info A dataframe containing sampling info for the tips of the tree. 386 rows and 6 variables, with the first column being taxa labels (id).\ndf_alleles The allele table with original raw data to be processed to SNP data. It is a table of nucleotides with 386 rows x 385 variables. The first row contains tips labels. Column names are non-sense. The rownames (exept for the first one) contains the snp position along the genome.\n\n## load `tree_nwk`, `df_info`, `df_alleles`, and `df_bar_data` from 'TDbook'\ntree &lt;- tree_nwk\nsnps &lt;- df_alleles\nsnps_strainCols &lt;- snps[1,] \nsnps&lt;-snps[-1,] # drop strain names\ncolnames(snps) &lt;- snps_strainCols\n\ngapChar &lt;- \"?\"\nsnp &lt;- t(snps)  # not rectangular!\nlsnp &lt;- apply(snp, 1, function(x) {\n        x != snp[1,] & x != gapChar & snp[1,] != gapChar\n    })  # different from row 1, not missing\nlsnp &lt;- as.data.frame(lsnp) \nlsnp$pos &lt;- as.numeric(rownames(lsnp))  # position from rownames\nlsnp &lt;- tidyr::gather(lsnp, name, value, -pos)  \nsnp_data &lt;- lsnp[lsnp$value, c(\"name\", \"pos\")] # only TRUEs\n\nsnp_data A dataframe containing SNP position data. 6482 x 2. The first column contains taxa labels coresponding to the tips of the tree (name). There are multiple rows per taxon, the second colum is the position pos of the snp in the genome. This is used as the x-variable in the plot.\nIn the dataframe snp_data the rows are ordered by position along the sequence (the x-dimension of this data), but the first column is the strain (taxon) name which matches the tips in the phylogenetic tree.\n\n## visualize the tree \np &lt;- ggtree(tree) \n\n## attach the sampling information data set \n## and add symbols colored by location\np &lt;- p %&lt;+% df_info + geom_tippoint(aes(color=location))\np\n\n\n\n\n\n\nAdd SNP and Trait plots aligned to the tree\nUse geom_facet with reference to the respective dataframes/tibbles to add plots alignted to the tree. For the SNP plot, we will use geom_point which allows x-y plotting, with x-coordinate determined by pos and the y-coordinate aligned with the taxon. The symbol is the vertical line |.\n\n## visualize SNP and Trait data using dot and bar charts,\n## and align them based on tree structure\np1 &lt;- p + geom_facet(panel = \"SNP\", data = snp_data, geom = geom_point, \n               mapping=aes(x = pos, color = location), shape = '|') \np1\n\n\n\n\ndf_bar_data is some simulated data with an id column specifying the taxon names, and a dummy_bar_value containing some data.\n\np1 + geom_facet(panel = \"Trait\", data = df_bar_data, geom = geom_col, \n                aes(x = dummy_bar_value, color = location, \n                fill = location), orientation = 'y', width = .6) +\n    theme_tree2(legend.position=c(.05, .85))"
  },
  {
    "objectID": "posts/2023-04-04-morphometric-workflow/index.html",
    "href": "posts/2023-04-04-morphometric-workflow/index.html",
    "title": "Morphometrics Overview",
    "section": "",
    "text": "Acknowledgements\nMaterial for this lecture was borrowed and adopted from\n\nhttps://digitalcommons.pace.edu/cgi/viewcontent.cgi?article=1003&context=oer\nEmma Sherratʻs quick guide to Geomorph\n\n\n\nLearning objectives\n\n\n\n\n\n\nLearning objectives\n\n\n\nAt the end of this lesson you will:\n\nUnderstand the overview of a morphometrics analysis\n\n\n\n\n\nOverview\n\n\nMorphometric Workflow\n\n\n\n\n\n\nTypical Morphometric Worflow\n\n\n\n\nAsk interesting question, develop hypotheses, collect specimens\nTake photos/images of the specimens\nPlace landmarks on the specimens in the photos at key locations. Choose landmarks that are:\nPresent on all specimens\nRelevant to question\nClearly defined\nAnalyze how the landmarks vary in relation to each other among specimens\nPerform statistical analyses to test for significant differences in body shape among populations, species, or whatever aspect that is relevant to your hypotheses\nPlot components of variation in shape that are not correlated with each other (i.e. the principal component scores).\n\n\n\n\n\nTypes of Questions that Morphometrics can Answer (not exhaustive)\n\nRelationship of parts to other parts.\nAllometry - relationship of shape to size\nComparison among groups\nCovariation of shape with other factors\nCovariation of morphology with phylogeny\nMorpholgical integration\nModularity"
  },
  {
    "objectID": "posts/2023-01-19-your-computer-filesystems/index.html",
    "href": "posts/2023-01-19-your-computer-filesystems/index.html",
    "title": "Introduction to your computerʻs terminal utilities",
    "section": "",
    "text": "Read ahead\n\n\n\nFor future lectures, Iʻll give you some reading or podcasts to prepare\n\n\n\n\n\nMaterial for this lecture was borrowed and adopted from\n\nhttps://academind.com/tutorials/terminal-zsh-basics"
  },
  {
    "objectID": "posts/2023-01-19-your-computer-filesystems/index.html#the-kernel",
    "href": "posts/2023-01-19-your-computer-filesystems/index.html#the-kernel",
    "title": "Introduction to your computerʻs terminal utilities",
    "section": "The kernel",
    "text": "The kernel\nThe kernel is the part of your computerʻs operating system that loads first once you start up. It is kind of like your computerʻs autonomic nervous system. It recognizes all of the physical hardware attached to it, enables communication between components (and device drivers), and monitors maintenance functions like turning on the fan when it gets hot, manages virtual memory, gives warnings when the hard drive gets full, manages multitasking, and manages security and file permissions. In the mac this is the XNU kernel, in modern Windows machines it is the Windows NT kernel.\n\n\n\n\n\n[Source: Map of MacOS: the heart of everything is called Darwin; and within it, we have separate system utilities (the shell) and the XNU kernel, which is composed in parts by the Mach kernel and by the BSD kernel.]"
  },
  {
    "objectID": "posts/2023-01-19-your-computer-filesystems/index.html#the-shell",
    "href": "posts/2023-01-19-your-computer-filesystems/index.html#the-shell",
    "title": "Introduction to your computerʻs terminal utilities",
    "section": "The shell",
    "text": "The shell\nThe shell is another key part of the core operating system (note in the diagram above it is part of the System Utilities, and the partner of the kernel). The shell is a software (an app) that allows humans to control the computer. You are already familiar with the GUI interface, or the Graphical User Interface. It is important that you are comfortable using the Command Line Interface as well.\n\n\n\n\n\n\nThere are many reasons to be proficient in the shell:\n\n\n\n\nData analysis increasingly uses many files. The shell provides a simple but very powerful means to do all kinds of operations on files: move, delete, organize, combine, rename, etc.\nUsing the shell encourages you to understand your computerʻs filesystem, and helps you to more precisely control input and output to any place along your file paths.\nShell operations are fast.\nYou can use wildcards to control matching or excluding many files.\nThe shell can be used to execute (run) software.\nThe shell is probably the oldest app, so it is very stable with lasting power.\nIt is part of the OS, so when your apps fail or you are having some issues, you would turn to the shell to kill troublesome processes (programs) or diagnose and fix the issues.\n\n\n\nMacs use the same terminal utilities as UNIX/Linux systems. On the Mac, the command line interface app is called Terminal, which you will find in your Application folder, in the Utilities subfolder (here is a screentshot of our GUI Interface).\n\n\n\n\n\nOn a PC you would use the Command Prompt otherwise known as the Windows Command Processor or CMD. If you used a pre-Windows machine, you would be familiar with MS-DOS. To open CMD:\n\nOpen the Start Menu and type “command prompt” or\nPress Win + R and type “cmd” in the run box or\nPress Win + X and select Command Prompt from the menu.\n\nNote: you may see Windows PowerShell or Windows Terminal instead, these are similar apps."
  },
  {
    "objectID": "posts/2023-01-19-your-computer-filesystems/index.html#the-dots",
    "href": "posts/2023-01-19-your-computer-filesystems/index.html#the-dots",
    "title": "Introduction to your computerʻs terminal utilities",
    "section": "The dots",
    "text": "The dots\n\n“.” is the current working directory (where you are currently)\n“..” is the directory one level up\n“./foldername” will take you to the folder one level down, for example “./Data”\n\nYou can use these paths to move directories using cd or to list the contents of the directories using ls or to make new directories using mkdir\nls .\nls ./Data\nmkdir ./Data/A\nMake multiple directories:\nmkdir ./Data/B ./Data/C\nList the files one level up or two levels up:\nls ..  # for PC use dir ..\nls ../..\nUp one level, and over to another directory:\nls ../AnotherDirectory\nYou can wander anywhere along your computerʻs file directory! Just add more steps to the path."
  },
  {
    "objectID": "posts/2023-01-19-your-computer-filesystems/index.html#rtistry",
    "href": "posts/2023-01-19-your-computer-filesystems/index.html#rtistry",
    "title": "Introduction to your computerʻs terminal utilities",
    "section": "rtistry",
    "text": "rtistry\n\n\n\n\n\n[‘Flametree’ from Danielle Navarro https://art.djnavarro.net]"
  },
  {
    "objectID": "posts/2023-03-28-program-flow/index.html",
    "href": "posts/2023-03-28-program-flow/index.html",
    "title": "Program Flow",
    "section": "",
    "text": "Material for this lecture was borrowed and adopted from\n\nhttps://www.stephaniehicks.com/jhustatcomputing2022/posts/2022-09-22-control-structures"
  },
  {
    "objectID": "posts/2023-03-28-program-flow/index.html#if-else",
    "href": "posts/2023-03-28-program-flow/index.html#if-else",
    "title": "Program Flow",
    "section": "if-else",
    "text": "if-else\nif-else is the most commonly used conditional statement in programming. If a condition is true, a statement is executed:\n\nif-else comes in many flavors:\n\nJust if\nIf the condition is TRUE, execution happens. If FALSE, nothing happens:\nif(&lt;condition&gt;) {\n        ## do something\n} \n## Continue with rest of code\n\n\nif-else\nIf-else allows for a different action when the condition is false:\nif(&lt;condition&gt;) {\n        ## do something\n} \nelse {\n        ## do something else\n}\nif and else can be daisy-chained:\n\n\nif-else if-else if-else etc.\nYou can have a series of tests, which will stop and execute the statement at the condition that is TRUE. (Everything following will be ignored):\nif(&lt;condition1&gt;) {\n        ## do something\n} else if(&lt;condition2&gt;)  { \n        ## do something different\n} else if(&lt;condition2&gt;) {\n        ## do something different\n} else { \n        ## do this if none of the above is true\n}        \n\nExample: Draw a random value between zero and 10, then test for values greater than 3.\n\nx &lt;- runif(n=1, min=0, max=10)  \nx\n\n[1] 9.765364\n\nif(x &gt; 6) {\n    y &lt;- 10\n  } else if (x &gt; 3){\n    y &lt;- 5\n  } else { y &lt;- 0 }\nx\n\n[1] 9.765364\n\ny\n\n[1] 10\n\n\nWith the if-else structure we can test multiple conditions on the same variable. Here, three ranges of values for x. Of course, the else conditional is not necessary. You could just have a string of ifs:\nif(&lt;condition1&gt;) {\n\n}\n\nif(&lt;condition2&gt;) {\n\n}\nAs long as your logic is sound (and you have them in the right order), you could be OK.\n\n\n\n\n\n\nNote-TESTING\n\n\n\nNOTE it is always important to TEST your code against several datasets for which you can verify the answers. Be sure to try cases where your code might get tripped up. Anticipating the errors will have you a lot of headache."
  },
  {
    "objectID": "posts/2023-03-28-program-flow/index.html#while-loops",
    "href": "posts/2023-03-28-program-flow/index.html#while-loops",
    "title": "Program Flow",
    "section": "while Loops",
    "text": "while Loops\nwhile loops begin by testing a condition.\n\nIf it is TRUE, then they execute the code contained within the loop.\n\nIf FALSE the loop is exited (no execution).\n\n\n\nExample:\n\ncount &lt;- 0\nwhile(count &lt; 10) {\n        print(count)\n        count &lt;- count + 1\n}\n\n[1] 0\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n\n\nwhile loops can potentially result in infinite loops if not written properly. Use with care!\nSometimes there will be more than one condition in the test.\n\nz &lt;- 5\nset.seed(1)\n\nwhile(z &gt;= 3 && z &lt;= 10) {\n        coin &lt;- rbinom(1, 1, 0.5)\n        \n        if(coin == 1) {  ## random walk\n                z &lt;- z + 1\n        } else {\n                z &lt;- z - 1\n        } \n}\nprint(z)\n\n[1] 2\n\n\n\n\n\n\n\n\nPro-tip\n\n\n\n& is the logical and - both conditions must be true in order to return TRUE\n| is the logical or - only one condition must be true in order to return TRUE\nWhat’s the difference between using one & or two && (or one | vs. ||)?\nIf you use only one &, these are vectorized operations, meaning they will evaluate the logical conditional on the vector, and can return a vector, like this:\n\n-2:2\n\n[1] -2 -1  0  1  2\n\n((-2:2) &gt;= 0) & ((-2:2) &lt;= 0)\n\n[1] FALSE FALSE  TRUE FALSE FALSE\n\n\nIf you use two &&, then these are operations on single values.\n\n2 &gt;= 0\n\n[1] TRUE\n\n(2 &gt;= 0) && (-2 &lt;= 0)\n\n[1] TRUE\n\n(-2 &gt;= 0) && (-2 &lt;= 0)\n\n[1] FALSE"
  },
  {
    "objectID": "posts/2023-03-28-program-flow/index.html#repeat-loops",
    "href": "posts/2023-03-28-program-flow/index.html#repeat-loops",
    "title": "Program Flow",
    "section": "repeat Loops",
    "text": "repeat Loops\nrepeat initiates an infinite loop right from the start. These are not commonly used in statistical or data analysis applications, but they do have their uses.\n\n\n\n\n\n\nIMPORTANT (READ THIS AND DON’T FORGET… I’M SERIOUS… YOU WANT TO REMEMBER THIS.. FOR REALZ PLZ REMEMBER THIS)\n\n\n\nThe only way to exit a repeat loop is to call break.\n\n\nOne possible paradigm might be in an iterative algorithm where you may be searching for a solution and you do not want to stop until you are close enough to the solution.\nIn this kind of situation, you often don’t know in advance how many iterations it’s going to take to get “close enough” to the solution.\n\nx0 &lt;- 1\ntol &lt;- 1e-8\n\nrepeat {\n        x1 &lt;- computeEstimate()\n        \n        if(abs(x1 - x0) &lt; tol) {  ## Close enough?\n                break\n        } else {\n                x0 &lt;- x1\n        } \n}\n\n\n\n\n\n\n\nNote\n\n\n\nThe above code will not run if the computeEstimate() function is not defined (I just made it up for the purposes of this demonstration).\n\n\n\n\n\n\n\n\nPro-tip\n\n\n\nThe loop above is a bit dangerous because there is no guarantee it will stop.\nYou could get in a situation where the values of x0 and x1 oscillate back and forth and never converge.\nBetter to set a hard limit on the number of iterations by using a for loop and then report whether convergence was achieved or not."
  },
  {
    "objectID": "posts/2023-03-28-program-flow/index.html#next-break",
    "href": "posts/2023-03-28-program-flow/index.html#next-break",
    "title": "Program Flow",
    "section": "next, break",
    "text": "next, break\nnext is used to skip an iteration of a loop.\n\nfor(i in 1:100) {\n        if(i &lt;= 20) {\n                ## Skip the first 20 iterations\n                next                 \n        }\n        ## Do something here\n}\n\nbreak is used to exit a loop immediately, regardless of what iteration the loop may be on.\n\nfor(i in 1:100) {\n      print(i)\n\n      if(i &gt; 20) {\n              ## Stop loop after 20 iterations\n              break  \n      }   \n}\n\n\nAnother example\nBoth flowcharts and pseudocode can help to diagram the logic and modularity of the code:"
  },
  {
    "objectID": "posts/2023-01-24-intro-git-github/index.html",
    "href": "posts/2023-01-24-intro-git-github/index.html",
    "title": "Introduction to git/GitHub",
    "section": "",
    "text": "Helpful references for this lecture\n\n\n\n\nHappy Git with R from Jenny Bryan\nChapter on git and GitHub in dsbook from Rafael Irizarry\nGitHub introduction in module 1 from Andreas Handel\n\n\n\n\n\nMaterial for this lecture was borrowed and adopted from\n\nhttps://www.stephaniehicks.com/jhustatcomputing2022/\nhttps://andreashandel.github.io/MADAcourse/Tools_Github_Introduction.html"
  },
  {
    "objectID": "posts/2023-01-24-intro-git-github/index.html#acknowledgements",
    "href": "posts/2023-01-24-intro-git-github/index.html#acknowledgements",
    "title": "Introduction to git/GitHub",
    "section": "",
    "text": "Material for this lecture was borrowed and adopted from\n\nhttps://www.stephaniehicks.com/jhustatcomputing2022/\nhttps://andreashandel.github.io/MADAcourse/Tools_Github_Introduction.html"
  },
  {
    "objectID": "posts/2023-01-24-intro-git-github/index.html#git",
    "href": "posts/2023-01-24-intro-git-github/index.html#git",
    "title": "Introduction to git/GitHub",
    "section": "git",
    "text": "git\nGit is software that implements what is called a version control system for a repository of files (also known as a repo). The main idea is that as you (and your collaborators) work on a project, the git software tracks, and records any changes made by anyone.\nGitHub is an online server and user interface that provides powerful tools for distribution of your repository, bug tracking, collaboration, and also allows you to create easy websites for each repository.\nGit and GitHub together provide an organized way to track your projects, and all of the tools you will need in this course are free.\n\n\n\n\n\nFigure 1. Whether working solo (A) or collaborating in a group (B) version tracking by naming files is a mess when you want to retrace the steps of the analysis (C). Git and GitHub track all changes and the complete branching tree of the project (D). The commit history is a powerful tool to retrace the development of the project or can be used to roll back to any prior version.\n\n\n\n\n[Source: Jenny Bryan]\nIt is very well suited for collaborative work, as it was developed by Linus Torvalds (in about 10 days of coding!) for collaborative software development of the Linux kernel pretty interesting interview with Torvalds. What it did really well was distributed control, and allowing everyone to have their own copy of the repository.\nGit/GitHub is now the dominant version control system with GitHub hosting over 200 million repositories worldwide! It is used very broadly for all kinds of repos now including data science projects, book projects, courses, and anything that needs collaborative management of mostly text files.\nAnother great and fun read about Git/GitHub and why it is a great tool for data analysis is in this article by Jenny Bryan."
  },
  {
    "objectID": "posts/2023-01-24-intro-git-github/index.html#what-to-not-use-gitgithub-for",
    "href": "posts/2023-01-24-intro-git-github/index.html#what-to-not-use-gitgithub-for",
    "title": "Introduction to git/GitHub",
    "section": "What to (not) use Git/GitHub for",
    "text": "What to (not) use Git/GitHub for\nGitHub is ideal if you have a project with (possibly many) smallish files, and most of those files are text files (such as R code, LaTeX, Quarto/(R)Markdown, etc.) and different people work on different parts of the project.\nGitHub is less useful if you have a lot of non-text files (e.g. Word or PowerPoint) and different team members might want to edit the same document at the same time. In that instance, a solution like Google Docs, Word+Dropbox, Word+Onedrive, etc. might be better.\nGitHub also has a problem with large files. Anything above around 50MB can lead to very slow syncing and sometimes outright failure. Unfortunately, once GitHub has choked on a large file, it can be quite tricky to fix the repository to work again. This is because the ENTIRE history is saved, including the addition of the huge file. Therefore keep large (&gt;50MB) files out of your GitHub repositories. If you have to work with such files, try to reduce them first before placing into the GitHub repository. Or as alternative, place those files in another sync service (e.g. Dropbox, OneDrive, GoogleDrive) and load them from there.\nFinally, if you have data, you need to be careful since by default, GitHub repositories are public. You can set the repository to private, but you need to be careful that you don’t accidentally expose confidential data to the public. It is in general not a good idea to have confidential data on GitHub. First anonymize your data (ensure that it is not at risk of identifiability), then you can place it in a private repository. If you put it in a public repo, be very careful!! (And you may need IRB approval, check with your institutional research office.)\n\n\n\n\n\n\nTip\n\n\n\n\nGit/GitHub has version control features like a turbo-charged version of “track changes” but more rigorous, powerful, and scaled up to multiple files\nGreat for solo or collaborative work\nSaves the entire history of every change made\nAllows for multiple verisions or “branches” to be developed and later merged\nGitHub allows distributed collaboration (potentially among complete strangers) and has greatly promoted open-source software development, collaboration, distribution, and bug/issue tracking for users to get help\nGitHub allows webpages for your projects or repositories\n\n\n\nNote that other interfaces to Git exist, e.g., Bitbucket, but GitHub is the most widely used one."
  },
  {
    "objectID": "posts/2023-01-24-intro-git-github/index.html#why-use-gitgithub",
    "href": "posts/2023-01-24-intro-git-github/index.html#why-use-gitgithub",
    "title": "Introduction to git/GitHub",
    "section": "Why use git/GitHub?",
    "text": "Why use git/GitHub?\nYou want to use GitHub to avoid this:\n\n\n\n\n\nHow not to use GitHub image from PhD Comics\n\n\n\n\nTo learn a bit more about Git/GitHub and why you might want to use it, read this article by Jenny Bryan.\nNote her explanation of what’s special with the README.md file on GitHub."
  },
  {
    "objectID": "posts/2023-01-24-intro-git-github/index.html#how-to-use-gitgithub",
    "href": "posts/2023-01-24-intro-git-github/index.html#how-to-use-gitgithub",
    "title": "Introduction to git/GitHub",
    "section": "How to use Git/GitHub",
    "text": "How to use Git/GitHub\nGit/GitHub is fundamentally based on commands you type into the command line. Lots of online resources show you how to use the command line. This is the most powerful, and the way I almost always interact with git/GitHub. However, many folks find this the most confusing way to use git/GitHub. Alternatively, there are graphical interfaces.\n\nGitHub itself provides a grapical interface with basic functionality.\nRStudio also has Git/GitHub integration. Of course this only works for R project GitHub integration.\nThere are also third party GitHub clients with many advanced features, most of which you won’t need initially, but might eventually.\n\nNote: As student, you can (and should) upgrade to the Pro version of GitHub for free (i.e. access to unlimited private repositories is one benefit), see the GitHub student developer pack on how to do this."
  },
  {
    "objectID": "posts/2023-01-24-intro-git-github/index.html#sec-profile",
    "href": "posts/2023-01-24-intro-git-github/index.html#sec-profile",
    "title": "Introduction to git/GitHub",
    "section": "Set up your profile in git on your computer",
    "text": "Set up your profile in git on your computer\nBefore making changes to your repository, GitHub will want to verify your identity.\nIn order for your computer to talk to GitHub smoothly, you will need to set up your username and email in the git profile stored on your computer.\n\n\n\n\n\n\nWarning\n\n\n\nBe sure to match your GitHub account username and email! Otherwise GitHub wonʻt know who you are\n\n\n\n\nTerminal\n\ngit config --global user.name 'GitHubUsername'\ngit config --global user.email 'GitHub_email@example.com'\ngit config --global --list\n\nThat last line will show all of your current git config settings.\nIf you are using Rstudio, easy directions are provided here https://happygitwithr.com/hello-git.html"
  },
  {
    "objectID": "posts/2023-01-24-intro-git-github/index.html#sec-token",
    "href": "posts/2023-01-24-intro-git-github/index.html#sec-token",
    "title": "Introduction to git/GitHub",
    "section": "Set up your Personal Authentication Token on your computer",
    "text": "Set up your Personal Authentication Token on your computer\nGitHub will also want to check your credentials to authenticate you are really you before writing changes to your repo.\nThere are several ways to do this, but the easiest is the protocol for HTTP authentication. You will generate a Personal Access Token for HTTPS from your GitHub account which will be stored on your personal machine.\nI prefer the GithHub command line interface or gh to do this. To install the CLI, follow the instructions here for your operating system. For Mac users, I suggest that you install homebrew, it is a command-line general software manager for many different software packages.\n\n\n\n\n\n\nStoring your Personal Access Token\n\n\n\n\nFrom GitHub: Generate your personal access token instructions\n\nFind the token generator on GitHub under your User Icon &gt; settings &gt; developer settings (left side bar) &gt; Personal access tokens &gt; Tokens (classic) &gt; Generate new Token &gt; classic note this menu may change\nSelect at least these scopes: “admin:org”,“repo”, “user”, “gist”, and “workflow”\n\nFrom your Command Line: Use gh auth login to store your token and follow the prompts.\n\nselect HTTPS for your preferred protocol\nselect Y for authenticate with GitHub credentials\nAlternatively if you want to do this all from the command line you can run the following line (if your token is saved in mytoken.txt):\n\n\n\nTerminal\n\ngh auth login --with-token &lt; mytoken.txt\n\n\n\n\nMany more details are explained nicely here https://happygitwithr.com/https-pat.html\nYou only have to store your credentials once for each computer (or PAT expiration date), then you can push and pull from GitHub to your heartʻs content. It really is a nice way to do things securely."
  },
  {
    "objectID": "posts/2023-01-24-intro-git-github/index.html#configuring-your-default-git-editor",
    "href": "posts/2023-01-24-intro-git-github/index.html#configuring-your-default-git-editor",
    "title": "Introduction to git/GitHub",
    "section": "Configuring your default git editor",
    "text": "Configuring your default git editor\nYou may want to set your default git editor to something you know how to use (it will come up when you have a merge conflict).\nFor example the nano editor is easy to use on the command line for a Unix shell:\n\n\nMac/Linux\n\n$ git config --global core.editor \"nano -w\"\n\nThe Carpentries provide a full list of editors by operating system, a great resource."
  },
  {
    "objectID": "posts/2023-02-09-data/index.html",
    "href": "posts/2023-02-09-data/index.html",
    "title": "Types of Data",
    "section": "",
    "text": "✏️"
  },
  {
    "objectID": "posts/2023-02-09-data/index.html#acknowledgements",
    "href": "posts/2023-02-09-data/index.html#acknowledgements",
    "title": "Types of Data",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nMaterial for this lecture was borrowed and adopted from\n\nhttps://andreashandel.github.io/MADAcourse/Data_Types.html\nhttps://r-coder.com/data-types-r/#Raw_data_type_in_R\nhttps://www.stat.auckland.ac.nz/~paul/ItDT/HTML/node76.html # Learning objectives\n\n\n\n\n\n\n\nLearning objectives\n\n\n\nAt the end of this lesson you will:\n\nUnderstand different types of data and how they are represented computationally\nUnderstand that different data types require different analysis approaches\nRecognize different base data types in R and know how to work with them\nRecognize the base data structures or objects in R and how to use them to do what you want"
  },
  {
    "objectID": "posts/2023-02-09-data/index.html#basic-data-types",
    "href": "posts/2023-02-09-data/index.html#basic-data-types",
    "title": "Types of Data",
    "section": "Basic data types",
    "text": "Basic data types\n\n\n\n\n\n\nR has six basic (atomic) types of data:\n\n\n\n\n\n\nAtomic Type\nShort Description\nSize in bytes\n\n\n\n\nstring (or character)\ntext\n1 (per character)\n\n\ninteger\ncountable numbers\n4\n\n\nreal\nreal numbers\n8\n\n\nlogical\nTRUE or FALSE\n4\n\n\ncomplex\nnumbers with imaginary component\n\n\n\nraw\nraw bytes\n1\n\n\n\nAll other data types are derived from the atomic types\n\n\nNote: Most computers use 64-bit operating systems these days, so the sizes above are for 64-bit software.\nFor a quick tour of the data types, see https://r-coder.com/data-types-r\nString/character: Character values are alphanumeric values (plus whitespace, punctuation, etc.). A string is a collection of characters, in other words “text”.\n\nstrings can be pasted together using the paste() function.\nR has powerful tools for string manipulation, including searching, replacing, and customized partial matching (with or without replacement) using wildcards and perl-like regular expressions (or regex) using base functions such as\n\ngrep()\nsub()\ngsub()\nsubstr()\n\nThere are also packages specific for string manipulation including the stringr package which is part of the tidyverse.\n\nIt is very likely that you will need to work with strings at some point during a data analysis, even if it is only to find specific values, clean up variable names, etc.\nThese problems can be quite the headache! But instead of editing them by hand and possibly making an error, it is better to do this with code. It also makes it easier to keep a record of the original data and all of the changes made to it, improving the reproducibility of your analysis.\nThere is a learning curve to using these tools, especially regex syntax, but they are very powerful and well worth your time.\n\n\n\n\n\n\nGood sources for practice manipulating strings:\n\n\n\n\nFor beginners: Review the Strings chapter (14) of R4DS, and do the exercises.\nThe string processing chapter (25) of IDS\nthe Character Vectors chapter in the STAT 545 book by Jenny Bryan\n\nDecide which one is right for your level and work through some examples. I think youʻll agree that it is worth your time.\n\n\nNumeric (double or integer): Variables of type numeric in R are either integers or double precision (representing real numbers).\n\nIntegers and real values are different, but in practice most R users donʻt pay attention to this distinction. Integer values tend to be coerced (converted) to real values if any mathematical operations are done to them.\nIf an integer is explicity needed, you can create them using functions such as as.integer().\nNote that when you type an integer value, e.g. x &lt;- 2, into R, this is considered numeric by default.\nIf you want to make sure a value is treated as integer, add an L, e.g. x &lt;- 2L.\n\nLogical: Logical variables are binary and can take on only two values, TRUE or FALSE (which are reserved words that only take on these meanings in R).\n\nIn R, logical values are treated as integers, and interpreted as 1 for TRUE and 0 for FALSE. It is possible to sum(TRUE) or a vector of logicals, for example.\n\nR also understands T, True, and true for TRUE, and the corresponding representations for FALSE.\nImportantly, logical comparisons are used for indexing. You will use logical comparisons when cleaning and checking your data, or running analyses, e.g., if you want to see if your variable x is greater than 5, then the R command x &gt; 5 will return either TRUE or FALSE, based on the value of x.\nNote: reserved words are understood as constants and should not be “quoted”."
  },
  {
    "objectID": "posts/2023-02-09-data/index.html#derived-data-types",
    "href": "posts/2023-02-09-data/index.html#derived-data-types",
    "title": "Types of Data",
    "section": "Derived data types",
    "text": "Derived data types\nR also allows derived data types called classes that are built up from atomic data types. There are R base classes as well as new classes that can be defined as needed by programmers (maybe you?).\nFactors: Are Rʻs class for categorical variables.\n\nFactors have names and values.\nFor example, a size factor may have names (or levels) of small, medium. and large with values 0,1,2. Here, the values simply indicate the different categories, with the names being the human-friendly labels for the values.\n\nFactors can be ordered/ordinal or not.\nFactors could be numeric values, e.g., the number of offspring.\nOr it could be a factor coding for 3 types of habitat (unordered),\nOr 3 levels of life history stage (ordered).\nAn excellent package to work with factors is the forcats package.\n\nFor more about factors, work through the Factors chapter of R4DS, and do the exercises.\nDate/time: Dates in base R are of the class Date (and are called POSIX variables). The lubridate package is a tidyverse package to work with dates, which many people find easier. There are other packages as well.\nAdditional resources are the Dates and times chapter of R4DS and the Parsing Dates and Times chapter of IDS.\nProgrammer-defined classes Many packages define their own classes. For example class phylo is used to represent phylogenetic trees in the ape package.\n\nThere are several functions that can show you the data type of an R object\nsuch as typeof(), mode(), storage.mode(), class() and str()."
  },
  {
    "objectID": "posts/2023-02-09-data/index.html#other-derived-data-types",
    "href": "posts/2023-02-09-data/index.html#other-derived-data-types",
    "title": "Types of Data",
    "section": "Other derived data types",
    "text": "Other derived data types\nTimeseries: A very useful set of tools for times-series analysis in R is the set of packages called the tidyverts. CRAN also has a Task View for Time Series Analysis. (A Task View on CRAN is a site that tries to combine and summarize various R packages for a specific topic). Another task view that deals with longitudinal/time-series data is the Survival Analysis Task View.\nOmics: The bioconductor website is your source for (almost) all tools and resources related to omics-type data analyses in R.\nText: Working with and analyzing larger sections of text is different from the simple string manipulation discussed above. These days, analysis of text often goes by the term natural language processing. Such text analysis will continue to increase in importance, given the increasing data streams of that type. If you are interested in doing full analyses of text data, the tidytext R package and the Text mining with R book are great resources. A short introduction to this topic is The text mining chapter (27) of IDS.\nImages: Images are generally converted into multiple matrices of values for different pixels of an image. For instance, one could divide an image into a 100x100 grid of pixels, and assign each pixel a RGB values and intensity. That means one would have 4 matrices of numeric values, each of size 100x100. One would then perform operations on those values. We won’t do anything with images here, there are some R packages for analyzing image data.\nVideos: Are a time-series of images. Analysis of videos therefore has an extra layer of complexity."
  },
  {
    "objectID": "posts/2023-02-09-data/index.html#for-sequences",
    "href": "posts/2023-02-09-data/index.html#for-sequences",
    "title": "Types of Data",
    "section": "For sequences",
    "text": "For sequences\n\n\n\nFunctions\nActions\n\n\n\n\nseq()\ngenerate a sequence of numbers\n\n\n1:10\nsequence from 1 to 10 by 1\n\n\nrep(x, times)\nreplicates x\n\n\nsample(x, size, replace=FALSE)\nsample size elements from x\n\n\nrnorm(n, mean=0, sd=1)\ndraw n samples from normal distribution"
  },
  {
    "objectID": "posts/2023-02-09-data/index.html#creating-or-coercing-objects-to-different-class",
    "href": "posts/2023-02-09-data/index.html#creating-or-coercing-objects-to-different-class",
    "title": "Types of Data",
    "section": "Creating or Coercing objects to different class",
    "text": "Creating or Coercing objects to different class\n\n\n\nFunctions\nActions\n\n\n\n\nvector()\ncreate a vector\n\n\nmatrix()\ncreate a matrix\n\n\ndata.frame()\ncreate a data frame\n\n\nas.vector(x)\ncoerces x to vector\n\n\nas.matrix(x)\ncoerces to matrix\n\n\nas.data.frame(x)\ncoerces to data frame\n\n\nas.character(x)\ncoerces to character\n\n\nas.numeric(x)\ncoerces to numeric\n\n\nfactor(x)\ncreates factor levels for elements of x\n\n\nlevels()\norders the factor levels as specified"
  },
  {
    "objectID": "posts/2023-02-09-data/index.html#footnotes",
    "href": "posts/2023-02-09-data/index.html#footnotes",
    "title": "Types of Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor example, logistic regression (a regression to predict a binary [yes/no] outcome) is used for classification. The underlying model predicts a quantitative outcome (a value between 0 and 1 usually interpreted as a probability), which is then binned to make categorical predictions.↩︎"
  },
  {
    "objectID": "posts/2023-02-01-reference-management/index.html",
    "href": "posts/2023-02-01-reference-management/index.html",
    "title": "Reference management",
    "section": "",
    "text": "Read ahead\n\n\n\nBefore class, you can prepare by reading the following materials:\n\nAuthoring in R Markdown from RStudio\nCitations from Reproducible Research in R from the Monash Data Fluency initiative\nBibliography from R Markdown Cookbook\n\n\n\n\n\n\nMaterial for this lecture was borrowed and adopted from\n\nhttps://www.stephaniehicks.com/jhustatcomputing2022/\nhttps://andreashandel.github.io/MADAcourse\nhttps://rmarkdown.rstudio.com/authoring_bibliographies_and_citations.html\nhttps://bookdown.org/yihui/rmarkdown-cookbook/bibliography.html\nhttps://monashdatafluency.github.io/r-rep-res/citations.html"
  },
  {
    "objectID": "posts/2023-02-01-reference-management/index.html#the-parts",
    "href": "posts/2023-02-01-reference-management/index.html#the-parts",
    "title": "Reference management",
    "section": "The Parts",
    "text": "The Parts\nThere are three basic parts:\n\nThe citation data which is stored in the .bib file Section 4, and\nIn-text citations included by the author into the .qmd document Section 7, and\nLinking the ʻ.bibʻ file in the YAML header Section 6.\n\nFrom this, both the in-text citations as well as the citation list at the end of the document will be rendered.\nThere are additional customization options. You can change the style of the bibliography using style and class files Section 8. This is at the level of the entire document, it is easy to switch. Within the document, there are also many options for in-text citation styles Section 9. You can also customize the style file by editing the LaTeX.\nReference managers are helper software (independent of BibTeX) that are wonderful tools to help you collect and organize your citation data Section 5."
  },
  {
    "objectID": "posts/2023-02-01-reference-management/index.html#citation-management-software",
    "href": "posts/2023-02-01-reference-management/index.html#citation-management-software",
    "title": "Reference management",
    "section": "Citation management software",
    "text": "Citation management software\nIn addition to .bib (BibTeX) there are a lot of file formats in use including .medline (MEDLINE), .ris (RIS), and .enl (EndNote), among others. You can generally download the results of your literature search in the format of your choice (some citation manager software can convert formats as well).\nIf you recall the output from citation(\"rmarkdown\") above, one option is to copy and paste the BibTeX output into a text file labeled .bib or into citation management software, but instead we can use Rʻs write_bib() function from the knitr package to create a bibliography file.\nLet’s run the following code in order to generate a my-refs.bib file\n\nknitr::write_bib(\"rmarkdown\", file = \"my-refs.bib\")\n\nYou can output multiple citations by passing a vector of package names:\n\nknitr::write_bib(c(\"rmarkdown\",\"base\"), file = \"my-refs.bib\")\n\nNow we can see we have the file saved locally.\n\nlist.files()\n\n[1] \"ButlerPapers.bib\" \"evolution.csl\"    \"index 2.html\"     \"index.qmd\"       \n[5] \"index.rmarkdown\"  \"my-refs.bib\"     \n\n\nIf you open up the my-refs.bib file, you will see\n@Manual{R-base,\n  title = {R: A Language and Environment for Statistical Computing},\n  author = {{R Core Team}},\n  organization = {R Foundation for Statistical Computing},\n  address = {Vienna, Austria},\n  year = {2022},\n  url = {https://www.R-project.org/},\n}\n\n@Manual{R-rmarkdown,\n  title = {rmarkdown: Dynamic Documents for R},\n  author = {JJ Allaire and Yihui Xie and Jonathan McPherson and Javier Luraschi and Kevin Ushey and Aron Atkins and Hadley Wickham and Joe Cheng and Winston Chang and Richard Iannone},\n  year = {2021},\n  note = {R package version 2.8},\n  url = {https://CRAN.R-project.org/package=rmarkdown},\n}\n\n@Book{rmarkdown2018,\n  title = {R Markdown: The Definitive Guide},\n  author = {Yihui Xie and J.J. Allaire and Garrett Grolemund},\n  publisher = {Chapman and Hall/CRC},\n  address = {Boca Raton, Florida},\n  year = {2018},\n  note = {ISBN 9781138359338},\n  url = {https://bookdown.org/yihui/rmarkdown},\n}\n\n@Book{rmarkdown2020,\n  title = {R Markdown Cookbook},\n  author = {Yihui Xie and Christophe Dervieux and Emily Riederer},\n  publisher = {Chapman and Hall/CRC},\n  address = {Boca Raton, Florida},\n  year = {2020},\n  note = {ISBN 9780367563837},\n  url = {https://bookdown.org/yihui/rmarkdown-cookbook},\n}\n\nNote there are three keys that we will use later on:\n\nR-rmarkdown\nrmarkdown2018\nrmarkdown2020"
  },
  {
    "objectID": "posts/2023-04-27-intro-OU-models/index.html",
    "href": "posts/2023-04-27-intro-OU-models/index.html",
    "title": "Introduction to OU Models",
    "section": "",
    "text": "Learning Objectives\n\n\n\nGoals: - Approaches for adaptive evolution (ouch, slouch, others) - Model-based vs statistical approaches\nConcepts: - Model comparison tools in R - Process-based models"
  },
  {
    "objectID": "posts/2023-04-27-intro-OU-models/index.html#the-ou-model-for-comparative-analysis",
    "href": "posts/2023-04-27-intro-OU-models/index.html#the-ou-model-for-comparative-analysis",
    "title": "Introduction to OU Models",
    "section": "The OU Model for Comparative Analysis",
    "text": "The OU Model for Comparative Analysis\nA Brownian motion process can be described using the following differential equation:\n\\[\n  dX(t) = \\sigma\\,dB(t).\n\\tag{1}\\]\nIf we imagine the phenotype \\(X\\) as changing through time \\(t\\), this equation says that in a small increment of time, the change will be proportional to the parameter \\(\\sigma\\). Here, \\(dB(t)\\) is a sample from a Brownian (white noise) process.\nA small step towards reality is the OU Process:\n\\[\n  dX(t) = \\alpha\\,(\\theta - X(t))\\,dt + \\sigma\\,dB(t).\n\\tag{2}\\]\nEq. Equation 2 expresses the amount of change in character \\(X\\) over the course of a small increment of time: specifically, \\(dX(t)\\) is the infinitesimal change in the character \\(X\\) over the infinitesimal interval from time \\(t\\) to time \\(t+dt\\). The term \\(dB(t)\\) is white noise; that is, the random variables \\(dB(t)\\) are independent and identically-distributed normal random variables, each with mean zero and variance \\(dt\\). The parameter \\(\\alpha\\) measures the strength of selection. When \\(\\alpha = 0\\), the deterministic part of the OU model is diminished and Equation 2 approaches the familiar BM model of pure drift,"
  },
  {
    "objectID": "posts/2023-04-27-intro-OU-models/index.html#ouch-package",
    "href": "posts/2023-04-27-intro-OU-models/index.html#ouch-package",
    "title": "Introduction to OU Models",
    "section": "ouch package",
    "text": "ouch package\nSee ouch lecture.\nGood starting points available in package ouch:\n- ?bimac help page for Bimaculatus character displacement dataset\n- example(bimac) example of bimac analysis - ?anolis.ssd help page for Anolis sexual size dimorphism dataset\nouch is a package designed to test adaptive hypotheses using variations of the OU process, including BM (Butler and King 2004; King and Butler 2022). OUCH implements a model that fits alpha and sigma parameters to the entire phylogeny, but allows the user to specify which branches belong to different selective regimes. The location of the optima are also fit.\n\nThe Data\nThe data in OUCH are most easily assembled as a data frame. Load the built in example from ouch and then print it to the screen (I only printed the head of the dataset here):\n\nrequire(ouch)\ndata(bimac)\nbimac\n\n\n\nLoading required package: ouch\n\n\n  node spcode species island size ancestor time OU.1   OU.3 OU.4  OU.LP\n1    1   &lt;NA&gt;    &lt;NA&gt;   &lt;NA&gt;   NA       NA    0   ns medium  anc medium\n2    2   &lt;NA&gt;    &lt;NA&gt;   &lt;NA&gt;   NA        1   12   ns medium  anc medium\n3    3   &lt;NA&gt;    &lt;NA&gt;   &lt;NA&gt;   NA        2   32   ns medium  anc  small\n4    4   &lt;NA&gt;    &lt;NA&gt;   &lt;NA&gt;   NA        3   34   ns medium  anc  small\n5    5   &lt;NA&gt;    &lt;NA&gt;   &lt;NA&gt;   NA        4   36   ns medium  anc  small\n6    6   &lt;NA&gt;    &lt;NA&gt;   &lt;NA&gt;   NA        3   36   ns medium  anc  small\n\n\nNOTE: a very important detail about ouch is that it matches trees with data and regimes using the node labels stored in the rownames of the objects you pass to the ouch functions. So it is important to make sure that your dataframes and vectors are appropriately named. The dataframe bimac already has the correct row names, but we do so here just to illustrate.\n\nrownames(bimac) &lt;- bimac$node\n\nouch was designed around a rectangular data model, so although the tree object is not a dataframe internally, it still helps us to build the data as a dataframe before making the ouchtree objects. The central organizing element is the node: it has a node number (usually an integer but it is actually a unique character string), an ancestor to which it is joined by a branch, a time since the root of the tree, and optional label such as a species name.\n\n\nThe hypotheses\nThe hypotheses which we use are assigned by painting particular regimes on branches. It is convenient to represent each model or hypothesis as a column on the dataframe, with the regime assigned to the node (that is, it is assigned to the branch connecting the node to its ancestor).\nMake an ouchtree object using the ouchtree constructor. with is a very nice function to create a small local environment so that you can use a dataframe’s elements directly without using the bimac$ prefix.\nIt is similar to an attach but it is temporary – only lasting as long as the call itself. I like it much better than attach because I sometimes forget what I’ve attached and run into problems later. Also, with attach, you are actually working with a copy of the original dataframe object, so updating values is tricky. With with, it is more clear what’s going on, and I don’t tend to make those mistakes.\n\ntree &lt;- with(bimac, ouchtree(node,ancestor,time/max(time),species))\nplot(tree)\n\n\n\n\nouch fits the OU model Eq. Equation 2 along each branch of the phylogeny. While \\(\\alpha\\) and \\(\\sigma\\) are held constant across the entire tree, the optima along each branch \\(\\theta\\) are allowed to vary. Users can then paint various combinations of optima on the tree to reflect various biological scenarios.\nFor example, the dataset bimac was used to test the hypothesis of character displacement using an interspecific daaset of body sizes and sympatry/allopatry Butler and King (2004). The analysis tested several different models, which are included with bimac. They are: OU.1 or global optimum, OU.3 or small, medium, and large regimes depending on the body size of the observed species (terminal branches only, internal branches painted medium, OU.4 or the same as OU.3 but with internal branches given their own unique regime called ancestral, and OU.LP based on a linear parsimony reconstruction of the colonization events (i.e., that as species came into sympatry, they diverged in body size).\n\n\nPlotting ouchtrees\nYou can plot the regime paintings on the tree, and set options such as line widths for prettier plots. ouch has a very nice feature which allows plotting of the alternative models on one plot.\n\nplot(tree, regimes=bimac[c(\"OU.1\", \"OU.3\", \"OU.4\", \"OU.LP\")], lwd=6)\n\n\n\n\nRemember that you can pass a single vector or a data frame to the regimes parameter, but it must have the appropriate row names or names in the case of a vector. The regimes are not part of the ouchtree object, because they represent our hypothesis of evolution along the tree, rather than the tree itself. It is part of the original dataframe from which we derived the tree, so remember to refer to bimac when passing the regimes to the plot function.\n\n\nFitting models\nThere are two main model fitting functions in ouch, brown, which fits Brownian motion models, and hansen, which fits OU models to comparative data. The call to brown is particularly simple, as it takes only the data and the tree:\n\nbrown(log(bimac['size']),tree)\n\n\ncall:\nbrown(data = log(bimac[\"size\"]), tree = tree)\n   nodes ancestors     times             labels     size\n1      1      &lt;NA&gt; 0.0000000               &lt;NA&gt;       NA\n2      2         1 0.3157895               &lt;NA&gt;       NA\n3      3         2 0.8421053               &lt;NA&gt;       NA\n4      4         3 0.8947368               &lt;NA&gt;       NA\n5      5         4 0.9473684               &lt;NA&gt;       NA\n6      6         3 0.9473684               &lt;NA&gt;       NA\n7      7         1 0.2105263               &lt;NA&gt;       NA\n8      8         7 0.3421053               &lt;NA&gt;       NA\n9      9         8 0.4736842               &lt;NA&gt;       NA\n10    10         9 0.6052632               &lt;NA&gt;       NA\n11    11        10 0.7368421               &lt;NA&gt;       NA\n12    12         9 0.7368421               &lt;NA&gt;       NA\n13    13         8 0.5789474               &lt;NA&gt;       NA\n14    14        13 0.6842105               &lt;NA&gt;       NA\n15    15        14 0.8947368               &lt;NA&gt;       NA\n16    16        15 0.9473684               &lt;NA&gt;       NA\n17    17         7 0.7368421               &lt;NA&gt;       NA\n18    18        17 0.7894737               &lt;NA&gt;       NA\n19    19        18 0.8947368               &lt;NA&gt;       NA\n20    20        19 0.9473684               &lt;NA&gt;       NA\n21    21        20 0.9736842               &lt;NA&gt;       NA\n22    22        19 0.9473684               &lt;NA&gt;       NA\n23    23         2 1.0000000       Anolis pogus 2.602690\n24    24         4 1.0000000   Anolis schwartzi 2.660260\n25    25         5 1.0000000   Anolis schwartzi 2.660260\n26    26         5 1.0000000   Anolis schwartzi 2.653242\n27    27         6 1.0000000     Anolis wattsii 2.674149\n28    28         6 1.0000000     Anolis wattsii 2.701361\n29    29        10 1.0000000 Anolis bimaculatus 3.161247\n30    30        11 1.0000000 Anolis bimaculatus 3.299534\n31    31        11 1.0000000 Anolis bimaculatus 3.328627\n32    32        12 1.0000000      Anolis leachi 3.353407\n33    33        12 1.0000000      Anolis leachi 3.360375\n34    34        13 1.0000000     Anolis nubilus 3.049273\n35    35        14 1.0000000     Anolis sabanus 2.906901\n36    36        15 1.0000000  Anolis gingivinus 2.980619\n37    37        16 1.0000000  Anolis gingivinus 2.933857\n38    38        16 1.0000000  Anolis gingivinus 2.975530\n39    39        17 1.0000000    Anolis oculatus 3.104587\n40    40        18 1.0000000     Anolis ferreus 3.346389\n41    41        20 1.0000000     Anolis lividus 2.928524\n42    42        21 1.0000000  Anolis marmoratus 2.939162\n43    43        21 1.0000000  Anolis marmoratus 2.990720\n44    44        22 1.0000000 Anolis terraealtae 3.058707\n45    45        22 1.0000000 Anolis terraealtae 3.068053\n\nsigma squared:\n           [,1]\n[1,] 0.04311003\n\ntheta:\nNULL\n   loglik  deviance       aic     aic.c       sic       dof \n 17.33129 -34.66257 -30.66257 -30.06257 -28.39158   2.00000 \n\n\nWhat is returned is an object of class browntree. It contains all input including the function call, the tree and data), as well as the parameter estimate for \\(\\sigma\\) and the model fit statistics including: the log-likelihood, the deviance (\\(-2*log(L)\\)), the information criteria \\(AIC\\), \\(AIC_c\\) (corrected for small sample size), and \\(SIC\\), and the model degrees of freedom.\nIt is a good practice to save this, as it encapsulates the analysis. From this, we can rerun the model fit.\n\nh1 &lt;- brown(log(bimac['size']),tree)\n\nhansen models are slightly more complex. In addition to \\(\\sigma\\), we are now fitting \\(\\alpha\\), the strength of selection, and all of the optima \\(\\theta\\) specified by our model. This maximum-likelihood search now requires an initial guess. If you have no idea, a good starting guess is 1. If you want to be sure, you can intiate searches with different starting guesses. You can also specify alternative optimization algorithms and increase or decrease the relative tolerance, which is the stringency by which convergence is assessed. Typically, the default is roughly reltol=1e-8, and the limit of machine precision is in the neighborhood of reltol=1e-15.\n\nh2 &lt;- hansen(log(bimac['size']),\n             tree,\n             bimac['OU.1'],\n             sqrt.alpha=1,\n             sigma=1)\nh3 &lt;- hansen(log(bimac['size']),\n             tree,bimac['OU.3'], \n             sqrt.alpha=1,\n             sigma=1)\nh4 &lt;- hansen(log(bimac['size']),\n             tree,\n             bimac['OU.4'], \n             sqrt.alpha=1,\n             sigma=1)\nh5 &lt;- hansen(log(bimac['size']),\n             tree,\n             bimac['OU.LP'], \n             sqrt.alpha=1,\n             sigma=1,\n             reltol=1e-5)\n\n\n\nhansentree and ouchtree methods\nWe can see the model results by typing h5, which will execute the print method for this class. You could also use the attributes function, but this will dump too much information. ouchtree objects and the classes derived from them contain information that is used in internal calculations of the algorithms, not of general interest to users.\nAdditional accessor functions include:\n\ncoef(h5)    # the coefficients of the fitted model\n\n$sqrt.alpha\n[1] 1.61658\n\n$sigma\n[1] 0.2249274\n\n$theta\n$theta$size\n   large   medium    small \n3.355087 3.040729 2.565249 \n\n\n$alpha.matrix\n        [,1]\n[1,] 2.61333\n\n$sigma.sq.matrix\n           [,1]\n[1,] 0.05059232\n\nlogLik(h5)   # the log-likelihood value\n\n[1] 24.81823\n\n\n\nsummary(h5) # (not printed) everything except the tree+data\n\nWe can now generate a table of our model fits:\n\n    # just the model fit statistics on a single line\nunlist(summary(h5)[c('aic', 'aic.c', 'sic', 'dof')])  \n\n      aic     aic.c       sic       dof \n-39.63645 -36.10704 -33.95898   5.00000 \n\nh &lt;- list(h1, h2, h3, h4, h5)   # store fitted models in a list\nnames(h) &lt;- c('BM', 'OU.1', 'OU.3', 'OU.4', 'OU.LP')\nsapply( h, function(x)\n             unlist(\n                summary(x)[c('aic', 'aic.c', 'sic', 'dof')]\n             ) \n      )                         # table with all models\n\n             BM      OU.1      OU.3      OU.4     OU.LP\naic   -30.66257 -25.39364 -29.15573 -35.22319 -39.63645\naic.c -30.06257 -24.13048 -25.62631 -29.97319 -36.10704\nsic   -28.39158 -21.98715 -23.47826 -28.41022 -33.95898\ndof     2.00000   3.00000   5.00000   6.00000   5.00000\n\n\nBy storing the model fits in a list, we can use apply methods to get the statistics from all the models at once. sapply returns a matrix if possible.\n\nh.ic &lt;- sapply( h, function(x) \n                     unlist(\n                      summary(x)[c('aic', 'aic.c', 'sic', 'dof')]\n                     ) \n               )  \nprint( h.ic, digits = 3)\n\n         BM  OU.1  OU.3  OU.4 OU.LP\naic   -30.7 -25.4 -29.2 -35.2 -39.6\naic.c -30.1 -24.1 -25.6 -30.0 -36.1\nsic   -28.4 -22.0 -23.5 -28.4 -34.0\ndof     2.0   3.0   5.0   6.0   5.0\n\n\n\n\nSimulation and bootstrap methods:\nsimulate generates random deviates or sets of simulated tip data based on the fitted model. The input is a fitted model hansentree or browntree, and the output is a list of dataframes, each comparable to the original data. These can then be used to refit the model.\n\nh5.sim &lt;- simulate(object = h5, nsim=10)   # saves 10 sets of simulated data\n                                  #  based on OU.LP\n\nupdate refits the model, with one or more parameters changed.\n\nsummary( update( object = h5, \n                 data = h5.sim[[1]] \n                )  # fit the first dataset\n        )   \n\n$call\nhansen(data = data, tree = object, regimes = regimes, sqrt.alpha = sqrt.alpha, \n    sigma = sigma)\n\n$conv.code\n[1] 0\n\n$optimizer.message\nNULL\n\n$alpha\n         [,1]\n[1,] 3.530333\n\n$sigma.squared\n           [,1]\n[1,] 0.04448437\n\n$optima\n$optima$size\n   large   medium    small \n3.333356 3.019454 2.619120 \n\n\n$loglik\n[1] 28.5958\n\n$deviance\n[1] -57.19159\n\n$aic\n[1] -47.19159\n\n$aic.c\n[1] -43.66218\n\n$sic\n[1] -41.51412\n\n$dof\n[1] 5\n\nh5.sim.fit &lt;- lapply( h5.sim, \n                      function(x) update(h5, x)\n                    ) # fit all 10 simulations\n\nbootstrap is a convenience function for generating parametric bootstraps of the parameter estimates. It takes the fitted model, performs the simulations, refits, and outputs a dataframe of parameter estimates.\n\nbootstrap( object = h5, nboot=10)\n\n      alpha sigma.squared optima.size.large optima.size.medium\n1  3.927412    0.03738445          3.323732           3.037843\n2  3.808633    0.04102961          3.229301           3.019838\n3  6.620335    0.08857152          3.201652           3.008789\n4  5.689668    0.06439393          3.387377           3.005807\n5  4.041209    0.04768774          3.367822           2.974627\n6  4.627289    0.05344219          3.392546           3.045898\n7  4.551867    0.07127402          3.271024           3.024687\n8  2.708405    0.04256169          3.344545           3.033925\n9  8.064081    0.05081199          3.343484           3.081023\n10 4.496143    0.10630749          3.257126           3.097088\n   optima.size.small   loglik       aic     aic.c       sic dof\n1           2.567702 31.46802 -52.93605 -49.40663 -47.25858   5\n2           2.560893 30.14563 -50.29125 -46.76184 -44.61378   5\n3           2.518318 26.20707 -42.41414 -38.88472 -36.73666   5\n4           2.661032 28.46116 -46.92231 -43.39290 -41.24484   5\n5           2.445358 28.90891 -47.81781 -44.28840 -42.14034   5\n6           2.726667 28.75600 -47.51201 -43.98259 -41.83454   5\n7           2.724046 25.30250 -40.60499 -37.07558 -34.92752   5\n8           2.609753 27.06160 -44.12320 -40.59378 -38.44572   5\n9           2.544137 34.51236 -59.02472 -55.49530 -53.34724   5\n10          2.586874 20.59745 -31.19490 -27.66548 -25.51742   5\n\n\n\n\npainting regimes on trees\nA new function in ouch is paint. Previously, it was up to users to set up regimes manually by editing spreadsheets. paint helps with this task by specifying the regimes on particular species, subtrees, or particular branches.\nThere are two parameters to paint, subtrees, which paints the entire subtree which descends from the node, and branch, which paints the branch connecting the node to it’s ancestor. For either, you specify the node label (remember it’s a character and needs to be quoted), and set it equal to the name of the regime you want to specify.\nLet’s try it on the bimac tree and try to recreate the OU.LP regime:\n\nplot(tree, node.names=T)\n\n\n\n\nPaint the subtrees first, take a look:\n\nou.lp &lt;- paint( tree, \n                subtree=c(\"1\"=\"medium\",\"9\"=\"large\",\"2\"=\"small\") \n               )\nplot(tree, regimes=ou.lp, node.names=T)\n\n\n\n\nBut there was an independent switch from medium to large at species gm, or node 38, and the node connecting 9 to its ancestor:\n\nou.lp &lt;- paint( tree, \n                subtree=c(\"1\"='medium',\"9\"='large',\"2\"='small'),\n                branch=c(\"38\"='large',\"2\"='medium')\n              )  \n\nCompare it to the original OU.LP from above.\n\nplot(tree, regimes=ou.lp, node.names=T)\n\n\n\n\nWe can create alternative paintings of the regimes to test against the data. Suppose we wanted to add a clade specific hypothesis that diverged in a similar time period (this is a completely made-up hypothesis, just for example):\n\nou.clades &lt;- paint( tree, \n                    subtree=c(\"1\"=\"A\",\"7\"=\"B\", \"8\"=\"C\"), \n                    branch=c(\"8\"=\"C\", \"7\"=\"C\", \"1\"=\"A\")\n                   )\nplot(tree, regimes=ou.clades, node.names=T)\n\n\n\n\nRun the model:\n\nh6 &lt;- hansen( \n              log(bimac['size']),\n              tree, \n              regimes=ou.clades, \n              sqrt.alpha=1,\n              sigma=1\n            )\n\nRebuild our table and compare models:\n\nh &lt;- append(h, h6)         # append (add on) new model results to our list h\nnames(h)[length(h)] &lt;- ou.clades    # add the name of the new model\n\nWarning in names(h)[length(h)] &lt;- ou.clades: number of items to replace is not\na multiple of replacement length\n\nnames(h)\n\n[1] \"BM\"    \"OU.1\"  \"OU.3\"  \"OU.4\"  \"OU.LP\" \"1\"    \n\nh.ic &lt;- sapply( h, function(x) \n                     unlist(\n                        summary(x)[c('aic', 'aic.c', 'sic', 'dof')]\n                     ) \n               )  \nprint( h.ic, digits = 3)\n\n         BM  OU.1  OU.3  OU.4 OU.LP     1\naic   -30.7 -25.4 -29.2 -35.2 -39.6 -30.7\naic.c -30.1 -24.1 -25.6 -30.0 -36.1 -27.1\nsic   -28.4 -22.0 -23.5 -28.4 -34.0 -25.0\ndof     2.0   3.0   5.0   6.0   5.0   5.0"
  },
  {
    "objectID": "posts/2023-03-23-apply/index.html",
    "href": "posts/2023-03-23-apply/index.html",
    "title": "Vectorization with Apply Functions",
    "section": "",
    "text": "Learning objectives\n\n\n\nAt the end of this lesson you will:\n\nRecognize the different types of apply functions\nBe able to use apply functions to perform operations on objects\nBe introduced to writing functions for apply functions\nHave gained another skill in modular programming"
  },
  {
    "objectID": "posts/2023-03-23-apply/index.html#sapply-and-lapply",
    "href": "posts/2023-03-23-apply/index.html#sapply-and-lapply",
    "title": "Vectorization with Apply Functions",
    "section": "sapply and lapply",
    "text": "sapply and lapply\nAnother common type is lapply, which operates on list objects and returns a list. sapply (s for simplify) is almost identical to lapply, but tries to make prettier output by returning a vector or a matrix if possible (instead of a list):\n\nsapply( 1:5, square ) \n\n[1]  1  4  9 16 25\n\nlapply( 1:5, square )\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 9\n\n[[4]]\n[1] 16\n\n[[5]]\n[1] 25\n\n\n\nThere is also apply() which works on matrices or arrays, and has an index argument for whether it should apply the function over rows or columns.\ntapply to apply the function across a grouping index or treatments.\nmapply to apply to multiple lists simultaneously.\nouter which applies the function to an outer product of two arrays, and more.\naggregate is actually a user-friendly wrapper for tapply, used to apply a function across groups.\n\nAll of the apply functions work in the same way. Donʻt get overwhelmed - I mainly use sapply or lapply, and aggregate, and occasionally apply if I need to work over rows. Thatʻs all you need to remember, consult the help page when you need."
  },
  {
    "objectID": "posts/2023-03-23-apply/index.html#exercises",
    "href": "posts/2023-03-23-apply/index.html#exercises",
    "title": "Vectorization with Apply Functions",
    "section": "Exercises",
    "text": "Exercises\n\nPerform the following computation using an apply function.\n\n\nmylist &lt;- vector(\"list\")   ## creates a null (empty) list\nfor (i in 1:4) {\n   mylist[i] &lt;- list(data.frame(x=rnorm(3), y=rnorm(3)))  \n}\n\n\nPlot x as a function of y for each dataframe using an apply function.\nUsing an apply function, compute an anova on x ~ y on each dataframe, and save the anova output (there should be 4 of them) to a list or dataframe.\nWrite a for loop that finds the sum of the sequence of integers from 1 to 100, then accomplish the same computation with an apply function."
  },
  {
    "objectID": "posts/2023-02-07-scripts/index.html",
    "href": "posts/2023-02-07-scripts/index.html",
    "title": "Saving your work as R scripts",
    "section": "",
    "text": "✏️"
  },
  {
    "objectID": "posts/2023-02-07-scripts/index.html#make-sure-you-can-see-file-endings",
    "href": "posts/2023-02-07-scripts/index.html#make-sure-you-can-see-file-endings",
    "title": "Saving your work as R scripts",
    "section": "Make sure you can see file endings",
    "text": "Make sure you can see file endings\nDoes your MacOS or Windows environment show you the file endings (i.e., .R, .pdf, .csv, etc.)? If not be sure to turn them on. Try the instructions below or you can google for “show all file exensions in” (Mac or Windows, etc.).\n\nMac OS\nThis is a Finder preference. From any Finder window, click on the menu bar: Finder &gt;  Preferences &gt; Advanced &gt; Click on Show all finename extensions.\n\n\nWindows\nThis is in File Explorer (Windows key + E). Click on the menu: View &gt; Show &gt; File Name Extensions. You can also choose to show all hidden files if you wish. For pictures see here"
  },
  {
    "objectID": "posts/2023-02-07-scripts/index.html#text-editor-environment",
    "href": "posts/2023-02-07-scripts/index.html#text-editor-environment",
    "title": "Saving your work as R scripts",
    "section": "Text editor environment",
    "text": "Text editor environment\nWhile I love the R text editor for writing R scripts, for working with multiple .qmd and other files I find it helpful to have a full-featured plain text editor. A new tool that I discovered is the Sublime text editor. If youʻd like to try it out, you can download it here: https://www.sublimetext.com\nA couple of features I like is that you can have multiple panes open. For example if you want to copy text from an old script to a new script, you can easily see and do that.\nIt also allows you to organize Projects, various folders that will appear on the sidebar to preserve your workspace. This helps when you are writing text documents across folders. So for example if you have your Rclass folder in one place and your website folder in another, you can have both open within the Sublime project. When you finish working on it you can save the project and reopen it later.\nTo create a project start by opening a new file. Then choose the Project &gt; Add folder to Project... on the menu bar. You can load mulitple folders.\nIt has contextual highlighting for Quarto as well as GitHub markdown.\nIt also has integration with command line R https://bishwarup-paul.medium.com/a-guide-to-using-r-in-sublime-text-27f78b33f872. You can run R commands in a lower terminal pane, sent directly from your text document in sublime."
  },
  {
    "objectID": "posts/2023-02-07-scripts/index.html#different-desktop-windows",
    "href": "posts/2023-02-07-scripts/index.html#different-desktop-windows",
    "title": "Saving your work as R scripts",
    "section": "Different Desktop Windows",
    "text": "Different Desktop Windows\nItʻs also nice to have multiple desktops to organize your work. - It makes it easier to find your different apps. - You may have one workspace for your text editor, and another for your Terminal or CMD prompt, for example. - If I am working with multiple git repos, I might have one desktop just for my Terminal windows with a separate Terminal open for each one.\nTo use multiple desktops: On Windows On Mac\nOne the Mac, you can open new desktops by using three fingers to swipe up on the trackpad. Switch between them by swiping left or right with three fingers."
  },
  {
    "objectID": "posts/2023-02-07-scripts/index.html#organize-your-projects-into-folders",
    "href": "posts/2023-02-07-scripts/index.html#organize-your-projects-into-folders",
    "title": "Saving your work as R scripts",
    "section": "Organize your Projects into Folders",
    "text": "Organize your Projects into Folders\nWeʻve been learning about reproducibility. One important aspect is file organization. Each project should be organized into one folder that contains:\n\nAll input data (usually in a Data folder)\nAll code and documentation\nAll output\n\nThe idea is to keep everything complete, self-contained, and clear. Move old versions into a “Trash” folder. If you donʻt end up looking back at it, then delete it! (Or if you are bold, delete it right away!)\nA really useful UNIX/CMD command is tree. It shows you the directory structure contained within any folder. It works on both MacOS and Windows.\nThis is in ASCII – so you can copy and paste it into your README.md file!\n\n\nTerminal/CMD\n\ntree myfolder\n\nIf it is not pre-installed on your mac, you may need to install it with homebrew:\n\n\nTerminal\n\nbrew install tree"
  },
  {
    "objectID": "posts/2023-04-04-measurement-error/index.html",
    "href": "posts/2023-04-04-measurement-error/index.html",
    "title": "Measurement Error",
    "section": "",
    "text": "Material for this lecture was borrowed and adopted from\n\nhttps://www.r-bloggers.com/2015/04/tips-tricks-8-examining-replicate-error/"
  },
  {
    "objectID": "posts/2023-04-04-measurement-error/index.html#protocol-for-assessing-me",
    "href": "posts/2023-04-04-measurement-error/index.html#protocol-for-assessing-me",
    "title": "Measurement Error",
    "section": "Protocol for assessing ME",
    "text": "Protocol for assessing ME\nThe percentage of measurement error is defined as the within-group component of variance divided by the total (within + betwee group) variance (Claude 2008):\n\\[\n\\%ME = \\frac{s^{2}_{within}}{s^{2}_{within} + s^{2}_{among}} \\times 100\n\\]\nWe can get the componets of variance \\(s^{2}\\) from the mean squares (\\(MSS\\)) of an ANOVA considering the individual (as a factor) source of variation. Individual here represents the within-group variation. The among and within variance can be estimated from the mean sum of squares and \\(m\\) the number of repeated measurements:\n\\[\ns^{2}_{among} = \\frac{MSS_{among} - MSS_{within}}{m}\n\\]\nand\n\\[\ns^{2}_{within} = MSS_{within}\n\\]"
  },
  {
    "objectID": "posts/2023-04-04-measurement-error/index.html#example",
    "href": "posts/2023-04-04-measurement-error/index.html#example",
    "title": "Measurement Error",
    "section": "Example",
    "text": "Example\nSuppose we are taking photographs of specimens, and then collecting landmark data from the photos. This is a pretty typical data collection pipeline.\nBecause we are taking 2D photos from 3D objects, one potential issue is whether the shape variation we obtain is real, or whether it is introduced by placing either the object or the camera at slightly different angles.\nAnother potential issue is whether we are placing the digitized landmarks in exactly the same place.\nThere may be additional issues as well - for example some small ambiguity on the physical object, or the material or photos may be of different quality.\n\nPlan your data management\nI always recommend storing your metadata in the filenames. That way you never lose the information.\nPhoto files: A good strategy for data management is to label the photo files: id_picture_replicate.jpg\nWhere: - id refers to the specimen, - picture the replicate photo (photo1 or photo2), and - replicate the replicate landmark coordinates (rep1 or rep2).\nWe can parse the metadata from the filenames by code such as:\n\nfiles &lt;- list.files()  # to read the file names from the current drectory\nfiles &lt;- files &lt;- c(\"id1_photo1_rep1.jpg\", \n                    \"id1_photo1_rep2.jpg\", \n                    \"id1_photo2_rep1.jpg\", \n                    \"id1_photo2_rep2.jpg\"\n                    )  # made up example to practice \n\n# Collect metadata, approach 1 - substr\nmeta &lt;- strsplit(files, \"_|\\\\.\")  # metadata. split filenames by _ or . \n                                  # Need to use \\\\ to escape the .\nid &lt;- sapply(meta, \"[[\", 1)\nphoto &lt;- sapply(meta, \"[[\", 2)\nrep &lt;- sapply(meta, \"[[\", 3)\n\n# Collect metadata approach 2 - sub\n# using sub and regular expressions to select (string1)_(string2)_(string3)\n# also ignoring the final .jpg, where . is escaped by \\\\\n\nid &lt;- sub(\n        \"^([a-zA-Z0-9]+)_([a-zA-Z0-9]+)_([a-zA-Z0-9]+)(\\\\.jpg)\", \n        \"\\\\1\", \n        files\n      )\nphoto &lt;- sub(\n         \"^([a-zA-Z0-9]+)_([a-zA-Z0-9]+)_([a-zA-Z0-9]+)(\\\\.jpg)\", \n         \"\\\\2\", \n         files\n       )\nrep &lt;- sub(\n         \"^([a-zA-Z0-9]+)_([a-zA-Z0-9]+)_([a-zA-Z0-9]+)(\\\\.jpg)\", \n         \"\\\\3\", \n         files\n       )\n\nWe can use these vectors along with the coordinates to test for measurement error with ANOVA.\n\n\nStatistical methods for Measurement Error:\nWe will assess measurement error at two levels, photography error and digitizing error:\nPhotography error: Take two sets of photos, each time placing the object in front of the camera and positioning the specimen. (I.e., the entire process to give us a good estimate of photo capture error)\nLandmark digitizing error: Collect landmarks twice, ideally in different sessions on different days or weeks.\nData: In this example we will have 4 sets of landmark data for each specimen, 2 photos x 2 digitizing replicates, allowing assessment of error associated with the digitization as well as error in capturing the shapes via the photographs.\nModel: We will use a nested ANOVA to estimate repeatability and (measurement error) of the landmarks, to try to separate the variation introduced by the digitization process, apart from the other sources of variation.\n\n\nAnalyze with ANOVA:\nNested ANOVA indicates that we have a nested structure of replicates within groups (i.e., rep1 of photo1 has nothing to do with rep1 of photo2. rep is nested within photo.\nIn R we specify a nested model forumula using : in the model term (to indicate interaction terms only with no main effect):\n\nlm.fit &lt;- lm(coords ~ id:photo:rep)\naov(lm.fit)\n\nData and model term objects: - coords is the data object (a vector or array) - id is a vector containing labels for each specimen - photo is a vector (photo is 1 or 2) - rep is a vector (digitizing replicate 1 or 2)\nLook at the values of the Mean Squares (MS) column in the ANOVA table. Compare the value for id:photo and id:photo:rep with id.\n\nRepeatability\nTo calculate the repeatability of our digitizing ability, we subtract the MS of the rep term from the individual term and divide by two (because we have two replicates):\n\n((MS(id) – MS(id:photo:rep))/2 )\n\nThen we calculate the ratio of this value to the total MS:\n\n((MS(id) – MS(id:photo:rep))/2 ) / (MS(id)+MS(id:photo)+MS(id:photo:rep))\n\nThe result is the repeatability, which in good circumstances is somewhere above 0.95; and thus 5% measurement error.\n\n\n\nSimplifed Simulated example:\nSimplifed example: 20 specimens, 1 photo, 2 digitzing reps:\n20 specimens: (single measurement dataset). 2 repetitons: Digitize each photo twice (once in each of two sessions on different days).\nHow repeatable are the measurements?\nSimulate the data:\n\ntrue_m &lt;- rnorm(20,20,3)  # true values for  specimens\nm1 &lt;- true_m + rnorm(20,0,0.5)  # measurement 1\nm2 &lt;- true_m + rnorm(20,0,0.5)  # measurement 2\n\nid &lt;- as.factor(rep(1:20, times=2))\nrep &lt;- gl(2, 20)\ntotal_m &lt;- c(m1, m2)\ncbind(id, total_m, rep)  # the data\n\n      id  total_m rep\n [1,]  1 24.15721   1\n [2,]  2 23.51620   1\n [3,]  3 20.32648   1\n [4,]  4 21.13753   1\n [5,]  5 19.63922   1\n [6,]  6 18.90770   1\n [7,]  7 20.80480   1\n [8,]  8 21.50574   1\n [9,]  9 19.54461   1\n[10,] 10 18.38292   1\n[11,] 11 16.51972   1\n[12,] 12 22.56470   1\n[13,] 13 20.90189   1\n[14,] 14 27.00296   1\n[15,] 15 19.14295   1\n[16,] 16 22.80917   1\n[17,] 17 21.36119   1\n[18,] 18 17.27635   1\n[19,] 19 20.43382   1\n[20,] 20 22.27521   1\n[21,]  1 24.22700   2\n[22,]  2 23.89706   2\n[23,]  3 19.29717   2\n[24,]  4 21.86834   2\n[25,]  5 20.88725   2\n[26,]  6 18.54368   2\n[27,]  7 20.65837   2\n[28,]  8 21.74068   2\n[29,]  9 18.87857   2\n[30,] 10 17.97394   2\n[31,] 11 16.30139   2\n[32,] 12 22.07608   2\n[33,] 13 21.17731   2\n[34,] 14 26.78290   2\n[35,] 15 19.46328   2\n[36,] 16 21.98306   2\n[37,] 17 21.04685   2\n[38,] 18 17.49118   2\n[39,] 19 20.60442   2\n[40,] 20 22.55779   2\n\n\nIs there a difference between the measurement sessions?\n\nsummary(aov(lm ( total_m ~ rep)))\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)\nrep          1   0.01   0.014   0.002  0.962\nResiduals   38 230.77   6.073               \n\n\nNo (thatʻs good!)\nIs there a difference between individual specimens?\n\nmod &lt;- summary(aov(lm( total_m ~ id )))\nmod\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nid          19 228.00  12.000   86.17 3.82e-15 ***\nResiduals   20   2.79   0.139                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nYes, and the resigual mean squared error looks small too (good!). How big is the measurement error?\n\ns2_within &lt;- ms_within &lt;- mod[[1]][2,3]\ns2_within\n\n[1] 0.1392685\n\nms_among &lt;- mod[[1]][1,3]\ns2_among &lt;- (ms_among-ms_within)/2\nME &lt;- s2_within/(s2_within+s2_among) * 100\nME\n\n[1] 2.294503\n\n\nNot bad. A rule of thumb is that 5% ME is good (95% repeatability). If we want to reduce ME, we can use the average of the two measurements in our analyses."
  },
  {
    "objectID": "posts/2023-03-30-morphometrics/index.html",
    "href": "posts/2023-03-30-morphometrics/index.html",
    "title": "Intro to Morphometrics",
    "section": "",
    "text": "Material for this lecture was borrowed and adopted from\n\nhttps://www.r-bloggers.com/2015/04/tips-tricks-8-examining-replicate-error/"
  },
  {
    "objectID": "posts/2023-03-30-morphometrics/index.html#protocol-for-assessing-me",
    "href": "posts/2023-03-30-morphometrics/index.html#protocol-for-assessing-me",
    "title": "Intro to Morphometrics",
    "section": "Protocol for assessing ME",
    "text": "Protocol for assessing ME\nThe percentage of measurement error is defined as the within-group component of variance divided by the total (within + betwee group) variance (Claude 2008):\n\\[\n\\%ME = \\frac{s^{2}_{within}}{s^{2}_{within} + s^{2}_{among}} \\times 100\n\\]\nWe can get the componets of variance \\(s^{2}\\) from the mean squares (\\(MSS\\)) of an ANOVA considering the individual (as a factor) source of variation. Individual here represents the within-group variation. The among and within variance can be estimated from the mean sum of squares and \\(m\\) the number of repeated measurements:\n\\[\ns^{2}_{among} = \\frac{MSS_{among} - MSS_{within}}{m}\n\\]\nand\n\\[\ns^{2}_{within} = MSS_{within}\n\\]"
  },
  {
    "objectID": "posts/2023-03-30-morphometrics/index.html#example",
    "href": "posts/2023-03-30-morphometrics/index.html#example",
    "title": "Intro to Morphometrics",
    "section": "Example",
    "text": "Example\nLet’s say you are taking photographs of your specimens and you want to quantify the error assocated with placing your landmarks in the same place every time (i.e. is your criteria for the landmark robust enough that its obvious where it should be placed on each specimen, and if you came back to the data a month or year later?)\nTo assess measurement error in this instance we could take two sets of pictures, each time removing and positioning the specimen. And we could digitize each image twice, preferably in different sessions (another day or week). This would give us 4 sets of landmark data for each specimen, allowing us to asses both error associated with the digitization as well as error in capturing the shapes via the photographs.\nAlternatively, if we were interested in inter-observer error vs. repeatability within observer, we could take one photograph and have it measured by two different people, each person taking two sets of measurements (preferably in different sessions).\n\nSimulated example:\nRepeat a set of five measurements, (measure twice), once in each of two sessions on different days. How repeatable are the measurements?\nSimulate the data:\n\ntrue_m &lt;- rnorm(5,20,3)  # true values\nm1 &lt;- true_m + rnorm(5,0,1) # measurements set 1\nm2 &lt;- true_m + rnorm(5,0,1)\nsession &lt;- gl(2, 5)\nindividual &lt;- as.factor(rep(1:5,2))\ntotal_m &lt;- c(m1, m2)\ncbind(individual, total_m, session)  # the data\n\n      individual  total_m session\n [1,]          1 20.44919       1\n [2,]          2 21.85592       1\n [3,]          3 23.57399       1\n [4,]          4 23.66764       1\n [5,]          5 15.82221       1\n [6,]          1 20.61579       2\n [7,]          2 22.73522       2\n [8,]          3 22.57645       2\n [9,]          4 23.67244       2\n[10,]          5 13.60084       2\n\n\n\nsummary(aov(total_m ~ session))\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)\nsession      1   0.47    0.47   0.035  0.857\nResiduals    8 108.45   13.56               \n\n\nThere is no difference between sessions (yay)!\n\nmod &lt;- summary(aov(total_m ~ individual))\nmod \n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nindividual   4 105.55  26.388   39.21 0.000574 ***\nResiduals    5   3.37   0.673                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe specimens measured (individuals) are significantly different.\n\ns2within &lt;- MSwithin &lt;- mod[[1]][2,3]\nMSamong &lt;- mod[[1]][1,3]\ns2among &lt;- (MSamong-MSwithin)/2\npME &lt;- s2within/(s2within+s2among)*100\npME\n\n[1] 4.974382\n\n\nAnd the percent measurement error is represented by pME. As a rough rule of thumb we want this to be less than 5%. If it is very high, we either want to practice more, or take multiple measurements of each variable and average them."
  },
  {
    "objectID": "posts/2023-03-30-morphometrics/index.html#calculating-distances",
    "href": "posts/2023-03-30-morphometrics/index.html#calculating-distances",
    "title": "Intro to Morphometrics",
    "section": "Calculating distances",
    "text": "Calculating distances\nYou may need to calculate distances between your landmarks, or you may have species centroids (the means in multiple dimensions), and you want to know the distance between species in morphospace.\nThe distance between two points (in k dimensions) is given by the square root of the sum of the squared differences between the points.\n\\[\nd_{AB} = \\sqrt{\\sum_{i=1}^{k}} (A_i - B_i)^2  \n\\]\nThis is analogous to calcuating the length of the hypotenuse in a right triangle. If there is more than one dimension, these squared differences are computed dimension by dimension, summed, then the entire quantity is square rooted.\nA 2-D example in R is:\n\nA &lt;- c(1,4)\nB &lt;- c(6,8)\nplot( c(A[1], B[1]),c(A[2], B[2]), xlim=c(0,10), ylim=c(0,10), \n      xlab=\"X\", ylab=\"Y\", cex=4,  pch=16, col=\"red\")\nlines( c(A[1], B[1]),c(A[2], B[2]), lty=2, col=\"red\" )\nlines( c(A[1], B[1]),c(A[2], A[2]), col=\"grey\" )\nlines( c(B[1], B[1]),c(A[2], B[2]), col=\"grey\" )\ntext(A[1], A[2], \"A\")\ntext(B[1], B[2], \"B\")\n\n\n\n\nCompute the distance:\n\nA-B\n\n[1] -5 -4\n\nsum((A-B)^2)\n\n[1] 41\n\nsqrt(sum((A-B)^2))\n\n[1] 6.403124\n\n\nWe can create a function:\n\ndistance &lt;- function( A, B ) { sqrt(sum((A-B)^2)) }\ndistance( A, B )\n\n[1] 6.403124\n\n\nTry it on a 3-D set of coordinates. Does it work?"
  },
  {
    "objectID": "posts/2023-03-30-morphometrics/index.html#angle-between-two-vectors",
    "href": "posts/2023-03-30-morphometrics/index.html#angle-between-two-vectors",
    "title": "Intro to Morphometrics",
    "section": "Angle between two vectors",
    "text": "Angle between two vectors\nThe angle \\(\\theta\\) between two vectors \\(\\overrightarrow{AB}\\) and \\(\\overrightarrow{CD}\\) can be calculated using the dot product and the rule of cosines.\nSuppose we have two vectors \\(V_1 = (x_1,y_1)\\), and \\(V2 = (x_2,y_2)\\). We can use our coordinates A, B above to calculate our first vector, and make up a second one. From geometry, we can think of vectors as originating at (0,0) with the vector coordinates indicating the head of the vector.\n\nV1 &lt;- AB &lt;- B - A  # using our A,B landmarks above\nV1 \n\n[1] 5 4\n\nV2 &lt;- c(2,4)  # another vector\n\nThe dot product is then:\n\\[\nV_1V_2 = x_1x_2 + y_1y_2 = |V_1| \\cdot |V_2| cos(\\theta)\n\\]\nRearranging:\n\\[\ncos(\\theta) = \\frac{ x_1x_2 + y_1y_2 } { |V_1| \\cdot |V_2| }\n\\]\nAlso \\[\n|V_1| = \\sqrt{  x_1^2 + y_1^2}\n\\]\nAnd similarly for \\(V_2\\).\nSo all we have to do is take the inverse cosine, arc cosine, or acos() to solve for the angle theta (\\(\\theta\\)).\n\nCalculating the angle in R\n\nV1 %*% V2  # the dot product. what is the difference with V1 * V2?\n\n     [,1]\n[1,]   26\n\n           # same as sum(V1*V2) \n\ntheta &lt;- acos( (V1 %*% V2) / (sqrt( V1%*%V1) * sqrt(V2%*%V2)) )\ntheta\n\n          [,1]\n[1,] 0.4324078\n\n\nWhere the result is in radians, not degrees (see the help page ?acos, all of Rʻs trig functions are in radians).\nNote: I should change this to using the arc tangent, to preserve the angle in case it is outside of 0,pi.\n\nRadians to degrees\nDegrees and radians are different units of measure for an angle. To covert to degrees, remember that a 360 degree circle is 2pi radians (or a half circle 180 degrees is pi radians):\nTo remember how to do this conversion, recall that the formula for the circumference of a circle is:\n\\[\nCircumference = 2 \\pi r\n\\]\nA few key facts:\n\nArc length : the distance along a curved line (the arc length of the entire circle is the circumference).\nRadian : the angle measured in relation to the radius. If you take the radius and lay it along the circle, the angle defined by the arc length of one radius is one radian.\n\nTo complete the circle, we need an arc length of \\(2\\pi r\\) (which is the circumferemce), or a little over 6 radians (because \\(2\\pi ~= 6\\)).\nDegrees : another unit of measure for angles, defined by one circle having 360 degrees.\n\n\nTherefore, the angle of a full circle is \\(360^\\circ\\), or equivalently, \\(2\\pi\\) radians:\n\\(180^\\circ = \\pi\\) radians\nIf we take a measurement M in radians: \\[\nM (rad)\\frac{180^\\circ}{\\pi rad} = (M\\frac{180}{\\pi} )^\\circ\n\\]\n\ntheta_degrees &lt;- theta/ pi * 180\ntheta_degrees\n\n         [,1]\n[1,] 24.77514"
  },
  {
    "objectID": "posts/2023-04-04-morphometrics-landmarks/index.html",
    "href": "posts/2023-04-04-morphometrics-landmarks/index.html",
    "title": "Intro to Geometric Morphometrics",
    "section": "",
    "text": "Acknowledgements\nReferences for this Material:\n\nBardua et al (2019) A Practical Guide to Sliding and Surface Semilandmarks in Morphometric Analyseshttps://academic.oup.com/iob/article/1/1/obz016/5526881\nEmma Sherratʻs quick guide to Geomorph\n\n\n\nLearning objectives\n\n\n\n\n\n\nLearning objectives\n\n\n\nAt the end of this lesson you will:\n\nBe able to collect landmark data\nUnderstand the terminology for landmark data\n\n\n\n\nPreparation\nPackages:\ngeomorph\nrgl\nMatrix\nPlease install if you donʻt have them. install.packages(c(\"geomorph\",\"rgl\",\"Matrix\"), dependencies=T)\nMac Users: Please also install Xquartz https://www.xquartz.org, a windowing system for rgl.\nFiles:\nred-fox-mandible-lateral.jpg Please download into your folder for this exercise. Source: Jim Russell\n\n\n\nOverview\nTraditional morphometrics uses linear measurements such as body length, hind limb length, etc. to characterize morphological variation. This works well for many questions, especially ones that relate to lever mechanics, such as locomotion or bite force, etc. but does not capture 3D variation well such as the geometric shape of a skull or the details of seed shape.\nGeometric morphometrics aims to characterize more complex shapes by the use of landmarks, locations on the specimens determined by the researcher to reveal key insights or capture the full range of variation.\nIn order to do this, we still need methods to characterize size vs. shape. For geometric morphometrics that is typically Procrustes superposition. Some other differences also involve the data. Whereas one can typically take linear measurements directly on the specimen (with calipers or a ruler), collecting landmark data usually involves some image capture.\n\n\nCapturing images\nThe most important thing is consistency and a protocol that minimizes error. While the specific protocol will vary by the type of object and technology (cameras, uCT, etc.), some general principles are :\n\nSpecimen Layout\n\nBe as consistent as possible in the preparation of specimens and their arrangement\nPlace every specimen on the same side. (Capture the same aspect of each specimen)\nSame pose or posture\nMouths open? Closed?\nFins or dewlaps exposed to show shape and articulatio, etc.\n\nSize and Metadata\n\nInclude a size standard (e.g., a ruler), ideally the same item, with X-Y information\nInclude the specimen ID in the photo itself (written on a card, etc.)\n\nCamera\n\nCamera in same position and same angle, preferably with a tripod\nUse the same camera and lens (or zoom, etc.)\nMake several backups (multiple flash cards, save to computer, etc.)\nSpecimen perpendicular to camera (check level)\nSame lighting\nTake several photos as backup\n\nData Management\n\nKeep a notebook (written log) of the specimens being photographed, in order\nTransfer files to raw data folder for photos (in same format and same order)\nName files with metadata in filenames for each specimen, save in another folder\nYou should have Raw_Photos, Named_Photos folders.\n\n\nErrors and distortions can be fixed with software, but it is much easier to minimize them at the start.\n\n\nLandmarks\nEssential features of good landmarks:\nChoose landmarks that are:\n\nPresent on all specimens\n\nRelevant to the question\n\nClearly defined (to be repeatable)\n\nConform to accepted best practices for your type of study (do a literature search)\n\nHere is an example of typical landmarks:\n Source\n\nDigitizing Landmarks on Images\nAcquiring landmarks usually involves some digitizing by hand (with computer assistance). For 2D landmark data the workflow is:\n\nRead in the image file\n\nSet the scale\n\nDigitize landmarks, collecting X-Y coordinate data\n\nImage files are rows and columns of pixels, the number of which is determined by the file size and resolution. Pixels therefore, form a grid which we can digitize, meaning we can indicate at which pixel our landmark of interest is. We translate the pixel coordinates into real-world distances, typically by ditigizing an object of known dimensions such as a ruler. Digitizing software typically asks the user to indicate the points on the screen and the known distances they correspond to.\nEasy-to use software for this purpose include ImageJ and FIJI, as well as the geomorph package in R. Before doing a large project, I would try them all to see which serves your purpose the best.\n\n\n\ngeomorph package\nThe package geomorph is widely used for geometric morphometrics, and will serve as a good example to illustrate the geometric morphometrics workflow all in R (depending on the project and the material, however, you may find it more convenient or efficient to use other software for digitizing).\n\nDigitizing landmarks in geomorph\nFor a quick example, letʻs do a 2D landmark collection on the Red Fox Mandible using digitize2d(). Emma Sherratʻs quick guide to Geomorph provides a more thorough walk-through for capturing landmarks in Chapter 15.\nThe syntax for digitize2d() is:\ndigitize2d(filelist, nlandmarks, scale = NULL, tpsfile, verbose = TRUE)\nWhere:\n\nfilelist : a vector of filenames of image files\nnlandmarks : the number of landmarks to digitize\n\nscale : the length of scale to use (optional)\n\ntpsfile : the name of an output file to create or read\n\n\nrequire(geomorph)\n\nfiles &lt;- c(\"red-fox-mandible-lateral.jpg\")\n# files &lt;- list.files(pattern = \"*.jpg\") \n             # for multiple files in your image directory\n\n\ndigitize2d( files, \n            nlandmarks=10, \n            scale=80, \n            tpsfile = \"mandible-lat.tps\", \n            verbose = TRUE) \n\nR will now wait for you to digitize the first landmark. In verbose mode, it will ask you to confirm each landmark (this can be turned off). If a landmark is missing, choose a for absent. If you make a mistake and choose n, it will ask you to redigitize it. The output will be sent to a .tpsfile.\n\n\n\nTPS format\nTPS format is a standard morphometric data format for storing landmark coordinate data.\nreadland.tps() reads TPS format and returns an array: p landmarks by k dimensions by N specimens.\nRemember an array is simply a multidimensional dataframe. Think of it here as a stack of spreadsheets. Each spreadsheet has rows for landmarks and columns for dimensions. We have a separate spreadsheet for each specimen, making this a 3-dimensional array.\nThe third dimension of this array contains names for each specimen, which are obtained from the image names in the .tps file. We can read in the data that we digitized above:\n\ndat &lt;- readland.tps(\"mandible-lat.tps\", specID = \"ID\", readcurves = FALSE, warnmsg = TRUE)\n\n\nNo curves detected; all points appear to be fixed landmarks.\n\ndat\n\n, , ../../images/red-fox-mandible-lateral.jpg\n\n           [,1]      [,2]\n [1,] 107.29125 109.47242\n [2,] 103.69036 108.94577\n [3,] 101.66872 108.37825\n [4,]  43.63469 105.62467\n [5,]  10.59532 111.73118\n [6,]  16.56440 102.17078\n [7,]  12.09659  98.37737\n [8,]  34.05770  93.06904\n [9,]  66.20128  89.99578\n[10,] 113.50334 107.91870\n\n\nNote that the name saved with the data in this case was the file name. For another example, check out the plethodon data included with geomorph\n\ndata(plethodon)\nattributes(plethodon)\n\n$names\n[1] \"land\"    \"links\"   \"species\" \"site\"    \"outline\"\n\nstr(plethodon)\n\nList of 5\n $ land   : num [1:12, 1:2, 1:40] 8.89 9.27 5.56 1.87 1.28 ...\n  ..- attr(*, \"dimnames\")=List of 3\n  .. ..$ : NULL\n  .. ..$ : NULL\n  .. ..$ : NULL\n $ links  : num [1:14, 1:2] 4 3 2 1 1 6 7 8 9 10 ...\n $ species: Factor w/ 2 levels \"Jord\",\"Teyah\": 1 1 1 1 1 1 1 1 1 1 ...\n $ site   : Factor w/ 2 levels \"Allo\",\"Symp\": 2 2 2 2 2 2 2 2 2 2 ...\n $ outline: num [1:3631, 1:2] 0.399 0.4 0.401 0.403 0.404 ...\n\n\n3D coordinates in tps format simply have a third dimension (z in addition to x and y) in each spreadsheet (one spreadsheet or table per specimen).\n\nSource: Emma Sherratʻs quick guide to Geomorph\n\n\nSemi-Landmarks\nOftentimes, there is more shape variation than what is captured by the landmarks themselves, which are only points. In addition, sometimes curves and surfaces have no distinct feature that can be reliably captured as the same homologous point from specimen to specimen. For example, the curvature of a smooth shell may lack any distinct landmarks. In both of these cases, semi-landmarks can be helpful.\nSemi-landmarks are regularly spaced points between two landmarks or along a curve or a surface.\nSliding semi-landmarks are spaced linearly along a curve.\nSurface semi-landmarks are spread out evenly in 3D along a surface.\nNOTE: One thing to note is that any error in the landmarks will also be propagated to the semilandmarks.\n\n\n\nFigure: Landmark and semilandmark data displayed on the caecilian Siphonops annulatus BMNH 1956.1.15.88. Points are colored as follows: landmarks (red), sliding semilandmarks (“curve points,” yellow), and surface semilandmarks (“surface points,” blue). For information regarding each cranial region, see Bardua et al. (2019). BMNH, Natural History Museum, London, UK.\n\n\nSource: Bardua et al 2019\nSee (Bardua et al. 2019) for an excellent review article and practical guide.\nAnnotated 3D version of this figure available at: https://sketchfab.com/3d-models/add35e2e8af94839b1f577bfcee32e54\nLetʻs practice digitizing 3D landmarks with the geomorph vignette: https://cran.r-project.org/web/packages/geomorph/vignettes/geomorph.digitize3D.html\n\n\nMore geomorphic morphometrics software\nhttps://academic.oup.com/view-large/223239151\n\n\n\n\n\nReferences\n\nBardua, C, R N Felice, A Watanabe, A -C Fabre, and A Goswami. 2019. “A Practical Guide to Sliding and Surface Semilandmarks in Morphometric Analyses.” Integrative Organismal Biology 1 (1). https://doi.org/10.1093/iob/obz016."
  },
  {
    "objectID": "posts/2023-02-28-dplyr/index.html",
    "href": "posts/2023-02-28-dplyr/index.html",
    "title": "Getting data in shape with dplyr",
    "section": "",
    "text": "Pre-lecture materials\n\n🌴\n\nRead ahead\n\n\n\n\n\n\nRead ahead\n\n\n\nBefore class, you can prepare by reading the following materials:\n\nhttps://r4ds.had.co.nz/tibbles\nhttps://jhudatascience.org/tidyversecourse/wrangle-data.html#data-wrangling\ndplyr cheat sheet from RStudio\n\n\n\n\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\n\nhttps://www.stephaniehicks.com/jhustatcomputing2022/posts/2022-09-06-managing-data-frames-with-tidyverse/\nhttps://rdpeng.github.io/Biostat776/lecture-managing-data-frames-with-the-tidyverse\nhttps://jhudatascience.org/tidyversecourse/get-data.html#tibbles\n\n\n\n\nLearning objectives\n\n\n\n\n\n\nLearning objectives\n\n\n\nAt the end of this lesson you will:\n\nUnderstand the tools available to get data into the proper structure and shape for downstream analyses\nLearn about the dplyr R package to manage data frames\nRecognize the key verbs (functions) to manage data frames in dplyr\nUse the “pipe” operator to combine verbs together\n\n\n\n\n\nOverview\nIt is still important to understand base R manipulations, particularly for things such as cleaning raw data, troubleshooting, and writing custom functions. But the tidyverse provides many useful tools for data manipuation and analysis of cleaned data. In this session we will learn about dplyr and friends.\n\n\nTidy data\nThe tidyverse has many slogans. A particularly good one for all data analysis is the notion of tidy data.\nAs defined by Hadley Wickham in his 2014 paper published in the Journal of Statistical Software, a tidy dataset has the following properties:\n\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a table.\n\n\n\n\nArtwork by Allison Horst on tidy data\n\n\n[Source: Artwork by Allison Horst]\n\n\nWhat shapes does the data need to be in?\nBeyond the data being tidy, however, we also need to think about what shape it needs to be in. Weʻll review concepts and tools in the next two lessons.\nNow that we have had some experience plotting our data, we can see the value of having rectangular dataframes. We can also see that for particular graphics and analyses, we need to have the data arranged in particular ways.\nFor example, take a look at this elegant graphic below. This single graphic is packed with information on fat, BMR, TEE, and activity levels, all for mulitple species. Is it more effective that individual bar plots? This arrangement is so helpful because you can imagine questions that can be answered with it by comparing the different aspects of the data.\n\n\n\n\n\n\nA very informative figure!\n\n\n\n Source: Gibbons, 2022 based on data of H. Ponzer et al., NATURE, 533:90, 2016\n\n\n\n\n\n\n\n\nCan you imagine what this dataset looks like in terms of organization?\n\n\n\n\nFirst imagine what it would look like variable by variable.\nHow might you intially plot the data?\nWhat organization would you need to make a single figure such as this?\n\n\n\nWe often do not know exactly what we need at the start of a data analysis. We have to play around with different data structures, rearrange the data, look for interesting plots to try, rerrange to fit the input requirements of new functions weʻve discovered, and so on.\n\n\nTibbles\nThe tidyverse uses as its central data structure, the tibble or tbl_df. Tibbles are a variation on data frames, claimed to be lazy and surly:\n\nThey don’t change variable names or types when you construct the tibble.\nDon’t convert strings to factors (the default behavior in data.frame()).\nComplain more when a variable doesnʻt exist.\nNo row.names() in a tibble. Instead, you must create a new variable.\nDisplay a different summary style for its print() method.\nAllows non-standard R names for variables\nAllows columns to be lists.\n\nHowever, most tidyverse functions also work on data frames. Itʻs up to you.\n\ntibble() constructor\nJust as with data frames, there is a tibble() constructor function, which functions in many ways with similar syntax as the data.frame() constructor.\nIf you havenʻt already done so, install the tidyverse:\n\ninstall.packages(\"tidyverse\")\n\n\nrequire(tibble)\n\nLoading required package: tibble\n\ntibble( iris[1:4,] )  # the first few rows of iris\n\n# A tibble: 4 × 5\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n         &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n1          5.1         3.5          1.4         0.2 setosa \n2          4.9         3            1.4         0.2 setosa \n3          4.7         3.2          1.3         0.2 setosa \n4          4.6         3.1          1.5         0.2 setosa \n\nx &lt;- 1:3\ntibble( x, x * 2 )  # name assigned at construction\n\n# A tibble: 3 × 2\n      x `x * 2`\n  &lt;int&gt;   &lt;dbl&gt;\n1     1       2\n2     2       4\n3     3       6\n\nsilly &lt;- tibble(      # an example of a non-standard names\n  `one - 3` = 1:3,  # name = value syntax\n  `12` = \"numeric\",\n  `:)` = \"smile\",\n)\nsilly\n\n# A tibble: 3 × 3\n  `one - 3` `12`    `:)` \n      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;\n1         1 numeric smile\n2         2 numeric smile\n3         3 numeric smile\n\n\n\n\nas_tibble() coersion\nas_tibble() converts an existing object, such as a data frame or matrix, into a tibble.\n\nas_tibble( iris[1:4,] )  # coercing a dataframe to tibble\n\n# A tibble: 4 × 5\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n         &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n1          5.1         3.5          1.4         0.2 setosa \n2          4.9         3            1.4         0.2 setosa \n3          4.7         3.2          1.3         0.2 setosa \n4          4.6         3.1          1.5         0.2 setosa \n\n\n\n\nAs output\nMost often we will get tibbles returned from tidyverse functions such as read_csv() from the readr package.\n\n\n\nThe dplyr package\nThe dplyr package, which is part of the tidyverse was written to supply a grammar for data manipulation, with verbs for the most common data manipulation tasks.\n\n\n\nArtwork by Allison Horst on the dplyr package\n\n\n[Source: Artwork by Allison Horst]\n\ndplyr functions\n\nselect(): return a subset of the data frame, using a flexible notation\nfilter(): extract a subset of rows from a data frame using logical conditions\narrange(): reorder rows of a data frame\nrelocate(): rearrange the columns of a data frame\nrename(): rename columns in a data frame\nmutate(): add new columns or transform existing variables\nsummarize(): generate summary statistics of the variables in the data frame, by strata if data are hierarchical\n%&gt;%: the “pipe” operator (from magrittr) connects multiple verbs together into a data wrangling pipeline (kind of like making a compound sentence)\n\nNote: Everything dplyr does could already be done with base R. What is different is a new syntax, which allows for more clarity of the data manipulations and the order, and perhaps makes the code more readable.\nInstead of the nested syntax, or typing the dataframe name over and over, we can pipe one operation into the next.\nAnother useful contribution is that dplyr functions are very fast, as many key operations are coded in C++. This will be important for very large datasets or repeated manipulations (say in a simulation study).\n\n\nstarwars dataset\nWe will use the starwars dataset included with dplyr. You should check out the help page for this dataset ?starwars.\nLetʻs start by using the skim() function to check out the dataset:\n\nrequire(dplyr)\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nclass(starwars)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nskimr::skim(starwars)\n\n\nData summary\n\n\nName\nstarwars\n\n\nNumber of rows\n87\n\n\nNumber of columns\n14\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n8\n\n\nlist\n3\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nname\n0\n1.00\n3\n21\n0\n87\n0\n\n\nhair_color\n5\n0.94\n4\n13\n0\n12\n0\n\n\nskin_color\n0\n1.00\n3\n19\n0\n31\n0\n\n\neye_color\n0\n1.00\n3\n13\n0\n15\n0\n\n\nsex\n4\n0.95\n4\n14\n0\n4\n0\n\n\ngender\n4\n0.95\n8\n9\n0\n2\n0\n\n\nhomeworld\n10\n0.89\n4\n14\n0\n48\n0\n\n\nspecies\n4\n0.95\n3\n14\n0\n37\n0\n\n\n\nVariable type: list\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nn_unique\nmin_length\nmax_length\n\n\n\n\nfilms\n0\n1\n24\n1\n7\n\n\nvehicles\n0\n1\n11\n0\n2\n\n\nstarships\n0\n1\n17\n0\n5\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nheight\n6\n0.93\n174.36\n34.77\n66\n167.0\n180\n191.0\n264\n▁▁▇▅▁\n\n\nmass\n28\n0.68\n97.31\n169.46\n15\n55.6\n79\n84.5\n1358\n▇▁▁▁▁\n\n\nbirth_year\n44\n0.49\n87.57\n154.69\n8\n35.0\n52\n72.0\n896\n▇▁▁▁▁\n\n\n\n\n\n\n\nSelecting columns with select()\n\n\n\n\n\n\nExample\n\n\n\nSuppose we wanted to take the first 3 columns only. There are a few ways to do this.\nWe could for example use numerical indices:\n\nnames(starwars)[1:3]\n\n[1] \"name\"   \"height\" \"mass\"  \n\n\nBut we can also use the names directly:\n\nsubset &lt;- select(starwars, c(name, sex:species))\nhead(subset)\n\n# A tibble: 6 × 5\n  name           sex    gender    homeworld species\n  &lt;chr&gt;          &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;  \n1 Luke Skywalker male   masculine Tatooine  Human  \n2 C-3PO          none   masculine Tatooine  Droid  \n3 R2-D2          none   masculine Naboo     Droid  \n4 Darth Vader    male   masculine Tatooine  Human  \n5 Leia Organa    female feminine  Alderaan  Human  \n6 Owen Lars      male   masculine Tatooine  Human  \n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe : normally cannot be used with names or strings, but inside the select() function you can use it to specify a range of variable names.\n\n\n\nBy exclusion\nVariables can be omited using the negative sign withing select():\n\nselect( starwars, -(sex:species))\n\nThe select() function also has several helper functions that allow matching on patterns. So, for example, if you wanted to keep every variable that ends with “color”:\n\nsubset &lt;- select(starwars, ends_with(\"color\"))\nstr(subset)\n\ntibble [87 × 3] (S3: tbl_df/tbl/data.frame)\n $ hair_color: chr [1:87] \"blond\" NA NA \"none\" ...\n $ skin_color: chr [1:87] \"fair\" \"gold\" \"white, blue\" \"white\" ...\n $ eye_color : chr [1:87] \"blue\" \"yellow\" \"red\" \"yellow\" ...\n\n\nOr all variables that start with n or m:\n\nsubset &lt;- select(starwars, starts_with(\"n\") | starts_with(\"m\"))\nstr(subset)\n\ntibble [87 × 2] (S3: tbl_df/tbl/data.frame)\n $ name: chr [1:87] \"Luke Skywalker\" \"C-3PO\" \"R2-D2\" \"Darth Vader\" ...\n $ mass: num [1:87] 77 75 32 136 49 120 75 32 84 77 ...\n\n\nYou can also use more general regular expressions. See the help page (?select) for more details.\n\n\n\nSubsetting with filter()\nThe filter() function is used to extract subsets of rows or observations from a data frame. This function is similar to the existing subset() function in base R, or indexing by logical comparisons.\n\n\n\nArtwork by Allison Horst on filter() function\n\n\n[Source: Artwork by Allison Horst]\n\n\n\n\n\n\nExample\n\n\n\nSuppose we wanted to extract the rows of the starwars data frame where the birthyear is greater than 100:\n\nage100 &lt;- filter(starwars, birth_year &gt; 100)\nhead(age100)\n\n# A tibble: 5 × 14\n  name      height  mass hair_color skin_color eye_color birth_year sex   gender\n  &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n1 C-3PO        167    75 &lt;NA&gt;       gold       yellow           112 none  mascu…\n2 Chewbacca    228   112 brown      unknown    blue             200 male  mascu…\n3 Jabba De…    175  1358 &lt;NA&gt;       green-tan… orange           600 herm… mascu…\n4 Yoda          66    17 white      green      brown            896 male  mascu…\n5 Dooku        193    80 white      fair       brown            102 male  mascu…\n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\n\n\n\nYou can see that there are now only 5 rows in the data frame and the distribution of the birth_year values is.\n\nsummary(age100$birth_year)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    102     112     200     382     600     896 \n\n\nWe can also filter on multiple conditions: and requires both conditions to be true, whereas or requires only one to be true. This time letʻs choose birth_year &lt; 100 and homeworld == \"Tatooine:\n\nage_tat &lt;- filter(starwars, birth_year &lt; 100 & homeworld == \"Tatooine\")\nselect(age_tat, name, height, mass, birth_year, sex)\n\n# A tibble: 8 × 5\n  name               height  mass birth_year sex   \n  &lt;chr&gt;               &lt;int&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt; \n1 Luke Skywalker        172    77       19   male  \n2 Darth Vader           202   136       41.9 male  \n3 Owen Lars             178   120       52   male  \n4 Beru Whitesun lars    165    75       47   female\n5 Biggs Darklighter     183    84       24   male  \n6 Anakin Skywalker      188    84       41.9 male  \n7 Shmi Skywalker        163    NA       72   female\n8 Cliegg Lars           183    NA       82   male  \n\n\nOther logical operators you should be aware of include:\n\n\n\n\n\n\n\n\nOperator\nMeaning\nExample\n\n\n\n\n==\nEquals\nhomeworld == Tatooine\n\n\n!=\nDoes not equal\nhomeworld != Tatooine\n\n\n&gt;\nGreater than\nheight &gt; 170.0\n\n\n&gt;=\nGreater than or equal to\nheight &gt;= 170.0\n\n\n&lt;\nLess than\nheight &lt; 170.0\n\n\n&lt;=\nLess than or equal to\nheight &lt;= 170.0\n\n\n%in%\nIncluded in\nhomeworld %in% c(\"Tatooine\", \"Naboo\")\n\n\nis.na()\nIs a missing value\nis.na(mass)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you are ever unsure of how to write a logical statement, but know how to write its opposite, you can use the ! operator to negate the whole statement.\nA common use of this is to identify observations with non-missing data (e.g., !(is.na(homweworld))).\n\n\n\n\nSorting data with arrange()\narrange() is like the sort function in a spreadsheet, or order() in base R. arrange() reorders rows of a data frame according to one of the columns. Think of this as sorting your rows on the value of a column.\nHere we can order the rows of the data frame by birth_year, so that the first row is the earliest (oldest) observation and the last row is the latest (most recent) observation.\n\nstarwars &lt;- arrange(starwars, birth_year)\n\nWe can now check the first few rows\n\nhead(select(starwars, name, birth_year), 3)\n\n# A tibble: 3 × 2\n  name                  birth_year\n  &lt;chr&gt;                      &lt;dbl&gt;\n1 Wicket Systri Warrick          8\n2 IG-88                         15\n3 Luke Skywalker                19\n\n\nand the last few rows.\n\ntail(select(starwars, name, birth_year), 3)\n\n# A tibble: 3 × 2\n  name           birth_year\n  &lt;chr&gt;               &lt;dbl&gt;\n1 Poe Dameron            NA\n2 BB8                    NA\n3 Captain Phasma         NA\n\n\nColumns can be arranged in descending order using the helper function desc().\n\nstarwars &lt;- arrange(starwars, desc(birth_year))\n\nLooking at the first three and last three rows shows the dates in descending order.\n\nhead(select(starwars, name, birth_year), 3)\n\n# A tibble: 3 × 2\n  name                  birth_year\n  &lt;chr&gt;                      &lt;dbl&gt;\n1 Yoda                         896\n2 Jabba Desilijic Tiure        600\n3 Chewbacca                    200\n\ntail(select(starwars, name, birth_year), 3)\n\n# A tibble: 3 × 2\n  name           birth_year\n  &lt;chr&gt;               &lt;dbl&gt;\n1 Poe Dameron            NA\n2 BB8                    NA\n3 Captain Phasma         NA\n\n\n\n\nRearranging columns with relocate()\nMoving a column to a new location is done by specifying the column names, and indicating where they go with the .before= or .after= arguments specifing a location (another column).\nrelocate(.data, ..., .before = NULL, .after = NULL)\n\n\nRenaming columns with rename()\nRenaming a variable in a data frame in R is accomplished using the names() function. The rename() function is designed to make this process easier.\nHere you can see the names of the first six variables in the starwars data frame.\n\nhead(starwars[, 1:6], 3)\n\n# A tibble: 3 × 6\n  name                  height  mass hair_color skin_color       eye_color\n  &lt;chr&gt;                  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;            &lt;chr&gt;    \n1 Yoda                      66    17 white      green            brown    \n2 Jabba Desilijic Tiure    175  1358 &lt;NA&gt;       green-tan, brown orange   \n3 Chewbacca                228   112 brown      unknown          blue     \n\n\nSuppose we wanted to drop the _color. The syntax is newname = oldname:\n\nstarwars &lt;- rename(starwars, hair = hair_color, skin = skin_color, eye = eye_color)\nhead(starwars[, 1:6], 3)\n\n# A tibble: 3 × 6\n  name                  height  mass hair  skin             eye   \n  &lt;chr&gt;                  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;            &lt;chr&gt; \n1 Yoda                      66    17 white green            brown \n2 Jabba Desilijic Tiure    175  1358 &lt;NA&gt;  green-tan, brown orange\n3 Chewbacca                228   112 brown unknown          blue  \n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow would you do the equivalent in base R without dplyr?\n\n\n\n\nAdding columns with mutate()\nThe mutate() function computes transformations of variables in a data frame.\n\n\n\nArtwork by Allison Horst on mutate() function\n\n\n[Source: Artwork by Allison Horst]\nFor example, we may want to adjust height for mass:\n\nstarwars &lt;- mutate(starwars, heightsize = height / mass )\nhead(starwars)\n\n# A tibble: 6 × 15\n  name  height  mass hair  skin  eye   birth_year sex   gender homeworld species\n  &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;  \n1 Yoda      66    17 white green brown        896 male  mascu… &lt;NA&gt;      Yoda's…\n2 Jabb…    175  1358 &lt;NA&gt;  gree… oran…        600 herm… mascu… Nal Hutta Hutt   \n3 Chew…    228   112 brown unkn… blue         200 male  mascu… Kashyyyk  Wookiee\n4 C-3PO    167    75 &lt;NA&gt;  gold  yell…        112 none  mascu… Tatooine  Droid  \n5 Dooku    193    80 white fair  brown        102 male  mascu… Serenno   Human  \n6 Qui-…    193    89 brown fair  blue          92 male  mascu… &lt;NA&gt;      Human  \n# ℹ 4 more variables: films &lt;list&gt;, vehicles &lt;list&gt;, starships &lt;list&gt;,\n#   heightsize &lt;dbl&gt;\n\n\nThere is also the related transmute() function, which mutate()s and keeps only the transformed variables. Therefore, the result is only two columns in the transmuted data frame.\n\n\nPerform functions on groups using group_by()\nThe group_by() function is used to indicate groups within the data.\nFor example, what is the average height by homeworld?\nIn conjunction with the group_by() function, we often use the summarize() function.\n\n\n\n\n\n\nNote\n\n\n\nThe general operation here is a combination of\n\nSplitting a data frame by group defined by a variable or group of variables (group_by())\nsummarize() across those subsets\n\n\n\n\n\n\n\n\n\nExample\n\n\n\nWe can create a separate data frame that splits the original data frame by homeworld.\n\nworlds &lt;- group_by(starwars, homeworld)\n\nCompute summary statistics by planet (just showing mean and median here, almost any summary stat is available):\n\nsummarize(worlds, height = mean(height, na.rm = TRUE), \n          maxheight = max(height, na.rm = TRUE),\n          mass = median(mass, na.rm = TRUE))\n\n# A tibble: 49 × 4\n   homeworld      height maxheight  mass\n   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 Alderaan         176.      176.  64  \n 2 Aleen Minor       79        79   15  \n 3 Bespin           175       175   79  \n 4 Bestine IV       180       180  110  \n 5 Cato Neimoidia   191       191   90  \n 6 Cerea            198       198   82  \n 7 Champala         196       196   NA  \n 8 Chandrila        150       150   NA  \n 9 Concord Dawn     183       183   79  \n10 Corellia         175       175   78.5\n# ℹ 39 more rows\n\n\n\n\nsummarize() returns a data frame with homeworld as the first column, followed by the requested summary statistics. This is similar to the base R function aggregate().\n\n\n\n\n\n\nMore complicated example\n\n\n\nIn a slightly more complicated example, we might want to know what are the average masses within quintiles of height:\nFirst, we can create a categorical variable of height5 divided into quintiles\n\nqq &lt;- quantile(starwars$height, seq(0, 1, 0.2), na.rm = TRUE)\nstarwars &lt;- mutate(starwars, height.quint = cut(height, qq))\n\nNow we can group the data frame by the height.quint variable.\n\nquint &lt;- group_by(starwars, height.quint)\n\nFinally, we can compute the mean of mass within quintiles of height.\n\nsummarize(quint, mquint = mean(mass, na.rm = TRUE))\n\n# A tibble: 6 × 2\n  height.quint mquint\n  &lt;fct&gt;         &lt;dbl&gt;\n1 (66,165]       44.2\n2 (165,175]     187. \n3 (175,183]      79.2\n4 (183,193]      80.2\n5 (193,264]     106. \n6 &lt;NA&gt;           17  \n\n\n\n\nOddly enough there is a maximum mass in the second height quintile of Starwars characters. The biologist in me thinks maybe outliers?\n\n\nPiping multiple functions using %&gt;%\nThe pipe operator %&gt;% is very handy for stringing together multiple dplyr functions in a sequence of operations. It comes from the magritter package.\n Source:\nIn base R, there are two styles of applying multiple functions. The first is the resave the object after each operation.\nThe second is to nest functions, with the first at the deepest level (the heart of the onion), then working our way out:\n\nthird(second(first(x)))\n\nThe %&gt;% operator allows you to string operations in a left-to-right fashion, where the output of one flows into the next, i.e.:\n\nfirst(x) %&gt;% second %&gt;% third\n\n\n\n\n\n\n\nExample\n\n\n\nTake the example that we just did in the last section.\nThat can be done with the following sequence:\n\nstarwars %&gt;% \n  group_by(homeworld) %&gt;% \n  summarize(height = mean(height, na.rm = TRUE), \n          maxheight = max(height, na.rm = TRUE),\n          mass = median(mass, na.rm = TRUE))\n\n# A tibble: 49 × 4\n   homeworld      height maxheight  mass\n   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 Alderaan         176.      176.  64  \n 2 Aleen Minor       79        79   15  \n 3 Bespin           175       175   79  \n 4 Bestine IV       180       180  110  \n 5 Cato Neimoidia   191       191   90  \n 6 Cerea            198       198   82  \n 7 Champala         196       196   NA  \n 8 Chandrila        150       150   NA  \n 9 Concord Dawn     183       183   79  \n10 Corellia         175       175   78.5\n# ℹ 39 more rows\n\n\n\n\n\nData masking\nNotice that we did not have to specify the dataframe. This is because dplyr functions are built on a data masking syntax. From the dplyr data-masking help page:\n\nData masking allows you to refer to variables in the “current” data frame (usually supplied in the .data argument), without any other prefix. It’s what allows you to type (e.g.) filter(diamonds, x == 0 & y == 0 & z == 0) instead of diamonds[diamonds$x == 0 & diamonds$y == 0 & diamonds$z == 0, ]\n\nWhen you look at the help page for ?mutate for example, you will see a function definition like so:\n\nmutate(.data, ...)\n\nNote the .data, Which means that the data can be supplied as usual, or it can be inherited from the “current” data frame which is passed to it via a pipe.\n\n\n\nSample rows of data with slice_*()\nThe slice_sample() function will randomly sample rows of data.\nThe number of rows to show is specified by the n argument.\n\nThis can be useful if you do not want to print the entire tibble, but you want to get a greater sense of the variation.\n\n\n\n\n\n\n\nExample\n\n\n\n\nslice_sample(starwars, n = 10)\n\n# A tibble: 10 × 16\n   name         height  mass hair  skin  eye   birth_year sex   gender homeworld\n   &lt;chr&gt;         &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;    \n 1 Darth Vader     202   136 none  white yell…       41.9 male  mascu… Tatooine \n 2 San Hill        191    NA none  grey  gold        NA   male  mascu… Muunilin…\n 3 Bail Presto…    191    NA black tan   brown       67   male  mascu… Alderaan \n 4 Mon Mothma      150    NA aubu… fair  blue        48   fema… femin… Chandrila\n 5 Zam Wesell      168    55 blon… fair… yell…       NA   fema… femin… Zolan    \n 6 Anakin Skyw…    188    84 blond fair  blue        41.9 male  mascu… Tatooine \n 7 Cliegg Lars     183    NA brown fair  blue        82   male  mascu… Tatooine \n 8 Sebulba         112    40 none  grey… oran…       NA   male  mascu… Malastare\n 9 Quarsh Pana…    183    NA black dark  brown       62   &lt;NA&gt;  &lt;NA&gt;   Naboo    \n10 Gregar Typho    185    85 black dark  brown       NA   male  mascu… Naboo    \n# ℹ 6 more variables: species &lt;chr&gt;, films &lt;list&gt;, vehicles &lt;list&gt;,\n#   starships &lt;list&gt;, heightsize &lt;dbl&gt;, height.quint &lt;fct&gt;\n\n\n\n\nYou can also use slice_head() or slice_tail() to take a look at the top rows or bottom rows of your tibble. Again the number of rows can be specified with the n argument.\nThis will show the first 5 rows.\n\nslice_head(starwars, n = 5)\n\n# A tibble: 5 × 16\n  name  height  mass hair  skin  eye   birth_year sex   gender homeworld species\n  &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;  \n1 Yoda      66    17 white green brown        896 male  mascu… &lt;NA&gt;      Yoda's…\n2 Jabb…    175  1358 &lt;NA&gt;  gree… oran…        600 herm… mascu… Nal Hutta Hutt   \n3 Chew…    228   112 brown unkn… blue         200 male  mascu… Kashyyyk  Wookiee\n4 C-3PO    167    75 &lt;NA&gt;  gold  yell…        112 none  mascu… Tatooine  Droid  \n5 Dooku    193    80 white fair  brown        102 male  mascu… Serenno   Human  \n# ℹ 5 more variables: films &lt;list&gt;, vehicles &lt;list&gt;, starships &lt;list&gt;,\n#   heightsize &lt;dbl&gt;, height.quint &lt;fct&gt;\n\n\nThis will show the last 5 rows.\n\nslice_tail(starwars, n = 5)\n\n# A tibble: 5 × 16\n  name  height  mass hair  skin  eye   birth_year sex   gender homeworld species\n  &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;  \n1 Finn      NA    NA black dark  dark          NA male  mascu… &lt;NA&gt;      Human  \n2 Rey       NA    NA brown light hazel         NA fema… femin… &lt;NA&gt;      Human  \n3 Poe …     NA    NA brown light brown         NA male  mascu… &lt;NA&gt;      Human  \n4 BB8       NA    NA none  none  black         NA none  mascu… &lt;NA&gt;      Droid  \n5 Capt…     NA    NA unkn… unkn… unkn…         NA &lt;NA&gt;  &lt;NA&gt;   &lt;NA&gt;      &lt;NA&gt;   \n# ℹ 5 more variables: films &lt;list&gt;, vehicles &lt;list&gt;, starships &lt;list&gt;,\n#   heightsize &lt;dbl&gt;, height.quint &lt;fct&gt;\n\n\n\n\n\nSummary\nThe dplyr package provides an alternative syntax for manipulating data frames. In particular, we can often conduct the beginnings of an exploratory analysis with the powerful combination of group_by() and summarize().\nOnce you learn the dplyr grammar there are a few additional benefits\n\ndplyr can work with other data frame “back ends” such as SQL databases. There is an SQL interface for relational databases via the DBI package\ndplyr can be integrated with the data.table package for large fast tables\nMany people like the piping syntax for readability and clarity\n\n\n\nPost-lecture materials\n\nFinal Questions\n\n\n\n\n\n\nQuestions\n\n\n\n\nHow can you tell if an object is a tibble?\nUsing the trees dataset in base R (this dataset stores the girth, height, and volume for Black Cherry Trees) and using the pipe operator:\n\nconvert the data.frame to a tibble.\nfilter for rows with a tree height of greater than 70, and\norder rows by Volume (smallest to largest).\n\n\n\nhead(trees)\n\n  Girth Height Volume\n1   8.3     70   10.3\n2   8.6     65   10.3\n3   8.8     63   10.2\n4  10.5     72   16.4\n5  10.7     81   18.8\n6  10.8     83   19.7\n\n\n\n\n\n\nAdditional Resources\n\n\n\n\n\n\nTip\n\n\n\n\nhttps://jhudatascience.org/tidyversecourse/wrangle-data.html#data-wrangling\ndplyr cheat sheet from RStudio"
  },
  {
    "objectID": "posts/2023-03-23-lists-loops/index.html",
    "href": "posts/2023-03-23-lists-loops/index.html",
    "title": "Lists and For-Loops",
    "section": "",
    "text": "Learning objectives\n\n\n\nAt the end of this lesson you will:\n\nUnderstand the special features of lists\nBe able to access list elements and write to lists\nBe able to construct a for loop for repeated computation\nHave gained another skill in modular programming"
  },
  {
    "objectID": "posts/2023-03-23-lists-loops/index.html#accessing-list-elements",
    "href": "posts/2023-03-23-lists-loops/index.html#accessing-list-elements",
    "title": "Lists and For-Loops",
    "section": "Accessing list elements",
    "text": "Accessing list elements\n\n\n\n\n\n\nA lot of people get tripped up working with lists, but the same rules apply to lists as other objects.\n\n\n\nThere are just a couple of additional things:\n\nThe double bracket, and\n\nThe heirarchy of objects.\n\nOnce you understand that, itʻs simply applying the rules.\n\n\nList elements can be accessed with the usual operators for vectors:\n\n$ If the list is named\n[ ] By number or name of the list element with single brackets. Returns a list. Can use a vector of indices or names.\n[[ ]] By number or name with double brackets. Returns the element inside the list slot. Must be a single index or name.\n\nBy name This is why itʻs a good idea to name list elements.\n\napplicant$fullname\n\n[1] \"Mickey Mouse\"\n\napplicant[1]   ## returns a list of length one\n\n$fullname\n[1] \"Mickey Mouse\"\n\napplicant[[1]]  ## returns the object within applicant[1]\n\n[1] \"Mickey Mouse\"\n\n\nSingle brackets return lists. We can select multiple elements within single brackets:\n\napplicant[1:2]\n\n$fullname\n[1] \"Mickey Mouse\"\n\n$address\n[1] \"123 Main St.\"\n\napplicant[c(\"fullname\", \"address\")]  \n\n$fullname\n[1] \"Mickey Mouse\"\n\n$address\n[1] \"123 Main St.\"\n\n\nDouble brackets return the element within the list slot. But we can only select one:\n\napplicant[[1]]\n\n[1] \"Mickey Mouse\"\n\napplicant[[\"fullname\"]]\n\n[1] \"Mickey Mouse\"\n\n\n\napplicant[[1:2]]  ## cannot subset [[]] with more than one index\n\nError in applicant[[1:2]] : subscript out of bounds Error in applicant[[1:2]] : subscript out of bounds\nExclusion index (drops the state slot):\n\napplicant[-3]\n\n$fullname\n[1] \"Mickey Mouse\"\n\n$address\n[1] \"123 Main St.\"\n\n$scores\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    3    5    7    9\n[2,]    2    4    6    8   10\n\n\nAccessing elements inside an object within a list: Here we want to access elements of a matrix which is in a list.\n\napplicant[4]\n\n$scores\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    3    5    7    9\n[2,]    2    4    6    8   10\n\napplicant[[4]][2,1]  # Take the scores matrix, and grab row 2, column 1.\n\n[1] 2\n\napplicant[[4]][,3]  # Take the scores matrix, and grab all of column 3.\n\n[1] 5 6"
  },
  {
    "objectID": "projects/2023-01-24-project-0/index.html",
    "href": "projects/2023-01-24-project-0/index.html",
    "title": "Project 0 (optional)",
    "section": "",
    "text": "This exercise is modified from material developed by Stephanie Hicks."
  },
  {
    "objectID": "projects/2023-01-24-project-0/index.html#acknowledgements",
    "href": "projects/2023-01-24-project-0/index.html#acknowledgements",
    "title": "Project 0 (optional)",
    "section": "",
    "text": "This exercise is modified from material developed by Stephanie Hicks."
  },
  {
    "objectID": "projects/2023-03-22-oral-presentation/index.html",
    "href": "projects/2023-03-22-oral-presentation/index.html",
    "title": "Oral Presentation",
    "section": "",
    "text": "Oral Presentation\nOn the last day of class, we will have oral presentations.\nThey will be informal talks on your final projects. I want you to cover the following in about 10 minutes:\n\nBrief introduction to the disciplinary (biological, etc.) problem + More introduction to the data science (cleaning+analysis) problem.\n\nBriefly but Clearly describe your data, and itʻs structure\nWhat are you starting with? What are you aiming to get to?\n\nMethods - how did you do what you needed to do? Be sure to mention any new packages or tricky code you had to figure out. These often identify data manipulation steps that turn out to be critical.\nResults - what do the data show? What else did you find out?\nConclusions - what does it mean? What potential do you see here? If itʻs not finished, speculate on what you might be able to infer or what questions it might answer?\nWhat did you learn? What was most interesting, confusing, fun to figure out? Whatever you want to say\n\nSlides are optional, but can be helpful if you need a prompt to remember what to say. Or you can talk us through your writeup like I do in class, whatever is more comfortable for you. Do show us key elements like your repo, and how to access it, etc."
  },
  {
    "objectID": "projects/2023-02-16-project-1/index.html",
    "href": "projects/2023-02-16-project-1/index.html",
    "title": "Project 1",
    "section": "",
    "text": "This exercise is modified from material developed by Andreas Handel."
  },
  {
    "objectID": "projects/2023-02-16-project-1/index.html#acknowledgements",
    "href": "projects/2023-02-16-project-1/index.html#acknowledgements",
    "title": "Project 1",
    "section": "",
    "text": "This exercise is modified from material developed by Andreas Handel."
  },
  {
    "objectID": "projects/2023-03-22-project-2-3/index.html",
    "href": "projects/2023-03-22-project-2-3/index.html",
    "title": "Projects 2,3",
    "section": "",
    "text": "This exercise is modified from material developed by Andreas Handel."
  },
  {
    "objectID": "projects/2023-03-22-project-2-3/index.html#acknowledgements",
    "href": "projects/2023-03-22-project-2-3/index.html#acknowledgements",
    "title": "Projects 2,3",
    "section": "",
    "text": "This exercise is modified from material developed by Andreas Handel."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Oral Presentation\n\n\n\n\n\n\n\nproject 3\n\n\nprojects\n\n\n\n\nInformation for Final Oral Presenatation\n\n\n\n\n\n\nApr 27, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n  \n\n\n\n\nProjects 2,3\n\n\n\n\n\n\n\nproject 2\n\n\nproject 3\n\n\nprojects\n\n\n\n\nInformation for Projects 2 and 3\n\n\n\n\n\n\nMar 22, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n  \n\n\n\n\nProject 1\n\n\n\n\n\n\n\nproject 1\n\n\nprojects\n\n\n\n\nInformation for Project 1\n\n\n\n\n\n\nJan 30, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n  \n\n\n\n\nProject 0 (optional)\n\n\n\n\n\n\n\nproject 0\n\n\nprojects\n\n\n\n\nInformation for Project 0 (entirely optional, but hopefully useful and fun!)\n\n\n\n\n\n\nJan 23, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Introduction to Data Science in R for Biologists!",
    "section": "",
    "text": "Welcome to Data Science for Biologists at the University of Hawaiʻi!"
  },
  {
    "objectID": "index.html#what-is-this-course",
    "href": "index.html#what-is-this-course",
    "title": "Welcome to Introduction to Data Science in R for Biologists!",
    "section": "What is this course?",
    "text": "What is this course?\nThis course covers the basics of computational and programming skills required for research in biological sciences and related disciplines. We will cover practical issues in data organization and management as well as programming in R and the tidyverse. Some of the topics will include: data ethics, best practices for coding and reproducible research, introduction to data visualizations, best practices for working with special data types (dates/times, text data, etc), best practices for storing data, basics of debugging, organizing and commenting code, basics of interacting with other computational resources from R. Topics in statistical data analysis, morphometrics, phylogenetic tree visualization, and other practical examples provide working examples."
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Welcome to Introduction to Data Science in R for Biologists!",
    "section": "Getting started",
    "text": "Getting started\nPlease look over the Syllabus and Schedule under General Information. Lectures are provided under the course materials tab."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Welcome to Introduction to Data Science in R for Biologists!",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis course was developed and is maintained by Marguerite Butler.\nA big thank you to Stephanie Hicks for generously sharing the beautifully designed quarto template for this course.\nMaterials have been adapted from courses developed by the following individuals (more to come): Stephanie Hicks.\nThe course materials are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Linked and embedded materials are governed by their own licenses. I assume that all external materials used or embedded here are covered under the educational fair use policy. If this is not the case and any material displayed here violates copyright, please let me know and I will remove it."
  },
  {
    "objectID": "index.html#useful-free-r-resources",
    "href": "index.html#useful-free-r-resources",
    "title": "Welcome to Introduction to Data Science in R for Biologists!",
    "section": "Useful (Free) R Resources",
    "text": "Useful (Free) R Resources\nIntro to R (by the R Core Group): https://cran.r-project.org/doc/manuals/r-release/R-intro.html R for Data Science: http://r4ds.had.co.nz/ Intro to Data Science: http://rafalab.dfci.harvard.edu/dsbook/ Various “Cheat Sheets”: https://www.rstudio.com/resources/cheatsheets/ DataCamp: http://www.datacamp.com R reference card: http://cran.r-project.org/doc/contrib/Short-refcard.pdfUCLA R Data Import/Export (by the R Core Group): https://cran.r-project.org/doc/manuals/r-release/R-data.html"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "lectures.html",
    "href": "lectures.html",
    "title": "Lectures",
    "section": "",
    "text": "Introduction to OU Models\n\n\n\n\n\n\n\nmodule 8\n\n\nweek 15\n\n\ncomparative analysis\n\n\nphylogenetic trees\n\n\nOrnstein Uhlenbeck\n\n\nBrownian motion\n\n\n\n\nComparative analyses with Ornstein-Uhlenbeck models\n\n\n\n\n\n\nApr 27, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n  \n\n\n\n\nThe ggtree-verse\n\n\n\n\n\n\n\nmodule 7\n\n\nweek 13\n\n\nphylogenetic trees\n\n\nphylo\n\n\nggtree\n\n\ntreedata\n\n\n\n\nWorking with trees with tidytree and plotting with ggtree\n\n\n\n\n\n\nApr 25, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n  \n\n\n\n\nAll about trees\n\n\n\n\n\n\n\nmodule 7\n\n\nweek 13\n\n\nphylogenetic trees\n\n\nnexus\n\n\nfasta\n\n\nnewick\n\n\nbeast\n\n\niqtree\n\n\n\n\nGetting trees in to R and understanding their formats\n\n\n\n\n\n\nApr 18, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n  \n\n\n\n\nSetting Up for IQTREE2\n\n\n\n\n\n\n\nmodule 7\n\n\nweek 13\n\n\niqtree2\n\n\nphylogenetic inference\n\n\ncommand line\n\n\n\n\nGetting set up for phylogenetic inference using IQTREE2 on your local machine\n\n\n\n\n\n\nApr 13, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n  \n\n\n\n\nPhylogenetic Inference with IQTREE2\n\n\n\n\n\n\n\nmodule 7\n\n\nweek 13\n\n\niqtree2\n\n\nphylogenetic inference\n\n\ncommand line\n\n\ngene concordance\n\n\nsite concordance\n\n\ngene tree\n\n\nspecies tree\n\n\n\n\nMaximum Likelihood phylogenetic tree inference and gene-tree species-tree concordance\n\n\n\n\n\n\nApr 13, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n  \n\n\n\n\nProcrustes Superimposition and Analyses\n\n\n\n\n\n\n\nmodule 6\n\n\nweek 13\n\n\ngeometric morphometrics\n\n\nGPA\n\n\nprocrustes superimposition\n\n\nsize\n\n\nshape\n\n\n\n\nA tour of analyses with geometric morphometric data\n\n\n\n\n\n\nApr 11, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n  \n\n\n\n\nMorphometrics Overview\n\n\n\n\n\n\n\nmodule 6\n\n\nweek 12\n\n\ncontrol structures\n\n\nif else\n\n\n(do) while\n\n\nprogramming\n\n\n\n\nSome important considerations in a typical workflow\n\n\n\n\n\n\nApr 4, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n  \n\n\n\n\nIntro to Geometric Morphometrics\n\n\n\n\n\n\n\nmodule 6\n\n\nweek 12\n\n\ncontrol structures\n\n\nif else\n\n\n(do) while\n\n\nprogramming\n\n\n\n\nGetting started with morphometrics\n\n\n\n\n\n\nApr 4, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n  \n\n\n\n\nMeasurement Error\n\n\n\n\n\n\n\nmodule 6\n\n\nweek 12\n\n\ncontrol structures\n\n\nif else\n\n\n(do) while\n\n\nprogramming\n\n\n\n\nNo measurements are perfect, so quantifying repeatability is important\n\n\n\n\n\n\nMar 30, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n  \n\n\n\n\nIntro to Morphometrics\n\n\n\n\n\n\n\nmodule 6\n\n\nweek 11\n\n\ncontrol structures\n\n\nif else\n\n\n(do) while\n\n\nprogramming\n\n\n\n\nGetting started with some issues in morphometrics\n\n\n\n\n\n\nMar 30, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n  \n\n\n\n\nProgram Flow\n\n\n\n\n\n\n\nmodule 5\n\n\nweek 11\n\n\ncontrol structures\n\n\nif else\n\n\n(do) while\n\n\nprogramming\n\n\n\n\nControl the flow of your program using programming control statements\n\n\n\n\n\n\nMar 28, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n  \n\n\n\n\nVectorization with Apply Functions\n\n\n\n\n\n\n\nmodule 5\n\n\nweek 10\n\n\napply\n\n\nlists\n\n\nfor loops\n\n\nprogramming\n\n\n\n\nApply functions can help with vectorization and scaling up\n\n\n\n\n\n\nMar 23, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n  \n\n\n\n\nLists and For-Loops\n\n\n\n\n\n\n\nmodule 5\n\n\nweek 10\n\n\nlists\n\n\nfor loops\n\n\nprogramming\n\n\n\n\nDonʻt get frustrated, learn a few simple rules about lists. Then you can massively scale.\n\n\n\n\n\n\nMar 23, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n  \n\n\n\n\nWriting your own functions\n\n\n\n\n\n\n\nmodule 4\n\n\nweek 8\n\n\nprogramming\n\n\nfunctions\n\n\nmethods\n\n\nscope\n\n\n\n\nand all about functions in R\n\n\n\n\n\n\nMar 21, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n  \n\n\n\n\nJoining data with dplyr\n\n\n\n\n\n\n\nmodule 3\n\n\nweek 7\n\n\ntidyr\n\n\ntidyverse\n\n\ndplyr\n\n\ntibble\n\n\npipe\n\n\n\n\nAdd\n\n\n\n\n\n\nMar 9, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n  \n\n\n\n\nReshaping data with dplyr\n\n\n\n\n\n\n\nmodule 4\n\n\nweek 8\n\n\ntidyr\n\n\ntidyverse\n\n\ndplyr\n\n\ntibble\n\n\n\n\nAdd\n\n\n\n\n\n\nMar 9, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n  \n\n\n\n\nA small review of univariate parametric statistics\n\n\n\n\n\n\n\nmodule 3\n\n\nweek 8\n\n\nunivariate\n\n\nstatistics\n\n\nggplot2\n\n\ndplyr\n\n\n\n\nAdd\n\n\n\n\n\n\nMar 7, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n  \n\n\n\n\nA small tour of multivariate analysis\n\n\n\n\n\n\n\nmodule 3\n\n\nweek 7\n\n\nmultivariate\n\n\nstatistics\n\n\nR\n\n\n\n\nAdd\n\n\n\n\n\n\nFeb 28, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n  \n\n\n\n\nGetting data in shape with dplyr\n\n\n\n\n\n\n\nmodule 3\n\n\nweek 7\n\n\ntidyr\n\n\ntidyverse\n\n\ndplyr\n\n\ntibble\n\n\npipe\n\n\n\n\nAdd\n\n\n\n\n\n\nFeb 28, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n  \n\n\n\n\nThe ggplot2 package\n\n\n\n\n\n\n\nmodule 3\n\n\nweek 7\n\n\nR\n\n\nprogramming\n\n\nplotting\n\n\nggplot2\n\n\ndata visualization\n\n\n\n\nIntroduction to the gplot2 grammar of graphics\n\n\n\n\n\n\nFeb 21, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n  \n\n\n\n\nPlotting Systems\n\n\n\n\n\n\n\nmodule 3\n\n\nweek 6\n\n\nplotting\n\n\nggplot2\n\n\nlattice\n\n\ndata visualization\n\n\n\n\nShowing you base plotting, lattice, and ggplot2\n\n\n\n\n\n\nFeb 16, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n  \n\n\n\n\nTidying and Exploring Data\n\n\n\n\n\n\n\nmodule 2\n\n\nweek 4\n\n\ndata\n\n\ndata structures\n\n\nobjects\n\n\n\n\nAll about data and how it is represented in R\n\n\n\n\n\n\nFeb 14, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n  \n\n\n\n\nTypes of Data\n\n\n\n\n\n\n\nmodule 2\n\n\nweek 4\n\n\ndata\n\n\ndata structures\n\n\nobjects\n\n\n\n\nAll about data and how it is represented in R\n\n\n\n\n\n\nFeb 9, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n  \n\n\n\n\nSaving your work as R scripts\n\n\n\n\n\n\n\nmodule 1\n\n\nweek 3\n\n\nR\n\n\nscripts\n\n\nreproducibility\n\n\n\n\nAdd\n\n\n\n\n\n\nFeb 7, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n  \n\n\n\n\nWhat is the question?\n\n\n\n\n\n\n\nmodule 2\n\n\nweek 3\n\n\ndata\n\n\nquestions\n\n\ndata mining\n\n\n\n\nLetʻs talk about scientific excellence & what data can and cannot do\n\n\n\n\n\n\nJan 31, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n  \n\n\n\n\nData IO\n\n\n\n\n\n\n\nmodule 2\n\n\nweek 3\n\n\ndata\n\n\ninput\n\n\noutput\n\n\nformats\n\n\n\n\nSo many ways to get data into R\n\n\n\n\n\n\nJan 31, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n  \n\n\n\n\nReference management\n\n\n\n\n\n\n\nmodule 1\n\n\nweek 3\n\n\nQuarto\n\n\nauthoring\n\n\nBibTeX\n\n\nprogramming\n\n\n\n\nHow to use citations and include your bibliography in R Quarto.\n\n\n\n\n\n\nJan 31, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n  \n\n\n\n\nReproducible Research\n\n\n\n\n\n\n\nmodule 1\n\n\nweek 2\n\n\nR\n\n\nreproducibility\n\n\n\n\nIntroduction to reproducible research\n\n\n\n\n\n\nJan 26, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n  \n\n\n\n\nLiterate Statistical Programming and Quarto\n\n\n\n\n\n\n\nmodule 1\n\n\nweek 2\n\n\nMarkdown\n\n\nQuarto\n\n\nprogramming\n\n\n\n\nIntroduction to literate statistical programming tools including Quarto Markdown\n\n\n\n\n\n\nJan 26, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to git/GitHub\n\n\n\n\n\n\n\nmodule 1\n\n\nweek 2\n\n\nprogramming\n\n\nversion control\n\n\ngit\n\n\nGitHub\n\n\n\n\nVersion control is a game changer; or how I learned to love git/GitHub\n\n\n\n\n\n\nJan 24, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to your computerʻs terminal utilities\n\n\n\n\n\n\n\nmodule 1\n\n\nweek 1\n\n\nprogramming\n\n\nfilesystem\n\n\nshell\n\n\n\n\nSo much power; or how I got my computer to do my bidding\n\n\n\n\n\n\nJan 19, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction and The Big Idea\n\n\n\n\n\n\n\nmodule 1\n\n\nweek 1\n\n\nintroduction\n\n\n\n\nThe big idea\n\n\n\n\n\n\nJan 17, 2023\n\n\nMarguerite Butler\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "",
    "text": "Delivery: In person\nCourse time: Tuesdays and Thursdays from 10:30-11:45am\nCourse location: BIL 319A\nAssignments: Weekly small quizzes, four projects\n\n\n\n\nTo add the course: Let me know so I can give you an override\nRegister for ZOOL710 CRN 85424 3 credits.\nAttendance is highly recommended for group work. Lectures will be recorded on request.\nPlease contact course instructor if interested in auditing.\nUndergraduates are welcome to join with approval.\n\n\n\n\n\nMarguerite A. Butler (https://butlerlab.org)\n\nOffice Location: Edmondson 318\nEmail: mbutler808 at gmail.com\nOffice Hours: After class and by appointment\n\n\n\n\n\nIn order of preference, here is a preferred list of ways to get help:\n\nI strongly encourage you to use the course SLACK channel, before joining office hours. You can get your answers faster, and other students in the class (who likely have similar questions) can also benefit from the questions and answers given. Everyone is encouraged to participate.\nYou are welcome to join office hours to get more group interactive feedback.\nIf you are not able to make the office hours, appointments can be made by email."
  },
  {
    "objectID": "syllabus.html#course-information",
    "href": "syllabus.html#course-information",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "",
    "text": "Delivery: In person\nCourse time: Tuesdays and Thursdays from 10:30-11:45am\nCourse location: BIL 319A\nAssignments: Weekly small quizzes, four projects\n\n\n\n\nTo add the course: Let me know so I can give you an override\nRegister for ZOOL710 CRN 85424 3 credits.\nAttendance is highly recommended for group work. Lectures will be recorded on request.\nPlease contact course instructor if interested in auditing.\nUndergraduates are welcome to join with approval.\n\n\n\n\n\nMarguerite A. Butler (https://butlerlab.org)\n\nOffice Location: Edmondson 318\nEmail: mbutler808 at gmail.com\nOffice Hours: After class and by appointment\n\n\n\n\n\nIn order of preference, here is a preferred list of ways to get help:\n\nI strongly encourage you to use the course SLACK channel, before joining office hours. You can get your answers faster, and other students in the class (who likely have similar questions) can also benefit from the questions and answers given. Everyone is encouraged to participate.\nYou are welcome to join office hours to get more group interactive feedback.\nIf you are not able to make the office hours, appointments can be made by email."
  },
  {
    "objectID": "syllabus.html#important-links",
    "href": "syllabus.html#important-links",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "Important Links",
    "text": "Important Links\n\nCourse website: coming soon.\nGitHub repository with all course material: coming soon."
  },
  {
    "objectID": "syllabus.html#learning-objectives",
    "href": "syllabus.html#learning-objectives",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "Learning Objectives:",
    "text": "Learning Objectives:\nUpon successfully completing this course, students will be able to:\n\nInstall and configure software necessary for a statistical programming environment\nDiscuss generic programming language concepts as they are implemented in a high-level statistical language\nWrite and debug code in base R and the tidyverse\nBuild basic data visualizations using R and the tidyverse\nDiscuss best practices for coding and reproducible research, basics of data ethics and management, basics of working with special data types, and basics of storing data"
  },
  {
    "objectID": "syllabus.html#lectures",
    "href": "syllabus.html#lectures",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "Lectures",
    "text": "Lectures\nIn Fall 2023, we will have in person lectures. If requested, I will attempt to record them and post to the website."
  },
  {
    "objectID": "syllabus.html#textbook-and-other-course-material",
    "href": "syllabus.html#textbook-and-other-course-material",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "Textbook and Other Course Material",
    "text": "Textbook and Other Course Material\nThere is no required textbook. We will make use of several freely available textbooks and other materials. All course materials will be provided. We will use the R software for data analysis, and git for version control and data sharing, all of which is freely available for download."
  },
  {
    "objectID": "syllabus.html#software",
    "href": "syllabus.html#software",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "Software",
    "text": "Software\nPlease install R onto your laptop. You can obtain R from the Comprehensive R Archive Network. There are versions available for Mac, Windows, and Unix/Linux. This software is required for this course.\nIt is important that you have the latest version of R installed. For this course we will be using R version 4.2.1 or higher. You can determine what version of R you have by starting up R and typing into the console R.version.string and hitting the return/enter key. If you do not have the proper version of R installed, go to CRAN and download and install the latest version.\nSome students like to use the Rstudio interface, but this is optional. The RStudio interactive development environment (IDE) requires that R be installed, and so is an “add-on” to R. You can obtain the RStudio Desktop for free from the RStudio web site. You can determine the version of RStudio by looking at menu item Help &gt; About RStudio. You should be using RStudio version 1.4.1106 or higher."
  },
  {
    "objectID": "syllabus.html#quizzes",
    "href": "syllabus.html#quizzes",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "Quizzes",
    "text": "Quizzes\nThere will be weekly (short) quizzes on Laulima. These are intended to be low-stakes to assist you in checking your understanding."
  },
  {
    "objectID": "syllabus.html#projects",
    "href": "syllabus.html#projects",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "Projects",
    "text": "Projects\nThere will be one optional assignment and 4 graded assignments, due every 2–3 weeks. Projects will be submitted electronically via GitHub (more on this later).\nThe project assignments will be due on\n\nProject 0: February 3, 11:59pm (optional and not graded but hopefully useful and fun)\nProject 1: February 17, 11:59pm\nProject 2: March 10, 11:59pm\nProject 3: April 7, 11:59pm\nProject 4: May 5, 11:59pm\n\n\nProject collaboration\nPlease feel free to study together and talk to one another about project assignments. The mutual instruction that students give each other is among the most valuable that can be achieved.\nHowever, it is expected that project assignments will be implemented and written up independently unless otherwise specified. Specifically, please do not share analytic code or output. Please do not collaborate on write-up and interpretation. Please do not access or use solutions from any source before your project assignment is submitted for grading."
  },
  {
    "objectID": "syllabus.html#discussion-forum",
    "href": "syllabus.html#discussion-forum",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "Discussion Forum",
    "text": "Discussion Forum\nThe course will make use of SLACK to ask and answer questions and discuss any of the course materials. Please engage and provide answers as well as questions. The Instructor will monitor SLACK and answer questions when appropriate."
  },
  {
    "objectID": "syllabus.html#exams",
    "href": "syllabus.html#exams",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "Exams",
    "text": "Exams\nThere are no exams in this course."
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "Grading",
    "text": "Grading\nGrades in the course will be based on weekly quizzes (20%), participation (10%) and projects (70%). Each of Projects 1–3 counts approximately equally in the final grade. Grades will be posted on Laulima."
  },
  {
    "objectID": "syllabus.html#policy-for-submitted-projects-late",
    "href": "syllabus.html#policy-for-submitted-projects-late",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "Policy for submitted projects late",
    "text": "Policy for submitted projects late\nThe policy for late submissions is as follows:\n\nEach student will be given two free “late days” for the rest of the course.\nA late day extends the individual project deadline by 24 hours without penalty.\nThe late days can be applied to just one project (e.g. two late days for Project 2), or they can be split across the two projects (one late day for Project 2 and one late day for Project 3). This is entirely left up to the discretion of the student.\nLate days are intended to give you flexibility: you can use them for any reason no questions asked.\nYou do not get any bonus points for not using your late days, and they are not transferrable.\n\nFor students who exceed their free late days:\n\nI will be deducting 5% for each extra late day. For example, if you have already used all of your late days for the term, we will deduct 5% for the assignment that is &lt;24 hours late, 10% points for the assignment that is 24-48 hours late, and 15% points for the assignment that is 48-72 hours late, etc.\nI will not grade assignments that are more than 3 days past the original due date.\n\n\nRegrading Policy\nIt is very important to me that all assignments are properly graded. If you believe there is an error in your assignment grading, please send an email within 7 days of receiving the grade explaining the issue. No re-grade requests will be accepted orally, and no regrade requests will be accepted more than 7 days after you receive the grade for the assignment."
  },
  {
    "objectID": "syllabus.html#academic-ethics-and-student-conduct-code",
    "href": "syllabus.html#academic-ethics-and-student-conduct-code",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "Academic Ethics and Student Conduct Code",
    "text": "Academic Ethics and Student Conduct Code\nThe faculty, staff, and students participating in courses of the School of Life Sciences assume a responsibility to uphold the Universityʻs missions of academic excellence and social responsibility as appropriate for an institute of higher education. Violations of the UH Systemwide Student Conduct Code includes but is not limited to: cheating; plagiarism; providing copies of your work to other students which is submitted as their own; obtaining copies of said work by others; using copies of said work or representing any portion of another person’s work as your own (i.e., plagiarism); misconduct. While we encourage you to discuss strategies for problem solving, and even collaborate by working through the problems/strategies together, giving someone all the answers is cheating. If you are unsure please ask.\nPlagiarism is when you use information or present ideas, whether by paraphrase or direct quote, from a source (be it published or a classmate) without giving proper credit to that source. Cheating in any way will be reported to the attention of UH Office of Judicial Affairs, and result in an F in this course. Students should be familiar with the policies and procedures specified under the Systemwide Student Conduct Code portal."
  },
  {
    "objectID": "syllabus.html#disability-support-service",
    "href": "syllabus.html#disability-support-service",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "Disability Support Service",
    "text": "Disability Support Service\nStudents requiring accommodations for disabilities should register with the Kokua program at Student Disability Services. It is the responsibility of the student to register for accommodations. The Kokua office will send me a notification once you are registered, however, they often do not share information regarding the specifics. If the accommodations are not sufficient to ensure your success, please contact me as soon as possible so that we may work together on providing for an effective learning environment."
  },
  {
    "objectID": "syllabus.html#prerequisites",
    "href": "syllabus.html#prerequisites",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "Prerequisites",
    "text": "Prerequisites\nThis is an applied quantitative course. I will not discuss the mathematical details of specific data analysis approaches, however some statistical background and being comfortable with quantitative thinking is useful. Previous experience with writing computer programs in general and R in particular is also helpful, but not necessary. If you have no programming experience, expect to spend extra time getting yourself familiar with R. As long as you are willing to invest the time to learn the programming and you do not mind thinking quantitatively, you should be able to take the course, independent of your background.\n\nGetting set up\nYou must install R and RStudio on your computer in order to complete this course. These are two different applications that must be installed separately before they can be used together:\n\nR is the core underlying programming language and computing engine that we will be learning in this course\nRStudio is an interface into R that makes many aspects of using and programming R simpler\n\nBoth R and RStudio are available for Windows, macOS, and most flavors of Unix and Linux. Please download the version that is suitable for your computing setup.\nThroughout the course, we will make use of numerous R add-on packages that must be installed over the Internet. Packages can be installed using the install.packages() function in R. For example, to install the tidyverse package, you can run\n\ninstall.packages(\"tidyverse\")\n\nin the R console.\n\nHow to Download R for Windows\nGo to https://cran.r-project.org and\n\nClick the link to “Download R for Windows”\nClick on “base”\nClick on “Download R 4.2.1 for Windows”\n\n\n\n\n\n\n\nWarning\n\n\n\nFor all software, please download the latest version.\n\n\n\n\nHow to Download R for the Mac\nGoto https://cran.r-project.org and\n\nClick the link to “Download R for (Mac) OS X”.\nClick on “R-4.2.1.pkg” (or the latest version)\n\n\n\nHow to Download RStudio\nGoto https://rstudio.com and\n\nClick on “Products” in the top menu\nThen click on “RStudio” in the drop down menu\nClick on “RStudio Desktop”\nClick the button that says “DOWNLOAD RSTUDIO DESKTOP”\nClick the button under “RStudio Desktop” Free\nUnder the section “All Installers” choose the file that is appropriate for your operating system."
  },
  {
    "objectID": "syllabus.html#general-disclaimers",
    "href": "syllabus.html#general-disclaimers",
    "title": "Zool710: Data Science in R for Biologists Syllabus",
    "section": "General Disclaimers",
    "text": "General Disclaimers\n\nThis syllabus is a general plan, deviations announced to the class by the instructor may be necessary."
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Learning R\n\nBig Book of R: https://www.bigbookofr.com\nList of resources to learn R (but also Python, SQL, Javascript): https://github.com/delabj/datacamp_alternatives/blob/master/index.md\nlearnr4free. Resources (books, videos, interactive websites, papers) to learn R. Some of the resources are beginner-friendly and start with the installation process: https://www.learnr4free.com/en\nData Science with R by Danielle Navarro: https://robust-tools.djnavarro.net"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "For Qmd files (markdown document with Quarto which are cross-language executable code), go to the course GitHub repository and navigate the directories, or best of all clone the repo and navigate within RStudio or your text browser and R console.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDates\nTopics\nProjects\n\n\n\n\n\nModule 1\n\nComputational Tools for Data Science\n\n\n\n\n\n\n\n\n\n\n\nWeek 1\nJan 17\n👋 Installing R/RStudio, GitHub [html] [Qmd]\n\n\n\n\n\nJan 19\n👩‍💻 Shell scripts, File Organization [html] [Qmd]\n\n\n\n\nWeek 2\nJan 24\n🐙 Intro to Git and GitHub [html] [Qmd]\n🌴 Project 0 [html] [Qmd]\n\n\n\n\n\n\n\n\n\n\nModule 2\n\nReproducible Research\n\n\n\n\n\n\n\n\n\n\n\n\nJan 26\n🐙 Reproducible Research [html] [Qmd]\n\n\n\n\n\nJan 26\n🐙 Literate Programming / Intro to Quarto [html] [Qmd]\n\n\n\n\nWeek 3\nJan 31\n🔬 What is the Question? [html] [Qmd]\n\n\n\n\n\n\n\n\n\n\n\nModule 3\n\nData Analysis in R \n\n\n\n\n\n\n\n\n\n\n\n\nJan 31\n👓 Reading/Writing data [html] [Qmd]\n\n\n\n\n\nFeb 2\n👓 Reference Management [html] [Qmd]\n🍂 Project 0 due\n\n\n\nWeek 4\nFeb 7\n🐙 Scripts [html] [Qmd]\n\n\n\n\n\nFeb 9\n🔩 Data [html] [Qmd]\n\n\n\n\n\n\n\n\n\n\n\nModule 3\n\nData Visulization in R\n\n\n\n\n\n\n\n\n\n\n\nWeek 5\nFeb 14\n🔪 Tidying Data [html] [Qmd]\n\n\n\n\n\nFeb 16\n📊 Plotting Systems (Base R) [html] [Qmd]\n🌱 Project 1 assigned\n\n\n\n\n\n\n\n\n\n\nWeek 6\nFeb 21\n📊 Plotting with ggplot2 [html] [Qmd]\n\n\n\n\n\nFeb 23\n📊 ggplot2 continued\n\n\n\n\n\n\n\n\n\n\n\nModule 4\n\nTour of Stats in R and Data Wrangling\n\n\n\n\n\n\n\n\n\n\n\nWeek 7\nFeb 28\n🐒 Tidyverse and data wrangling [html] [Qmd]\n\n\n\n\n\nMar 2\n🐎 A tour of some multivariate methods in R [html] [Qmd]\n🌱 Project 1 due\n\n\n\n\n\n\n\n\n\n\nWeek 8\nMar 7\n📆 A tour of some univariate methods in R [html] [Qmd]\n\n\n\n\n\nMar 9\n🐎 Joining a.k.a. merging data [html] [Qmd]\n\n\n\n\n\nMar 9\n🐎 Reshaping data [html] [Qmd]\n\n\n\n\n\n\n\n\n\n\n\nModule 5\n\nProgramming Elements\n\n\n\n\n\n\n\n\n\n\n\nWeek 9\nMar 14\n🤝 Spring Break\n\n\n\n\n\nMar 16\n☀️ Spring Break\n\n\n\n\n\n\n\n\n\n\n\nWeek 10\nMar 21\n🍄 Functions [html] [Qmd]\n\n\n\n\n\n\n\n\n\n\n\n\nMar 23\n🌵 Lists and For Loops [html] [Qmd]\n\n\n\n\n\n\n\n\n\n\n\n\nMar 23\n🌵 Apply Functions [html] [Qmd]\n\n\n\n\n\n\n\n\n\n\n\nWeek 11\nMar 28\n🎡 Program Flow [html] [Qmd]\n\n\n\n\n\n\n\n\n\n\n\nModule 6\n\nIntro to Morphometrics\n\n\n\n\n\n\n\n\n\n\n\n\nMar 30\n📏 Intro to Morphometrics [html] [Qmd]\n\n\n\n\n\n\n\n\n\n\n\nWeek 12\nApr 4\n📐 Landmark-based Morphometrics\n🍂 Project 2 due\n\n\n\n\n\n\n\n\n\n\n\nApr 4\n💀 3D Morphometrics\n\n\n\n\n\nApr 6\n🐾 3D Morphometrics\n🌰 Project 3 data check due\n\n\n\n\n\n\n\n\n\n\nModule 7\n\nPhylogenetic Trees\n\n\n\n\n\n\n\n\n\n\n\nWeek 13\nApr 11\n🌱 Trees as Data Objects\n\n\n\n\n\nApr 13\n🌴 Reading and writing trees, ape, ggtree, treeio\n\n\n\n\n\n\n\n\n\n\n\nWeek 14\nApr 18\n🌳 Reshaping Trees\n\n\n\n\n\n\n\n\n\n\n\n\nApr 20\n🌺 Annotating Tree Plots\n📝 Project 3 peer review due\n\n\n\n\n\n\n\n\n\n\nModule 8\n\nOther Topics\n\n\n\n\n\n\n\n\n\n\n\nWeek 15\nApr 25\n🐍 Parsing Data from any Format\n\n\n\n\n\n\n\n\n\n\n\n\nApr 27\n🌰 Student choice topic\n\n\n\n\n\n\n\n\n\n\n\nWeek 16\nMay 2\n🏆 Final Presentations\n💐 Project 3 presentation (in class)\n\n\n\n\n\n\n\n\n\n\n\nMay 5\n\n🎊 Project 3 due"
  },
  {
    "objectID": "schedule.html#schedule-and-course-materials",
    "href": "schedule.html#schedule-and-course-materials",
    "title": "Schedule",
    "section": "",
    "text": "For Qmd files (markdown document with Quarto which are cross-language executable code), go to the course GitHub repository and navigate the directories, or best of all clone the repo and navigate within RStudio or your text browser and R console.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDates\nTopics\nProjects\n\n\n\n\n\nModule 1\n\nComputational Tools for Data Science\n\n\n\n\n\n\n\n\n\n\n\nWeek 1\nJan 17\n👋 Installing R/RStudio, GitHub [html] [Qmd]\n\n\n\n\n\nJan 19\n👩‍💻 Shell scripts, File Organization [html] [Qmd]\n\n\n\n\nWeek 2\nJan 24\n🐙 Intro to Git and GitHub [html] [Qmd]\n🌴 Project 0 [html] [Qmd]\n\n\n\n\n\n\n\n\n\n\nModule 2\n\nReproducible Research\n\n\n\n\n\n\n\n\n\n\n\n\nJan 26\n🐙 Reproducible Research [html] [Qmd]\n\n\n\n\n\nJan 26\n🐙 Literate Programming / Intro to Quarto [html] [Qmd]\n\n\n\n\nWeek 3\nJan 31\n🔬 What is the Question? [html] [Qmd]\n\n\n\n\n\n\n\n\n\n\n\nModule 3\n\nData Analysis in R \n\n\n\n\n\n\n\n\n\n\n\n\nJan 31\n👓 Reading/Writing data [html] [Qmd]\n\n\n\n\n\nFeb 2\n👓 Reference Management [html] [Qmd]\n🍂 Project 0 due\n\n\n\nWeek 4\nFeb 7\n🐙 Scripts [html] [Qmd]\n\n\n\n\n\nFeb 9\n🔩 Data [html] [Qmd]\n\n\n\n\n\n\n\n\n\n\n\nModule 3\n\nData Visulization in R\n\n\n\n\n\n\n\n\n\n\n\nWeek 5\nFeb 14\n🔪 Tidying Data [html] [Qmd]\n\n\n\n\n\nFeb 16\n📊 Plotting Systems (Base R) [html] [Qmd]\n🌱 Project 1 assigned\n\n\n\n\n\n\n\n\n\n\nWeek 6\nFeb 21\n📊 Plotting with ggplot2 [html] [Qmd]\n\n\n\n\n\nFeb 23\n📊 ggplot2 continued\n\n\n\n\n\n\n\n\n\n\n\nModule 4\n\nTour of Stats in R and Data Wrangling\n\n\n\n\n\n\n\n\n\n\n\nWeek 7\nFeb 28\n🐒 Tidyverse and data wrangling [html] [Qmd]\n\n\n\n\n\nMar 2\n🐎 A tour of some multivariate methods in R [html] [Qmd]\n🌱 Project 1 due\n\n\n\n\n\n\n\n\n\n\nWeek 8\nMar 7\n📆 A tour of some univariate methods in R [html] [Qmd]\n\n\n\n\n\nMar 9\n🐎 Joining a.k.a. merging data [html] [Qmd]\n\n\n\n\n\nMar 9\n🐎 Reshaping data [html] [Qmd]\n\n\n\n\n\n\n\n\n\n\n\nModule 5\n\nProgramming Elements\n\n\n\n\n\n\n\n\n\n\n\nWeek 9\nMar 14\n🤝 Spring Break\n\n\n\n\n\nMar 16\n☀️ Spring Break\n\n\n\n\n\n\n\n\n\n\n\nWeek 10\nMar 21\n🍄 Functions [html] [Qmd]\n\n\n\n\n\n\n\n\n\n\n\n\nMar 23\n🌵 Lists and For Loops [html] [Qmd]\n\n\n\n\n\n\n\n\n\n\n\n\nMar 23\n🌵 Apply Functions [html] [Qmd]\n\n\n\n\n\n\n\n\n\n\n\nWeek 11\nMar 28\n🎡 Program Flow [html] [Qmd]\n\n\n\n\n\n\n\n\n\n\n\nModule 6\n\nIntro to Morphometrics\n\n\n\n\n\n\n\n\n\n\n\n\nMar 30\n📏 Intro to Morphometrics [html] [Qmd]\n\n\n\n\n\n\n\n\n\n\n\nWeek 12\nApr 4\n📐 Landmark-based Morphometrics\n🍂 Project 2 due\n\n\n\n\n\n\n\n\n\n\n\nApr 4\n💀 3D Morphometrics\n\n\n\n\n\nApr 6\n🐾 3D Morphometrics\n🌰 Project 3 data check due\n\n\n\n\n\n\n\n\n\n\nModule 7\n\nPhylogenetic Trees\n\n\n\n\n\n\n\n\n\n\n\nWeek 13\nApr 11\n🌱 Trees as Data Objects\n\n\n\n\n\nApr 13\n🌴 Reading and writing trees, ape, ggtree, treeio\n\n\n\n\n\n\n\n\n\n\n\nWeek 14\nApr 18\n🌳 Reshaping Trees\n\n\n\n\n\n\n\n\n\n\n\n\nApr 20\n🌺 Annotating Tree Plots\n📝 Project 3 peer review due\n\n\n\n\n\n\n\n\n\n\nModule 8\n\nOther Topics\n\n\n\n\n\n\n\n\n\n\n\nWeek 15\nApr 25\n🐍 Parsing Data from any Format\n\n\n\n\n\n\n\n\n\n\n\n\nApr 27\n🌰 Student choice topic\n\n\n\n\n\n\n\n\n\n\n\nWeek 16\nMay 2\n🏆 Final Presentations\n💐 Project 3 presentation (in class)\n\n\n\n\n\n\n\n\n\n\n\nMay 5\n\n🎊 Project 3 due"
  }
]