{
  "hash": "38ca5c70a965cc4c5119eaae235a07fc",
  "result": {
    "markdown": "---\ntitle: \"Measurement Error\"\nauthor:\n  - name: Marguerite Butler\n    url: https://butlerlab.org\n    affiliation: School of Life Sciences, University of Hawaii\n    affiliation_url: https://manoa.hawaii.edu/lifesciences/\ndescription: \"No measurements are perfect, so quantifying repeatability is important\"\ndate: 2023-03-30\ncategories: [module 6, week 12, control structures, if else, (do) while, programming]\nbibliography: refs.bib\n---\n\n\n### Acknowledgements\n\nMaterial for this lecture was borrowed and adopted from\n\n- <https://www.r-bloggers.com/2015/04/tips-tricks-8-examining-replicate-error/>\n\n# Learning objectives\n\n::: callout-note\n# Learning objectives\n\n**At the end of this lesson you will:**\n\n-   Be able to estimate measurement error and repeatability\n:::\n\n# Overview\n\n# Measurement Error and Repeatability\n\nMorphometrics is all about assessing variability, within and between individuals. One of those sources of variability is measurement error. \n\nMeasurement Error (ME) itself comes from many potential sources:\n\n-  the measurement device (precision)\n-  definition of the measure\n-  quality of the measured material\n-  the measurer\n-  the environment of the measurer (hopefully small!)\n-  measurement protocol\n\nWe try to minimize ME so that we can reveal the underlying patterns we are interested in, but there will always be some ME. So it is important to quantify at least once at the beginning of the study. \n\n## Protocol for assessing ME\n\nThe percentage of measurement error is defined as the within-group component of variance divided by the total (within + betwee group) variance [@Claude:2008]:\n\n$$\n\\%ME = \\frac{s^{2}_{within}}{s^{2}_{within} + s^{2}_{among}} \\times 100\n$$\n\nWe can get the componets of variance $s^{2}$ from the mean squares ($MSS$) of an ANOVA considering the individual (as a factor) source of variation. Individual here represents the within-group variation. The among and within variance can be estimated from the mean sum of squares and $m$ the number of repeated measurements:\n\n$$\ns^{2}_{among} = \\frac{MSS_{among} - MSS_{within}}{m} \n$$\n\nand \n\n$$\ns^{2}_{within} = MSS_{within} \n$$\n\n## Example\n\nSuppose we are taking photographs of specimens, and then collecting landmark data from the photos. This is a pretty typical data collection pipeline. \n\nBecause we are taking 2D photos from 3D objects, one potential issue is whether the shape variation we obtain is real, or whether it is introduced by placing either the object or the camera at slightly different angles. \n\nAnother potential issue is whether we are placing the digitized landmarks in exactly the same place.  \n\nThere may be additional issues as well - for example some small ambiguity on the physical object, or the material or photos may be of different quality. \n\n### Plan your data management\n\nI always recommend storing your metadata in the filenames. That way you never lose the information. \n\n__Photo files:__  A good strategy for data management is to label the photo files:\n              `id_picture_replicate.jpg`\n\nWhere:\n-  __id__ refers to the specimen, \n-  __picture__ the replicate photo (photo1 or photo2), and \n-  __replicate__ the replicate landmark coordinates (rep1 or rep2). \n\nWe can parse the metadata from the filenames by code such as:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfiles <- list.files()  # to read the file names from the current drectory\nfiles <- files <- c(\"id1_photo1_rep1.jpg\", \n                    \"id1_photo1_rep2.jpg\", \n                    \"id1_photo2_rep1.jpg\", \n                    \"id1_photo2_rep2.jpg\")  # made up example to practice \n\n# Collect metadata, approach 1 - substr\nmeta <- strsplit(files, \"_|\\\\.\")  # metadata. split filenames by _ or . \n                                  # Need to use \\\\ to escape the .\nid <- sapply(meta, \"[[\", 1)\nphoto <- sapply(meta, \"[[\", 2)\nrep <- sapply(meta, \"[[\", 3)\n\n# Collect metadata approach 2 - sub\n# using sub and regular expressions to select (string1)_(string2)_(string3)\n# also ignoring the final .jpg, where . is escaped by \\\\\n\nid <- sub(\"^([a-zA-Z0-9]+)_([a-zA-Z0-9]+)_([a-zA-Z0-9]+)(\\\\.jpg)\", \"\\\\1\", files)\nphoto <- sub(\"^([a-zA-Z0-9]+)_([a-zA-Z0-9]+)_([a-zA-Z0-9]+)(\\\\.jpg)\", \"\\\\2\", files)\nrep <- sub(\"^([a-zA-Z0-9]+)_([a-zA-Z0-9]+)_([a-zA-Z0-9]+)(\\\\.jpg)\", \"\\\\3\", files)\n```\n:::\n\n\nWe can use these vectors along with the coordinates to test for measurement error with ANOVA. \n\n### Statistical methods for Measurement Error:\n\nWe will assess measurement error at two levels, photography error  and digitizing error: \n\n__Photography error:__ Take __two sets of photos__, each time placing the object in front of the camera and positioning the specimen. (I.e., the entire process to give us a good estimate of photo capture error)\n\n__Landmark digitizing error:__ Collect landmarks twice, ideally in different sessions on different days or weeks. \n\n__Data:__  In this example we will have 4 sets of landmark data for each specimen, 2 photos x 2 digitizing replicates, allowing assessment of error associated with the digitization as well as error in capturing the shapes via the photographs. \n\n__Model:__ We will use a [nested ANOVA](http://www.biostathandbook.com/nestedanova.html) to estimate repeatability and (measurement error) of the landmarks, to try to separate the variation introduced by the digitization process, apart from the other sources of variation. \n\n\n### Analyze with ANOVA: \n\nNested ANOVA indicates that we have a nested structure of replicates within groups (i.e., `rep1` of `photo1` has nothing to do with `rep1` of `photo2`. `rep` is nested within `photo`. \n\nIn R we specify a nested model forumula [using :](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/formula.html) in the model term (to indicate interaction terms only with no main effect): \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm.fit <- lm(coords ~ id:photo:rep)\naov(lm.fit)\n```\n:::\n\n__Data and model term objects:__\n-  __coords__ is the data object (a vector or array)\n-  __id__ is a vector containing labels for each specimen\n-  __photo__ is a vector (photo is 1 or 2)\n-  __rep__ is a vector (digitizing replicate 1 or 2)\n\n\nLook at the values of the Mean Squares (MS) column in the ANOVA table. Compare the value for `id:photo` and `id:photo:rep` with `id`. \n\nTo calculate the __repeatability__ of our digitizing ability, we subtract the __MS__ of the __rep__ term from the __individual__ term and divide by __two__ (because we have two replicates):\n__(MS(id) – MS(ind:photo:rep))/2__\n\nThen we calculate the ratio of this value to the total MS:\n__((MS(id) – MS(id:photo:rep))/2 ) / (MS(id)+MS(id:photo)+MS(id:photo:rep))__ \n\nThe result is the __repeatability__, which in good circumstances is somewhere above 0.95; and thus 5% __measurement error__.\n\n\n### Simplifed Simulated example:\n\nSimplifed example: 20 specimens, 1 photo, 2 digitzing reps:\n\n20 specimens: (single measurement dataset). \n2 repetitons: Digitize each photo twice (once in each of two sessions on different days).  \nHow repeatable are the measurements?  \n\nSimulate the data:  \n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrue_m <- rnorm(20,20,3)  # true values for  specimens\nm1 <- true_m + rnorm(20,0,0.5)  # measurement 1\nm2 <- true_m + rnorm(20,0,0.5)  # measurement 2\n\nid <- as.factor(rep(1:20, times=2))\nrep <- gl(2, 20)\ntotal_m <- c(m1, m2)\ncbind(id, total_m, rep)  # the data\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      id  total_m rep\n [1,]  1 17.10225   1\n [2,]  2 15.93986   1\n [3,]  3 16.19714   1\n [4,]  4 21.44124   1\n [5,]  5 21.54744   1\n [6,]  6 16.44169   1\n [7,]  7 18.90075   1\n [8,]  8 20.44293   1\n [9,]  9 25.09316   1\n[10,] 10 20.39886   1\n[11,] 11 25.94884   1\n[12,] 12 18.35997   1\n[13,] 13 21.62585   1\n[14,] 14 19.90764   1\n[15,] 15 27.66315   1\n[16,] 16 21.11648   1\n[17,] 17 16.21200   1\n[18,] 18 19.97062   1\n[19,] 19 20.03302   1\n[20,] 20 15.84359   1\n[21,]  1 17.12800   2\n[22,]  2 17.28850   2\n[23,]  3 16.03851   2\n[24,]  4 21.87341   2\n[25,]  5 20.78227   2\n[26,]  6 16.73352   2\n[27,]  7 17.15422   2\n[28,]  8 20.67574   2\n[29,]  9 24.17468   2\n[30,] 10 20.15450   2\n[31,] 11 26.62219   2\n[32,] 12 18.34595   2\n[33,] 13 21.26133   2\n[34,] 14 20.48446   2\n[35,] 15 27.57298   2\n[36,] 16 20.05384   2\n[37,] 17 17.00963   2\n[38,] 18 21.46221   2\n[39,] 19 20.56191   2\n[40,] 20 14.67021   2\n```\n:::\n:::\n\n\nIs there a difference between the measurement sessions?\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(aov(lm ( total_m ~ rep)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Df Sum Sq Mean Sq F value Pr(>F)\nrep          1      0    0.00       0  0.995\nResiduals   38    434   11.42               \n```\n:::\n:::\n\n\nNo (_thatʻs good!_)\n\nIs there a difference between individual specimens?\n\n::: {.cell}\n\n```{.r .cell-code}\nmod <- summary(aov(lm( total_m ~ id )))\nmod\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Df Sum Sq Mean Sq F value   Pr(>F)    \nid          19  427.3  22.491   67.72 4.01e-14 ***\nResiduals   20    6.6   0.332                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nYes, and the resigual mean squared error looks small too (good!). \nHow big is the measurement error? \n\n::: {.cell}\n\n```{.r .cell-code}\ns2_within <- ms_within <- mod[[1]][2,3]\ns2_within\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3321015\n```\n:::\n\n```{.r .cell-code}\nms_among <- mod[[1]][1,3]\ns2_among <- (ms_among-ms_within)/2\nME <- s2_within/(s2_within+s2_among) * 100\n```\n:::\n\nNot bad. A rule of thumb is that 5% ME is good (95% repeatability). If we want to reduce ME, we can use the average of the two measurements in our analyses. \n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}