{
  "hash": "8a51f2500fe7a72dc08e24f0b2997ba5",
  "result": {
    "markdown": "---\ntitle: \"Measurement Error\"\nauthor:\n  - name: Marguerite Butler\n    url: https://butlerlab.org\n    affiliation: School of Life Sciences, University of Hawaii\n    affiliation_url: https://manoa.hawaii.edu/lifesciences/\ndescription: \"Getting started with some issues in morphometrics\"\ndate: 2023-03-30\ncategories: [module 6, week 12, control structures, if else, (do) while, programming]\nbibliography: refs.bib\n---\n\n\n### Acknowledgements\n\nMaterial for this lecture was borrowed and adopted from\n\n-   <https://www.stephaniehicks.com/jhustatcomputing2022/posts/2022-09-22-control-structures>\n\n- <https://www.r-bloggers.com/2015/04/tips-tricks-8-examining-replicate-error/>\n- <>\n\n# Learning objectives\n\n::: callout-note\n# Learning objectives\n\n**At the end of this lesson you will:**\n\n-   Be able to estimate measurement error and repeatability\n-   \n:::\n\n# Overview\n\n# Morphometric Workflow\n\n::: {.callout-tip}\n## Typical Morphometric Worflow\n-  Ask interesting question, develop hypotheses, collect specimens\n-  Take photos/images of the specimens\n-  Place landmarks on the specimens in the photos at key locations. Choose landmarks that are:\n  +  Present on all specimens\n  +  Relevant to question\n  +  Clearly defined\n-  Analyze how the landmarks vary in relation to each other among specimens\n-  Perform statistical analyses to test for significant differences in body shape among\npopulations, species, or whatever aspect that is relevant to your hypotheses\n-  Plot components of variation in shape that are not correlated with each other (i.e. the principal component scores).\n:::\n\n\n\n# Types of Questions that Morphometrics can Answer (not exhaustive)\n\n1. Relationship of parts to other parts. \n2. Allometry - relationship of shape to size\n3. Comparison among groups\n4. Covariation of shape with other factors\n5. Covariation of morphology with phylogeny\n6. Morpholgical integration \n7. Modularity \n\n\n\n\n\n\n\n\n\n\nWe try to minimize ME so that we can reveal the underlying patterns we are interested in, but there will always be some ME. So it is important to quantify at least once at the beginning of the study. \n\n## Protocol for assessing ME\n\nThe percentage of measurement error is defined as the within-group component of variance divided by the total (within + betwee group) variance [@Claude:2008]:\n\n$$\n\\%ME = \\frac{s^{2}_{within}}{s^{2}_{within} + s^{2}_{among}} \\times 100\n$$\n\nWe can get the componets of variance $s^{2}$ from the mean squares ($MSS$) of an ANOVA considering the individual (as a factor) source of variation. Individual here represents the within-group variation. The among and within variance can be estimated from the mean sum of squares and $m$ the number of repeated measurements:\n\n$$\ns^{2}_{among} = \\frac{MSS_{among} - MSS_{within}}{m} \n$$\n\nand \n\n$$\ns^{2}_{within} = MSS_{within} \n$$\n\n## Example\n\nLet’s say you are taking photographs of your specimens. They are rather rounded and so it is hard to place them flat on the table to photograph from above. Issue 1 here is whether the shape variation we observe in the photo is real or due to placing the specimen at slightly different angles. Then, when once you have the photograph you need to digitize the landmarks. Issue 2 is whether you put the landmarks in the same place every time (i.e. is your criteria for the landmark robust enough that its obvious where it should be placed on each specimen, and if you came back to the data a month or year later?)\n\nTo assess measurement error in this instance we could take two sets of pictures, each time removing and positioning the specimen. And we could digitize each image twice, preferably in different sessions (another day or week). This would give us 4 sets of landmark data for each specimen, allowing us to asses both error associated with the digitization as well as error in capturing the shapes via the photographs. \n\nAlternatively, if we were interested in inter-observer error vs. repeatability within observer, we could take one photograph and have it measured by two different people, each person taking two sets of measurements (preferably in different sessions). \n\n### Simulated example:\n\nRepeat a set of five measurements, (measure twice), once in each of two sessions on different days.  How repeatable are the measurements?  \n\nSimulate the data:  \n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrue_m <- rnorm(5,20,3)  # true values\nm1 <- true_m + rnorm(5,0,1) # measurements set 1\nm2 <- true_m + rnorm(5,0,1)\nsession <- gl(2, 5)\nindividual <- as.factor(rep(1:5,2))\ntotal_m <- c(m1, m2)\ncbind(individual, total_m, session)  # the data\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      individual  total_m session\n [1,]          1 20.52958       1\n [2,]          2 23.10193       1\n [3,]          3 19.95276       1\n [4,]          4 18.87260       1\n [5,]          5 21.36754       1\n [6,]          1 21.51454       2\n [7,]          2 25.17617       2\n [8,]          3 17.83387       2\n [9,]          4 18.28750       2\n[10,]          5 19.81976       2\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(aov(total_m ~ session))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Df Sum Sq Mean Sq F value Pr(>F)\nsession      1   0.14   0.142   0.025  0.878\nResiduals    8  45.48   5.685               \n```\n:::\n:::\n\nThere is no difference between sessions (yay)!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod <- summary(aov(total_m ~ individual))\nmod \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            Df Sum Sq Mean Sq F value Pr(>F)  \nindividual   4  39.37   9.843   7.874 0.0219 *\nResiduals    5   6.25   1.250                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\nThe specimens measured (individuals) are significantly different. \n\n::: {.cell}\n\n```{.r .cell-code}\ns2within <- MSwithin <- mod[[1]][2,3]\nMSamong <- mod[[1]][1,3]\ns2among <- (MSamong-MSwithin)/2\npME <- s2within/(s2within+s2among)*100\npME\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 22.53755\n```\n:::\n:::\n\nAnd the percent measurement error is represented by `pME`.  As a rough rule of thumb we want this to be less than 5%.  If it is very high, we either want to practice more, or take multiple measurements of each variable and average them. \n\n\n\nFor example: Let’s say you are taking photographs of your specimens. They are rather rounded and so it is hard to place them flat on the table to photograph from above. Issue 1 here is whether the shape variation we observe in the photo is real or due to placing the specimen at slightly different angles. Then, when once you have the photograph you need to digitize the landmarks. Issue 2 is whether you put the landmarks in the same place every time (i.e. is your criteria for the landmark robust enough that its obvious where it should be placed on each specimen, and if you came back to the data a month or year later?)\n\nIn this instance we could take two sets of pictures, each time removing and positioning the specimen. And we could digitize each image twice, preferably in different sessions (another day or week). This would give us 4 sets of landmark data for each specimen.\n\nIf it were me, I would label the files:\n              Individual_photo1_rep1.jpg \nWhere I have the ID of the individual, followed by which picture (photo1 or photo2) and then the digitizing replicate (rep1 or rep2).\n\nTo test for differences between landmark sets:\n\n1) Read the coordinate data into R (using geomorph’s functions readland.tps() or readland.nts() for example). \n2) Use gpagen() to perform a Procrustes Superimposition\n3) Perform a Procrustes ANOVA in the style:\n\nprocD.lm(Y.gpa$coords ~ ind:photo:rep)\n# Y.gpa$coords is the 3D array of Procrustes residuals (shape data)\n# ind is a vector containing labels for each individual\n# photo is a vector designating whether the photo is 1 or 2\n# rep is a vector designating whether the replicate is 1 or 2\n\n(Tip! Use strsplit() to make these classifier vectors from the photo names, as we did in Tips & Tricks 5)\n\nSee here we use : in the model term – this means we are performing a nested ANOVA. \n\nWhat we are looking for in the resulting ANOVA table is the values of the Mean Squares (MS) column. Compare the value for ind:photo and ind:photo:rep with ind. \n\nTo calculate the repeatability of our digitizing ability, we subtract the MS of the photo term from the individual term and divide by two (because we have two replicates):\n(MS(ind) – MS(ind:photo:rep))/2 \nThen we calculate the ratio of this value to the total MS:\n((MS(ind) – MS(ind:photo:rep))/2 ) / (MS(ind)+MS(ind:photo)+MS(ind:photo:rep)) \n\nThe result is a value, which in good circumstances is somewhere above 0.95; a repeatability of 0.95 and thus 5% error.\n\nThe same can be done for the photos (ind:photo) but of course remember digitizing error is also in this term. This post is inspired by Chapter 9 of the Green Book, which I strongly recommend reading.\n\nRemember, all this can be done by accessing the parts of the ANOVA table using regular R indexing. Dump the output of procD.lm into an object e.g. called res then res[,3] will be the MS values.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}